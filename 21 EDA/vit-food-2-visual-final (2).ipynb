{"cells":[{"cell_type":"markdown","metadata":{"id":"7a8913de-e49e-40c9-89c7-6b847fac9def"},"source":["## 0. Getting setup\n","\n","As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n","\n","We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n","\n","To do so, we'll download [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n","\n","We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n","\n","`torchinfo` will help later on to give us a visual representation of our model.\n","\n","And since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions."]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:12:28.192016Z","iopub.status.busy":"2024-09-22T16:12:28.191575Z","iopub.status.idle":"2024-09-22T16:12:28.202174Z","shell.execute_reply":"2024-09-22T16:12:28.201215Z","shell.execute_reply.started":"2024-09-22T16:12:28.191967Z"},"id":"ebe46d77-6c4d-4102-9994-2cb89f633f18","outputId":"027b2a5d-262a-4205-b274-1bb055b41e5c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch version: 2.4.1\n","torchvision version: 0.19.1\n"]}],"source":["# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n","try:\n","    import torch\n","    import torchvision\n","    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n","    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")\n","except:\n","    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n","    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","    import torch\n","    import torchvision\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:28.204122Z","iopub.status.busy":"2024-09-22T16:12:28.203795Z","iopub.status.idle":"2024-09-22T16:12:28.221637Z","shell.execute_reply":"2024-09-22T16:12:28.220761Z","shell.execute_reply.started":"2024-09-22T16:12:28.204090Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<function torchinfo.torchinfo.summary(model: 'nn.Module', input_size: 'INPUT_SIZE_TYPE | None' = None, input_data: 'INPUT_DATA_TYPE | None' = None, batch_dim: 'int | None' = None, cache_forward_pass: 'bool | None' = None, col_names: 'Iterable[str] | None' = None, col_width: 'int' = 25, depth: 'int' = 3, device: 'torch.device | str | None' = None, dtypes: 'list[torch.dtype] | None' = None, mode: 'str | None' = None, row_settings: 'Iterable[str] | None' = None, verbose: 'int | None' = None, **kwargs: 'Any') -> 'ModelStatistics'>"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","summary"]},{"cell_type":"markdown","metadata":{"id":"30caf875-557e-410f-8dff-bd4a9f6c7ae4"},"source":["> **Note:** If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` and `torchvision`.\n","\n","Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) script from GitHub.\n","\n","The `helper_functions.py` script contains several functions we created in previous sections:\n","* `set_seeds()` to set the random seeds (created in [07. PyTorch Experiment Tracking section 0](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds)).\n","* `download_data()` to download a data source given a link (created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n","* `plot_loss_curves()` to inspect our model's training results (created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0))\n","\n","> **Note:** It may be a better idea for many of the functions in the `helper_functions.py` script to be merged into `going_modular/going_modular/utils.py`, perhaps that's an extension you'd like to try.\n"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:12:28.223146Z","iopub.status.busy":"2024-09-22T16:12:28.222786Z","iopub.status.idle":"2024-09-22T16:12:28.242131Z","shell.execute_reply":"2024-09-22T16:12:28.241366Z","shell.execute_reply.started":"2024-09-22T16:12:28.223105Z"},"id":"960eb156-c1b1-4e76-a812-01bf045835bd","outputId":"d14f2af2-ca52-40c3-d9fc-16764f03a64b","trusted":true},"outputs":[],"source":["# Continue with regular imports\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# # Try to import the going_modular directory, download it from GitHub if it doesn't work\n","# try:\n","#     from going_modular.going_modular import data_setup, engine\n","#     from helper_functions import download_data, set_seeds, plot_loss_curves\n","# except:\n","#     # Get the going_modular scripts\n","#     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n","#     !git clone https://github.com/mrdbourke/pytorch-deep-learning\n","#     !mv pytorch-deep-learning/going_modular .\n","#     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n","#     !rm -rf pytorch-deep-learning\n","#     from going_modular.going_modular import data_setup, engine\n","#     from helper_functions import download_data, set_seeds, plot_loss_curves"]},{"cell_type":"markdown","metadata":{"id":"4f9bdd26-26ac-4756-bd8e-b7a50799f28b"},"source":["> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.execute_input":"2024-09-22T16:12:28.244534Z","iopub.status.busy":"2024-09-22T16:12:28.244215Z","iopub.status.idle":"2024-09-22T16:12:28.256806Z","shell.execute_reply":"2024-09-22T16:12:28.255948Z","shell.execute_reply.started":"2024-09-22T16:12:28.244503Z"},"id":"5e246f92-e509-474e-b6c7-c82cf11cb8ca","outputId":"c4505b6c-fbc3-41dd-a841-7b9c02dbf2b5","trusted":true},"outputs":[{"data":{"text/plain":["'cpu'"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"execution":{"iopub.execute_input":"2024-09-22T16:12:28.258285Z","iopub.status.busy":"2024-09-22T16:12:28.257901Z","iopub.status.idle":"2024-09-22T16:12:28.265417Z","shell.execute_reply":"2024-09-22T16:12:28.264753Z","shell.execute_reply.started":"2024-09-22T16:12:28.258239Z"},"id":"RSlwLcWz13BX","outputId":"64c4a228-ccd3-4035-c17d-ad485ed9cd63","trusted":true},"outputs":[],"source":["\n","# raise ZeroDivisionError"]},{"cell_type":"markdown","metadata":{"id":"de0f9531-64f3-4e13-8482-ce545d608900"},"source":["## 10. Using a pretrained ViT from `torchvision.models` on the same dataset\n","\n","We've discussed the benefits of using pretrained models in [06. PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/).\n","\n","But since we've now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.\n","\n","### 10.1 Why use a pretrained model?\n","\n","An important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.\n","\n","And in modern day machine learning, the original fully trained ViT would likely not be considered a \"super large\" training setup (models are continually getting bigger and bigger).\n","\n","Reading the ViT paper section 4.2:\n","\n","> Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately **30 days**.\n","\n","As of July 2022, the [price for renting a TPUv3](https://cloud.google.com/tpu/pricing) (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.\n","\n","To rent one for 30 straight days would cost **$5,760 USD**.\n","\n","This cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it's not.\n","\n","So having a pretrained model available through resources like [`torchvision.models`](https://pytorch.org/vision/stable/models.html), the [`timm` (Torch Image Models) library](https://github.com/rwightman/pytorch-image-models), the [HuggingFace Hub](https://huggingface.co/models) or even from the authors of the papers themselves (there's a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I'm a big fan of this trend, many of these resources can be found on [Paperswithcode.com](https://paperswithcode.com/)).\n","\n","If you're focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I'd highly recommend using a pretrained model."]},{"cell_type":"markdown","metadata":{"id":"93027389-1309-47c0-85d3-50e241b617b0"},"source":["### 10.2 Getting a pretrained ViT model and creating a feature extractor\n","\n","We can get a pretrained ViT model from `torchvision.models`.\n","\n","We'll go from the top by first making sure we've got the right versions of `torch` and `torchvision`.\n","\n","> **Note:** The following code requires `torch` v0.12+ and `torchvision` v0.13+ to use the latest `torchvision` model weights API."]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:28.267097Z","iopub.status.busy":"2024-09-22T16:12:28.266739Z","iopub.status.idle":"2024-09-22T16:12:28.276056Z","shell.execute_reply":"2024-09-22T16:12:28.275202Z","shell.execute_reply.started":"2024-09-22T16:12:28.267058Z"},"id":"30de8333-74b0-49ae-a81e-0266e6325f26","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.4.1\n","0.19.1\n"]}],"source":["# The following requires torch v0.12+ and torchvision v0.13+\n","import torch\n","import torchvision\n","print(torch.__version__)\n","print(torchvision.__version__)"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:28.277369Z","iopub.status.busy":"2024-09-22T16:12:28.277091Z","iopub.status.idle":"2024-09-22T16:12:29.305967Z","shell.execute_reply":"2024-09-22T16:12:29.304982Z","shell.execute_reply.started":"2024-09-22T16:12:28.277339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m__pycache__\u001b[m\u001b[m        modeling.py        modeling_resnet.py\n","configs.py         modeling_VA.py\n"]}],"source":["!ls models"]},{"cell_type":"markdown","metadata":{},"source":["### Setup for new file"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:29.307869Z","iopub.status.busy":"2024-09-22T16:12:29.307534Z","iopub.status.idle":"2024-09-22T16:12:42.431542Z","shell.execute_reply":"2024-09-22T16:12:42.430374Z","shell.execute_reply.started":"2024-09-22T16:12:29.307836Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: pip\n"]}],"source":["!pip install ml_collections"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:42.435339Z","iopub.status.busy":"2024-09-22T16:12:42.434998Z","iopub.status.idle":"2024-09-22T16:12:47.657624Z","shell.execute_reply":"2024-09-22T16:12:47.656416Z","shell.execute_reply.started":"2024-09-22T16:12:42.435299Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ViT_visualization'...\n","remote: Enumerating objects: 98, done.\u001b[K\n","remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98 (from 1)\u001b[K\n","Receiving objects: 100% (98/98), 16.27 MiB | 1.41 MiB/s, done.\n","Resolving deltas: 100% (36/36), done.\n","mv: rename ViT_visualization/models to ./models: Directory not empty\n","\u001b[34m__pycache__\u001b[m\u001b[m        modeling.py        modeling_resnet.py\n","configs.py         modeling_VA.py\n"]}],"source":["!git clone https://github.com/byM1902/ViT_visualization/\n","!mv ViT_visualization/models .\n","!rm -rf ViT_visualization\n","!ls models"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:47.659449Z","iopub.status.busy":"2024-09-22T16:12:47.659096Z","iopub.status.idle":"2024-09-22T16:12:47.866860Z","shell.execute_reply":"2024-09-22T16:12:47.865960Z","shell.execute_reply.started":"2024-09-22T16:12:47.659400Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["imported succesfully\n"]}],"source":["import typing\n","import io\n","import os\n","\n","import torch\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from urllib.request import urlretrieve\n","\n","from PIL import Image\n","from torchvision import transforms\n","\n","from models.modeling import VisionTransformer, CONFIGS\n","print(\"imported succesfully\")"]},{"cell_type":"code","execution_count":null,"id":"4f487b07","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### loading new file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:12:47.868695Z","iopub.status.busy":"2024-09-22T16:12:47.868084Z","iopub.status.idle":"2024-09-22T16:13:04.187420Z","shell.execute_reply":"2024-09-22T16:13:04.186205Z","shell.execute_reply.started":"2024-09-22T16:12:47.868660Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: wget\n"]}],"source":["'''loading weights'''\n","!mkdir -p weights\n","!wget -O weights/ViT-B_16-224.npz /tmp/Ubuntu.iso https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.189602Z","iopub.status.busy":"2024-09-22T16:13:04.189113Z","iopub.status.idle":"2024-09-22T16:13:04.194671Z","shell.execute_reply":"2024-09-22T16:13:04.193576Z","shell.execute_reply.started":"2024-09-22T16:13:04.189555Z"},"trusted":true},"outputs":[],"source":["# !rm -rf weights"]},{"cell_type":"code","execution_count":null,"id":"ddc8594d","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8f281a07","metadata":{},"source":["## Model state dict loading code"]},{"cell_type":"code","execution_count":73,"id":"d5fd2b35","metadata":{},"outputs":[],"source":["# pretrained_vit\n","def load_model_state_dict(model, relative_path_to_checkpoints=\"../Pretrained_checkpoints\", filename=\"model_state.pth\"):\n","    \"\"\"\n","    Load the state dictionary of a pretrained model from a sibling directory.\n","\n","    Parameters:\n","        model: The model object to load the state dictionary into.\n","        relative_path_to_checkpoints: Relative path from the current directory to the checkpoint directory (default: '../Pretrained_checkpoints').\n","        filename: The name of the checkpoint file (default: 'model_state.pth').\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Create the full path relative to the current directory\n","    file_path = os.path.join(relative_path_to_checkpoints, filename)\n","    \n","    # Load the state dictionary\n","    # Load the state dictionary\n","    # print(file_path)\n","    model_state_dict = torch.load(file_path, map_location='cpu', weights_only=True)\n","    model.load_state_dict(model_state_dict)\n","    print(f\"Model loaded from {file_path}\")\n"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.196357Z","iopub.status.busy":"2024-09-22T16:13:04.196021Z","iopub.status.idle":"2024-09-22T16:13:04.207482Z","shell.execute_reply":"2024-09-22T16:13:04.206659Z","shell.execute_reply.started":"2024-09-22T16:13:04.196314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded from ../Pretrained_checkpoints/model_state_dict.pth\n","model change model evaluation is successful!\n"]}],"source":["# Prepare Model\n","config = CONFIGS[\"ViT-B_16\"]\n","# model = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True)\n","model = VisionTransformer(config, num_classes=10, zero_head=False, img_size=224, vis=True)\n","\n","# model.load_from(np.load(\"weights/ViT-B_16-224.npz\"))\n","\n","load_model_state_dict(model, relative_path_to_checkpoints=\"../Pretrained_checkpoints\", filename=\"model_state_dict.pth\")\n","\n","# write herae model weights loading code tk\n","model.eval()\n","print('model change model evaluation is successful!')"]},{"cell_type":"code","execution_count":75,"id":"70a61cb5","metadata":{},"outputs":[{"data":{"text/plain":["VisionTransformer(\n","  (transformer): Transformer(\n","    (embeddings): Embeddings(\n","      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): Encoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x Block(\n","          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (attn): Attention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (out): Linear(in_features=768, out_features=768, bias=True)\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (proj_dropout): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","        )\n","      )\n","      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    )\n","  )\n","  (head): Linear(in_features=768, out_features=10, bias=True)\n",")"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":76,"id":"2f45280a","metadata":{},"outputs":[],"source":["class_names=['airplane',\n","  'automobile',\n","  'bird',\n","  'cat',\n","  'deer',\n","  'dog',\n","  'frog',\n","  'horse',\n","  'ship',\n","  'truck']"]},{"cell_type":"markdown","id":"c2c1af42","metadata":{},"source":["## Image loading\n"]},{"cell_type":"code","execution_count":77,"id":"039c5a26","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 5 images, 5 labels, 5 outputs, 5 image IDs.\n"]}],"source":["import os\n","import numpy as np\n","\n","def load_data_from_directory(directory_path):\n","    \"\"\"Load images, labels, outputs, and image IDs from the specified directory.\"\"\"\n","\n","    \n","    try:\n","        # Construct the full paths for each file\n","        ids_path = os.path.join(directory_path, 'ids.npy')\n","        images_path = os.path.join(directory_path, 'images.npy')\n","        labels_path = os.path.join(directory_path, 'labels.npy')\n","        outputs_path = os.path.join(directory_path, 'outputs.npy')\n","\n","        # Load the numpy arrays\n","        img_ids = np.load(ids_path)\n","        images = np.load(images_path)\n","        labels = np.load(labels_path)\n","        outputs = np.load(outputs_path)\n","\n","        return img_ids, images, labels, outputs\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None, None, None, None\n","\n","# Example usage\n","directory_path = '../saved_images/correct'  # Adjust as needed\n","img_ids, images, labels, outputs = load_data_from_directory(directory_path)\n","preds = np.argmax(outputs,-1)\n","\n","\n","# Example to check the loaded data\n","if images is not None:\n","    print(f\"Loaded {images.shape[0]} images, {labels.shape[0]} labels, {outputs.shape[0]} outputs, {img_ids.shape[0]} image IDs.\")\n"]},{"cell_type":"code","execution_count":78,"id":"38fa6e8b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2933714..1.908451].\n"]},{"name":"stdout","output_type":"stream","text":["Label: 6\n","classname: frog\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_image(image_idx, title=None):\n","    \"\"\"\n","    Plot a single image using Matplotlib.\n","\n","    Args:\n","        image (numpy.ndarray): The image to plot. It should be in the format (height, width, channels).\n","        title (str, optional): The title for the plot. Defaults to None.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    # Print label and output\n","    print(f\"Label: {labels[image_idx]}\")\n","    print(f\"classname: {class_names[labels[image_idx]]}\")\n","    # print(f\"Label class: {class_names[labels[image_idx]]}\")\n","    # print(f\"Output: {preds[image_idx]}\")\n","\n","    image = images[image_idx]\n","\n","    if image.ndim == 3 and image.shape[2] == 3:\n","        # Color image\n","        plt.imshow(image)\n","    elif image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\n","        # Grayscale image\n","        if image.ndim == 3:\n","            image = image.squeeze()  # Remove the channel dimension\n","        plt.imshow(image, cmap='gray')\n","    else:\n","        raise ValueError(\"Image format not recognized. Expected (H, W) or (H, W, 3) or (H, W, 1).\")\n","\n","    if title:\n","        plt.title(title)\n","    \n","    plt.axis('off')  # Hide axis\n","    plt.show()\n","\n","# Example usage\n","# Assuming `images` is a numpy array of images with shape (k, H, W, C)\n","# and you want to plot the first image:\n","image_idx = 3\n","plot_image(image_idx, title=\"Example Image\")\n"]},{"cell_type":"code","execution_count":79,"id":"24bff7ae","metadata":{},"outputs":[],"source":["img_one=images[image_idx:image_idx+1]\n"]},{"cell_type":"code","execution_count":80,"id":"ce0ca044","metadata":{},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["type(img_one)"]},{"cell_type":"code","execution_count":81,"id":"773a489f","metadata":{},"outputs":[{"data":{"text/plain":["(1, 32, 32, 3)"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["img_one.shape"]},{"cell_type":"code","execution_count":88,"id":"2c54366f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 224, 224])\n","tensor(9)\n"]}],"source":["# Convert image to PIL image for processing\n","image_pil = Image.fromarray((img_one.squeeze() * 255).astype(np.uint8))  # Squeeze out the batch dimension\n","\n","# Define preprocessing steps\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize to 224x224\n","    transforms.ToTensor(),          # Convert image to tensor (C, H, W)\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n","])\n","\n","# Apply preprocessing to the image\n","image_tensor = preprocess(image_pil)\n","\n","# Add batch dimension back (N, C, H, W)\n","image_tensor = image_tensor.unsqueeze(0)\n","\n","# print(image_tensor.shape)\n","# Run the image through the model\n","with torch.no_grad():  # No need for gradients during testing\n","    output = model(image_tensor)\n","\n","# Output will contain logits or predicted classes depending on the model architecture\n","print(np.argmax(output[0]))\n"]},{"cell_type":"code","execution_count":null,"id":"134be466","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":83,"id":"cf0c1abc","metadata":{},"outputs":[],"source":["# import torchvision.transforms as T\n","\n","# # Define the transform pipeline\n","# pretrained_vit_transforms = T.Compose([\n","#     T.Resize((224, 224)),  # Resize the image to 224x224\n","#     # T.ToTensor(),  # Convert the image to a PyTorch tensor (scales pixel values to [0, 1])\n","#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize with mean and std\n","# ])"]},{"cell_type":"code","execution_count":84,"id":"10c53b5f","metadata":{},"outputs":[],"source":["# img_one = np.transpose(img_one, (1, 2, 0))"]},{"cell_type":"code","execution_count":85,"id":"509a7cb7","metadata":{},"outputs":[],"source":["# img_one = pretrained_vit_transforms(img_one)  # This works with NumPy ar\n"]},{"cell_type":"code","execution_count":86,"id":"1a975976","metadata":{},"outputs":[],"source":["\n","# with torch.inference_mode():\n","#     # out = pretrained_vit((pretrained_vit_transforms),torch.tensor(np.transpose(img_one, (0, 3, 1, 2))))\n","#     # out=model(pretrained_vit_transforms(torch.tensor(np.transpose(img_one, (0, 3, 1, 2)))))\n","#     out=model(torch.tensor(np.transpose(img_one, (0, 3, 1, 2))))\n","\n","\n"]},{"cell_type":"code","execution_count":104,"id":"9f02fdd6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["12\n"]}],"source":["logits, att_mat = model(image_tensor)\n","\n","print(len(att_mat))\n","\n","# att_mat = torch.stack(att_mat).squeeze(1)\n","# # print(att_mat.shape)\n","# att_mat = torch.mean(att_mat, dim=1) \n","# # print(att_mat.shape)\n","\n","# residual_att = torch.eye(att_mat.size(1))\n","# aug_att_mat = att_mat + residual_att\n","# aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n","# # Recursively multiply the weight matrices over 12 layers.\n","# joint_attentions = torch.zeros(aug_att_mat.size())\n","# joint_attentions[0] = aug_att_mat[0]\n","# for n in range(1, aug_att_mat.size(0)):\n","#     joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n","# # The mask matrices over 12 layers correspond with the normal tokens and the class token.\n","# masks = [] # mask matrices of normal tokens\n","# masks_cls = [] # mask matrice of class token\n","# for i in range(len(joint_attentions)):\n","#   v = joint_attentions[i] # [n_tokens, n_tokens]\n","#   mask = v[1:, 1:] # 196x196\n","#   mask = mask/mask.max(axis=0).values\n","  # mask = mask / mask.max() # [im_w, im_h, 1]\n","  # masks.append(mask)\n","  # mask_cls = v[0, 1:] # Attention of token class with patch token\n","  # mask_cls = mask_cls / mask_cls.max() # [im_w, im_h, 1]\n","  # masks_cls.append(mask_cls)"]},{"cell_type":"code","execution_count":129,"id":"ce85b4c5","metadata":{},"outputs":[],"source":["import torch\n","\n","def extract_attention_matrix_for_layer(att_mat, layer_idx=0, row_idx=1, head_idx=1):\n","    \"\"\"\n","    Extracts the attention matrix for a particular layer, row (token), and attention head,\n","    excluding the class token.\n","\n","    Parameters:\n","    - att_mat (list of Tensors): List of attention matrices for each layer, where each tensor \n","      has shape [1, num_heads, num_tokens, num_tokens] (batch size 1 included).\n","    - layer_idx (int): The index of the layer to extract from\n","    - row_idx (int): The index of the token row to extract (1-based if excluding class token)\n","    - head_idx (int): The index of the attention head to extract\n","\n","    Returns:\n","    - extracted_attn (Tensor): Extracted attention matrix for the specified layer, row, and head,\n","      with the class token removed\n","    \"\"\"\n","\n","    # Step 1: Extract the attention matrix for the specified layer\n","    attn_layer = att_mat[layer_idx]  # Shape: [1, num_heads, num_tokens, num_tokens]\n","\n","    # Remove the batch dimension\n","    attn_layer = attn_layer.squeeze(0)  # Shape: [num_heads, num_tokens, num_tokens]\n","    print(f\"Shape of attention matrix for layer {layer_idx} after squeezing: {attn_layer.shape}\")\n","\n","    # Step 2: Extract the attention matrix for the specific head\n","    attn_head = attn_layer[head_idx]  # Shape: [num_tokens, num_tokens]\n","    print(f\"Shape after extracting attention for head {head_idx}: {attn_head.shape}\")\n","\n","    # Step 3: Remove the class token (first token)\n","    attn_no_cls = attn_head[1:, 1:]  # Shape: [n_patches, n_patches]\n","\n","    # Step 4: Extract attention corresponding to a specific row (token), adjusting for 1-based index\n","    extracted_attn = attn_no_cls[row_idx - 1, :]  # Shape: [n_patches]\n","\n","\n","    # Optional normalization for visualization purposes\n","    extracted_attn = extracted_attn / extracted_attn.max()\n","\n","    return extracted_attn\n"]},{"cell_type":"code","execution_count":132,"id":"b6f5d766","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of attention matrix for layer 1 after squeezing: torch.Size([12, 197, 197])\n","Shape after extracting attention for head 2: torch.Size([197, 197])\n","torch.Size([196])\n"]}],"source":["attn=extract_attention_matrix_for_layer(att_mat,layer_idx=1,row_idx=2,head_idx=2)\n","\n","print(attn.shape)"]},{"cell_type":"markdown","id":"e66767f5","metadata":{},"source":["## Visualisation and Heatmaps plotting Code"]},{"cell_type":"code","execution_count":168,"id":"5092f92b","metadata":{},"outputs":[],"source":["import skimage as ski\n","\n","def get_threshold(attn_grid,threshold,quantile_value=0.9):\n","    if threshold == 'mean':\n","        threshold = attn_grid.mean()\n","    elif threshold == 'median':\n","        threshold = attn_grid.median()\n","    elif threshold == 'decile':\n","        threshold = torch.quantile(attn_grid, 0.9)\n","    elif threshold == 'otsu':\n","        # maximise interclass seperation\n","        # threshold, _ = cv2.threshold(attn_grid.cpu().numpy(), 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        threshold = ski.filters.threshold_otsu(attn_row.cpu().numpy())\n","        # attn_grid = torch.tensor(binary_map, dtype=torch.float32)\n","    elif threshold == 'other':\n","        # Adjust quantile_value to get the top x% where x = quantile_value * 100\n","        threshold = torch.quantile(attn_grid, quantile_value)\n","    else:\n","        threshold = threshold # You can adjust this threshold as needed\n","    return threshold"]},{"cell_type":"code","execution_count":169,"id":"f1735871","metadata":{},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","\n","def plot_attn_heatmap(attn_row, q_idx, binary=False, threshold = 0.5, figsize = (4, 4),normalize_flag = False, ax=None, overlay = False,quantile_value=0.9):\n","    # Ensure the tensor is of shape (65,)\n","    assert attn_row.shape == (196,), \"attn_row should have a shape of (65,)\"\n","    \n","    # Remove the first element (corresponding to the [CLS] token if present)\n","    attn_grid = attn_row[1:]\n","    \n","    # Scaling of all values in attn_grid\n","    if normalize_flag:\n","        print(\"normalized\")\n","        attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n","    \n","    # attn_grid += \n","    # Apply binary thresholding if specified\n","    if binary:\n","        threshold = get_threshold(attn_grid, threshold,quantile_value)        \n","        attn_grid = torch.where(attn_grid >= threshold, 1.0, 0.0)\n","    \n","    # Reshape to 8x8\n","    attn_grid = attn_grid.view(15, 13)\n","\n","    query_coord = ((q_idx-1) // 15, (q_idx-1) % 13)\n","    \n","    # Define custom colormap: red, yellow, and blue (only applies if not binary)\n","    cmap = ListedColormap(['red', 'yellow', 'blue']) if not binary else ListedColormap(['blue', 'red'])\n","    \n","    # Define boundaries for the colormap (only applies if not binary)\n","    bounds = [0, 0.33, 0.66, 1.0] if not binary else [0, threshold, 1.0]\n","    \n","    # Define normalization between 0 and 1 using boundaries\n","    norm = BoundaryNorm(bounds, cmap.N)\n","\n","    # Plot the heatmap\n","    # plt.figure(figsize=figsize)\n","    ax_flag = True\n","\n","    if ax is None:\n","        plt.figure(figsize=figsize)\n","        ax_flag = False\n","        ax = plt.gca()  # Get the current axis if not provided\n","        # print(\"rescaled\")\n","\n","    if overlay and img is not None:\n","        # Remove the batch dimension\n","        # img = np.transpose(img, (1, 2, 0))  # Convert to HxWxC format\n","        attn_grid = attn_grid.cpu().numpy()\n","        target_size = img.shape[1:3]\n","        upsampled_attn_grid = cv2.resize(attn_grid, target_size, interpolation=cv2.INTER_NEAREST)\n","        mask = np.expand_dims(upsampled_attn_grid, axis=-1)  # Shape: (64, 64, 1)\n","        masked_img = img.squeeze(0) * mask  # Shape: (64, 64, 3)\n","        target_size = (attn_grid.shape[0], attn_grid.shape[1])\n","\n","        masked_img = np.clip(masked_img, a_min=0, a_max=1)\n","\n","        ax.imshow(masked_img)\n","    else:\n","        sns.heatmap(attn_grid.cpu().numpy(), annot=False, cmap=cmap, norm=norm, cbar=not(ax_flag), ax=ax, linewidths=0.5, linecolor='black')\n","        \n","    # Placing a black dot at the query position\n","    x = query_coord[1] + 0.5\n","    y  = query_coord[0] + 0.5\n","    x *= 4\n","    y *= 4\n","    \n","    ax.scatter(x,y , color='black', s=100, edgecolor='white', zorder=2)\n","    \n","    # Set title only if not using an existing axis\n","    if ax_flag is False:\n","        plt.title(\"Attention Heatmap\")\n","        plt.show()\n","    return threshold"]},{"cell_type":"code","execution_count":null,"id":"e9329a03","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":172,"id":"2257b22b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","28\n","torch.Size([196])\n","normalized\n"]},{"ename":"RuntimeError","evalue":"Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[172], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_row\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='mean')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='median')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='decile')\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[43mplot_attn_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43motsu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43moverlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mquantile_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# threshold = plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='otsu', figsize = (4,4), normalize_flag=False)\u001b[39;00m\n","Cell \u001b[0;32mIn[169], line 21\u001b[0m, in \u001b[0;36mplot_attn_heatmap\u001b[0;34m(attn_row, q_idx, binary, threshold, figsize, normalize_flag, ax, overlay, quantile_value)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# attn_grid += \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply binary thresholding if specified\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m---> 21\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[43mget_threshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquantile_value\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     22\u001b[0m     attn_grid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(attn_grid \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Reshape to 8x8\u001b[39;00m\n","Cell \u001b[0;32mIn[168], line 13\u001b[0m, in \u001b[0;36mget_threshold\u001b[0;34m(attn_grid, threshold, quantile_value)\u001b[0m\n\u001b[1;32m      9\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantile(attn_grid, \u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m threshold \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124motsu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# maximise interclass seperation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# threshold, _ = cv2.threshold(attn_grid.cpu().numpy(), 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m ski\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mthreshold_otsu(\u001b[43mattn_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# attn_grid = torch.tensor(binary_map, dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m threshold \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Adjust quantile_value to get the top x% where x = quantile_value * 100\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}],"source":["# Example usage\n","# attn_row = torch.rand(65)  # Example attention row\n","\n","head_idx=0\n","print(head_idx)\n","#frog\n","q_idx=28\n","print(q_idx)\n","layer_idx=5\n","attn_row = attn\n","print(attn_row.shape)\n","# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='mean')\n","# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='median')\n","# plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='decile')\n","threshold = plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='otsu', figsize = (4,4), normalize_flag=True,overlay=True,quantile_value=0)\n","# threshold = plot_attn_heatmap(attn_row,q_idx, binary=True, threshold='otsu', figsize = (4,4), normalize_flag=False)"]},{"cell_type":"markdown","id":"9b8edd13","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"id":"90b00ef5","metadata":{},"outputs":[{"ename":"EOFError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n","\u001b[0;31mEOFError\u001b[0m: "]}],"source":["raise EOFError"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # pretrained_vit\n","# def load_model_state_dict(model, relative_path_to_checkpoints=\"../Pretrained_checkpoints\", filename=\"model_state.pth\"):\n","#     \"\"\"\n","#     Load the state dictionary of a pretrained model from a sibling directory.\n","\n","#     Parameters:\n","#         model: The model object to load the state dictionary into.\n","#         relative_path_to_checkpoints: Relative path from the current directory to the checkpoint directory (default: '../Pretrained_checkpoints').\n","#         filename: The name of the checkpoint file (default: 'model_state.pth').\n","\n","#     Returns:\n","#         None\n","#     \"\"\"\n","#     # Create the full path relative to the current directory\n","#     file_path = os.path.join(relative_path_to_checkpoints, filename)\n","    \n","#     # Load the state dictionary\n","#     # Load the state dictionary\n","#     # print(file_path)\n","#     model_state_dict = torch.load(file_path, map_location='cpu', weights_only=True)\n","#     model.load_state_dict(model_state_dict)\n","#     print(f\"Model loaded from {file_path}\")\n"]},{"cell_type":"code","execution_count":null,"id":"1b502022","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# checkpoint_dir = '10 checkpoints'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# checkpoint_dir='10.8 checkpoints 8 head'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# base_dir=\"/path to you base dir\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# checkpoint_dir=base_dir+'/'+checkpoint_dir\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m load_model_state_dict(\u001b[43mmodel\u001b[49m, relative_path_to_checkpoints\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Pretrained_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["# # checkpoint_dir = '10 checkpoints'\n","# # checkpoint_dir='10.8 checkpoints 8 head'\n","\n","# # checkpoint_dir=\"Pretrained_checkpoints\"\n","# # filename = 'model_epoch_90.pth'\n","# # filename = 'model_state_001.pth'\n","# # 10 checkpoints\n","# # base_dir=\"/Users/yajatkapoor/Desktop/IITD SEMESTER-7/ELD431/BTP-Transformer-explainability\"\n","# # base_dir=\"/path to you base dir\"\n","# # checkpoint_dir=base_dir+'/'+checkpoint_dir\n","# load_model_state_dict(model, relative_path_to_checkpoints=\"../Pretrained_checkpoints\", filename=\"model_state_dict.pth\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"45a65cda-db08-441c-9f60-cf79138e029d"},"source":["Then we'll setup device-agnostic code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.208976Z","iopub.status.busy":"2024-09-22T16:13:04.208621Z","iopub.status.idle":"2024-09-22T16:13:04.220965Z","shell.execute_reply":"2024-09-22T16:13:04.220131Z","shell.execute_reply.started":"2024-09-22T16:13:04.208944Z"},"id":"b0b87f68-98cc-49f8-89bd-ff220a757f76","trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.222298Z","iopub.status.busy":"2024-09-22T16:13:04.222033Z","iopub.status.idle":"2024-09-22T16:13:04.230318Z","shell.execute_reply":"2024-09-22T16:13:04.229492Z","shell.execute_reply.started":"2024-09-22T16:13:04.222268Z"},"id":"KVorL4SeZ3vd","trusted":true},"outputs":[],"source":["class_names=['airplane',\n","  'automobile',\n","  'bird',\n","  'cat',\n","  'deer',\n","  'dog',\n","  'frog',\n","  'horse',\n","  'ship',\n","  'truck']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.231984Z","iopub.status.busy":"2024-09-22T16:13:04.231642Z","iopub.status.idle":"2024-09-22T16:13:04.241926Z","shell.execute_reply":"2024-09-22T16:13:04.240751Z","shell.execute_reply.started":"2024-09-22T16:13:04.231953Z"},"trusted":true},"outputs":[{"data":{"text/plain":["10"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["num_classes = len(class_names)\n","num_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:13:04.243300Z","iopub.status.busy":"2024-09-22T16:13:04.243016Z","iopub.status.idle":"2024-09-22T16:13:04.250553Z","shell.execute_reply":"2024-09-22T16:13:04.249714Z","shell.execute_reply.started":"2024-09-22T16:13:04.243270Z"},"trusted":true},"outputs":[],"source":["config = CONFIGS[\"ViT-B_16\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:14:26.546844Z","iopub.status.busy":"2024-09-22T16:14:26.545997Z","iopub.status.idle":"2024-09-22T16:14:28.612338Z","shell.execute_reply":"2024-09-22T16:14:28.611561Z","shell.execute_reply.started":"2024-09-22T16:14:26.546798Z"},"id":"b8e2dda6-8af0-4255-815f-4d885fa4b477","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model loaded for trianing is successful!\n"]}],"source":["# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n","\n","def get_vit():\n","  \n","    pretrained_vit = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=False)\n","    pretrained_vit.load_from(np.load(\"weights/ViT-B_16-224.npz\"))\n","\n","\n","    pretrained_vit.train()\n","    print('model loaded for trianing is successful!')\n","\n","    # 3. Freeze the base parameters\n","    for parameter in pretrained_vit.parameters():\n","      parameter.requires_grad = False\n","\n","    # 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n","    # set_seeds()\n","    torch.manual_seed(42)\n","\n","    import torch.nn as nn\n","    pretrained_vit.head = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n","    # pretrained_vit # uncomment for model output\n","    return pretrained_vit.to(device)\n","\n","pretrained_vit = get_vit()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:14:28.614354Z","iopub.status.busy":"2024-09-22T16:14:28.614008Z","iopub.status.idle":"2024-09-22T16:14:28.621539Z","shell.execute_reply":"2024-09-22T16:14:28.620754Z","shell.execute_reply.started":"2024-09-22T16:14:28.614312Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VisionTransformer(\n","  (transformer): Transformer(\n","    (embeddings): Embeddings(\n","      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): Encoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x Block(\n","          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (attn): Attention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (out): Linear(in_features=768, out_features=768, bias=True)\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (proj_dropout): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","        )\n","      )\n","      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    )\n","  )\n","  (head): Linear(in_features=768, out_features=10, bias=True)\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["pretrained_vit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:20.175228Z","iopub.status.busy":"2024-09-22T16:15:20.174261Z","iopub.status.idle":"2024-09-22T16:15:20.181234Z","shell.execute_reply":"2024-09-22T16:15:20.180256Z","shell.execute_reply.started":"2024-09-22T16:15:20.175183Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Linear(in_features=768, out_features=10, bias=True)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["pretrained_vit.head"]},{"cell_type":"markdown","metadata":{"id":"182fc970-1650-48b3-914d-0cb3e287beec"},"source":["Pretrained ViT feature extractor model created!\n","\n","Let's now check it out by printing a `torchinfo.summary()`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:20.183268Z","iopub.status.busy":"2024-09-22T16:15:20.182935Z","iopub.status.idle":"2024-09-22T16:15:20.195901Z","shell.execute_reply":"2024-09-22T16:15:20.194948Z","shell.execute_reply.started":"2024-09-22T16:15:20.183237Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VisionTransformer(\n","  (transformer): Transformer(\n","    (embeddings): Embeddings(\n","      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): Encoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x Block(\n","          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (ffn): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (attn): Attention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (out): Linear(in_features=768, out_features=768, bias=True)\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (proj_dropout): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","        )\n","      )\n","      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    )\n","  )\n","  (head): Linear(in_features=768, out_features=10, bias=True)\n",")"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["pretrained_vit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:15:20.816843Z","iopub.status.busy":"2024-09-22T16:15:20.816172Z","iopub.status.idle":"2024-09-22T16:15:21.832716Z","shell.execute_reply":"2024-09-22T16:15:21.831650Z","shell.execute_reply.started":"2024-09-22T16:15:20.816801Z"},"id":"8fbd83a1","outputId":"e0f66c3a-dc38-4124-8942-5e6dde5021e0","trusted":true},"outputs":[{"data":{"text/plain":["=======================================================================================================================================\n","Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n","=======================================================================================================================================\n","VisionTransformer (VisionTransformer)                   [32, 3, 224, 224]    [32, 10]             --                   Partial\n","├─Transformer (transformer)                             [32, 3, 224, 224]    [32, 197, 768]       --                   False\n","│    └─Embeddings (embeddings)                          [32, 3, 224, 224]    [32, 197, 768]       152,064              False\n","│    │    └─Conv2d (patch_embeddings)                   [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n","│    │    └─Dropout (dropout)                           [32, 197, 768]       [32, 197, 768]       --                   --\n","│    └─Encoder (encoder)                                [32, 197, 768]       [32, 197, 768]       --                   False\n","│    │    └─ModuleList (layer)                          --                   --                   (85,054,464)         False\n","│    │    └─LayerNorm (encoder_norm)                    [32, 197, 768]       [32, 197, 768]       (1,536)              False\n","├─Linear (head)                                         [32, 768]            [32, 10]             7,690                True\n","=======================================================================================================================================\n","Total params: 85,806,346\n","Trainable params: 7,690\n","Non-trainable params: 85,798,656\n","Total mult-adds (G): 6.43\n","=======================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 5189.86\n","Params size (MB): 342.62\n","Estimated Total Size (MB): 5551.75\n","======================================================================================================================================="]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Print a summary using torchinfo (uncomment for actual output)\n","summary(model=pretrained_vit,\n","        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"90c176e5-6453-4911-b8ec-97bab43b437d"},"source":["<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-summary-output-pytorch-vit.png\" alt=\"output of pytorch pretrained ViT model summary\" width=900 />\n","\n","Woohoo!\n","\n","Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).\n","\n","And the total number of parameters, 85,800,963, is the same as our custom made ViT model above.\n","\n","But the number of trainable parameters for `pretrained_vit` is much, much lower than our custom `vit` at only 2,307 compared to 85,800,963 (in our custom `vit`, since we're training from scratch, all parameters are trainable).\n","\n","This means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory."]},{"cell_type":"markdown","metadata":{"id":"a50dfe1f-a475-473d-bc23-ef3c58ba4854"},"source":["### 10.3 Preparing data for the pretrained ViT model\n","\n","We downloaded and created DataLoaders for our own ViT model back in section 2.\n","\n","So we don't necessarily need to do it again.\n","\n","But in the name of practice, let's download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.\n","\n","We can download pizza, steak and sushi images from the course GitHub and the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data).\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:21.835909Z","iopub.status.busy":"2024-09-22T16:15:21.835042Z","iopub.status.idle":"2024-09-22T16:15:21.840140Z","shell.execute_reply":"2024-09-22T16:15:21.839011Z","shell.execute_reply.started":"2024-09-22T16:15:21.835857Z"},"id":"94cb3900","trusted":true},"outputs":[],"source":["# from helper_functions import download_data\n","\n","# # Download pizza, steak, sushi images from GitHub\n","# image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","#                            destination=\"pizza_steak_sushi\")\n","\n","# image_path"]},{"cell_type":"markdown","metadata":{"id":"4696fecb-cd74-41ca-b1f7-02bbaf7f8ed3"},"source":["And now we'll setup the training and test directory paths."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:21.842000Z","iopub.status.busy":"2024-09-22T16:15:21.841595Z","iopub.status.idle":"2024-09-22T16:15:21.853002Z","shell.execute_reply":"2024-09-22T16:15:21.852005Z","shell.execute_reply.started":"2024-09-22T16:15:21.841955Z"},"id":"2e6ae0fe-73c0-4930-988a-e4df903084b6","trusted":true},"outputs":[],"source":["# # Setup train and test directory paths\n","# train_dir = image_path / \"train\"\n","# test_dir = image_path / \"test\"\n","# train_dir, test_dir"]},{"cell_type":"markdown","metadata":{"id":"c8736ad3-f510-4418-8c8e-f6cc3f2e1788"},"source":["Finally, we'll transform our images into tensors and turn the tensors into DataLoaders.\n","\n","Since we're using a pretrained model from `torchvision.models` we can call the `transforms()` method on it to get its required transforms.\n","\n","Remember, if you're going to use a pretrained model, it's generally important to **ensure your own custom data is transformed/formatted in the same way the data the original model was trained on**.\n","\n","We covered this method of \"automatic\" transform creation in [06. PyTorch Transfer Learning section 2.2](https://www.learnpytorch.io/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:15:21.856346Z","iopub.status.busy":"2024-09-22T16:15:21.856004Z","iopub.status.idle":"2024-09-22T16:15:21.864545Z","shell.execute_reply":"2024-09-22T16:15:21.863692Z","shell.execute_reply.started":"2024-09-22T16:15:21.856313Z"},"id":"6f48d40b-11f6-4e74-8503-cc29e073140e","outputId":"6465f95a-df14-4874-88cc-9c6b9f8731be","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ImageClassification(\n","    crop_size=[224]\n","    resize_size=[256]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n"]}],"source":["# Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","print(pretrained_vit_transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:15:21.866036Z","iopub.status.busy":"2024-09-22T16:15:21.865689Z","iopub.status.idle":"2024-09-22T16:15:30.345489Z","shell.execute_reply":"2024-09-22T16:15:30.344439Z","shell.execute_reply.started":"2024-09-22T16:15:21.866005Z"},"id":"DCMo22fMYy-7","outputId":"3169fd06-9d46-4dbc-e92e-77e8dd4622c7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar-10/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:04<00:00, 35270139.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting cifar-10/cifar-10-python.tar.gz to cifar-10\n","Files already downloaded and verified\n"]}],"source":["\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","train_data = datasets.CIFAR10(root='cifar-10', train=True, download=True, transform=pretrained_vit_transforms)\n","test_data = datasets.CIFAR10(root='cifar-10', train=False, download=True, transform=pretrained_vit_transforms)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:15:30.347093Z","iopub.status.busy":"2024-09-22T16:15:30.346774Z","iopub.status.idle":"2024-09-22T16:15:30.351065Z","shell.execute_reply":"2024-09-22T16:15:30.350174Z","shell.execute_reply.started":"2024-09-22T16:15:30.347061Z"},"id":"34WmsqJZ4rcC","outputId":"1ae79f46-5fea-452b-cd3b-39cb310518d6","trusted":true},"outputs":[],"source":["\n","# len(test_dataloader_pretrained)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVVnPNht5Ak-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TrhxAgR5A50"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-09-22T16:15:30.352691Z","iopub.status.busy":"2024-09-22T16:15:30.352324Z","iopub.status.idle":"2024-09-22T16:15:30.365244Z","shell.execute_reply":"2024-09-22T16:15:30.364410Z","shell.execute_reply.started":"2024-09-22T16:15:30.352656Z"},"id":"g5z3SyrRYzcl","outputId":"451d1644-b033-460f-84c9-f7371371263a","trusted":true},"outputs":[{"data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7ebe6f891ab0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7ebe6f891540>,\n"," ['airplane',\n","  'automobile',\n","  'bird',\n","  'cat',\n","  'deer',\n","  'dog',\n","  'frog',\n","  'horse',\n","  'ship',\n","  'truck'])"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["train_dataloader_pretrained = DataLoader(train_data, batch_size=32, shuffle=True)\n","test_dataloader_pretrained = DataLoader(test_data, batch_size=32, shuffle=False)\n","\n","# Get class names\n","class_names = train_data.classes\n","\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"execution":{"iopub.execute_input":"2024-09-22T16:15:30.366560Z","iopub.status.busy":"2024-09-22T16:15:30.366288Z","iopub.status.idle":"2024-09-22T16:15:30.374266Z","shell.execute_reply":"2024-09-22T16:15:30.373568Z","shell.execute_reply.started":"2024-09-22T16:15:30.366532Z"},"id":"tW4kYFrj5Bbe","outputId":"e6659d66-8bcb-4fb9-a4bf-e7b319882267","trusted":true},"outputs":[],"source":["\n","# train_dataloader_pretrained.device"]},{"cell_type":"markdown","metadata":{"id":"76244403-6d3b-472f-a4f0-ccbaa3dfd764"},"source":["And now we've got transforms ready, we can turn our images into DataLoaders using the `data_setup.create_dataloaders()` method we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n","\n","Since we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicking an improvement found in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580), a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"execution":{"iopub.execute_input":"2024-09-22T16:15:30.375582Z","iopub.status.busy":"2024-09-22T16:15:30.375245Z","iopub.status.idle":"2024-09-22T16:15:30.383686Z","shell.execute_reply":"2024-09-22T16:15:30.382903Z","shell.execute_reply.started":"2024-09-22T16:15:30.375538Z"},"id":"dd2f58ff-6182-453a-a802-70ff98c09557","outputId":"0076dd5f-4a28-494d-fe5a-e3e64eabb66d","trusted":true},"outputs":[],"source":["# # Setup dataloaders\n","# train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n","#                                                                                                      test_dir=test_dir,\n","#                                                                                                      transform=pretrained_vit_transforms,\n","#                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"]},{"cell_type":"markdown","metadata":{"id":"4e9da731-3c11-4d79-9e68-f006fcedd288"},"source":["### 10.4 Train feature extractor ViT model\n","\n","Feature extractor model ready, DataLoaders ready, time to train!\n","\n","As before we'll use the Adam optimizer (`torch.optim.Adam()`) with a learning rate of `1e-3` and `torch.nn.CrossEntropyLoss()` as the loss function.\n","\n","Our `engine.train()` function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) will take care of the rest."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:30.387594Z","iopub.status.busy":"2024-09-22T16:15:30.387172Z","iopub.status.idle":"2024-09-22T16:15:30.394842Z","shell.execute_reply":"2024-09-22T16:15:30.394001Z","shell.execute_reply.started":"2024-09-22T16:15:30.387540Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:30.396036Z","iopub.status.busy":"2024-09-22T16:15:30.395770Z","iopub.status.idle":"2024-09-22T16:15:30.403026Z","shell.execute_reply":"2024-09-22T16:15:30.402178Z","shell.execute_reply.started":"2024-09-22T16:15:30.396006Z"},"trusted":true},"outputs":[],"source":["# !mkdir model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:30.404392Z","iopub.status.busy":"2024-09-22T16:15:30.404039Z","iopub.status.idle":"2024-09-22T16:15:31.434951Z","shell.execute_reply":"2024-09-22T16:15:31.433672Z","shell.execute_reply.started":"2024-09-22T16:15:30.404351Z"},"trusted":true},"outputs":[],"source":["!mkdir model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.437715Z","iopub.status.busy":"2024-09-22T16:15:31.436866Z","iopub.status.idle":"2024-09-22T16:15:31.442107Z","shell.execute_reply":"2024-09-22T16:15:31.441095Z","shell.execute_reply.started":"2024-09-22T16:15:31.437663Z"},"trusted":true},"outputs":[],"source":["k1 = None\n","k2 = None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.444552Z","iopub.status.busy":"2024-09-22T16:15:31.443961Z","iopub.status.idle":"2024-09-22T16:15:31.471129Z","shell.execute_reply":"2024-09-22T16:15:31.470196Z","shell.execute_reply.started":"2024-09-22T16:15:31.444500Z"},"id":"PIs-NogO3oAR","trusted":true},"outputs":[],"source":["\n","import torch\n","\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","def train_step(model: torch.nn.Module, \n","               dataloader: torch.utils.data.DataLoader, \n","               loss_fn: torch.nn.Module, \n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","  \"\"\"Trains a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to training mode and then\n","  runs through all of the required training steps (forward\n","  pass, loss calculation, optimizer step).\n","\n","  Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","  \"\"\"\n","  # Put model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss, train_acc = 0, 0\n","\n","  # Loop through data loader data batches\n","  for batch, (X, y) in tqdm(enumerate(dataloader),\"training batches\",total=len(dataloader),):\n","      # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","\n","      # 1. Forward pass\n","      global k1, k2\n","      y_pred,_ = model(X)\n","      k1 = y_pred\n","      k2 = y\n","#       y_pred = torch.tensor(y_pred).to(device)\n","      \n","\n","      # 2. Calculate  and accumulate loss\n","      loss = loss_fn(y_pred, y)\n","      train_loss += loss.item() \n","\n","      # 3. Optimizer zero grad\n","      optimizer.zero_grad()\n","\n","      # 4. Loss backward\n","      loss.backward()\n","\n","      # 5. Optimizer step\n","      optimizer.step()\n","\n","      # Calculate and accumulate accuracy metric across all batches\n","      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","  # Adjust metrics to get average loss and accuracy per batch \n","  train_loss = train_loss / len(dataloader)\n","  train_acc = train_acc / len(dataloader)\n","  return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module, \n","              dataloader: torch.utils.data.DataLoader, \n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","  \"\"\"Tests a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to \"eval\" mode and then performs\n","  a forward pass on a testing dataset.\n","\n","  Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","  \"\"\"\n","  # Put model in eval mode\n","  model.eval() \n","\n","  # Setup test loss and test accuracy values\n","  test_loss, test_acc = 0, 0\n","\n","  # Turn on inference context manager\n","  with torch.inference_mode():\n","      # Loop through DataLoader batches\n","      for batch, (X, y) in tqdm(enumerate(dataloader),\"testing batches\",total=len(dataloader),):\n","          # Send data to target device\n","          X, y = X.to(device), y.to(device)\n","\n","          # 1. Forward pass\n","          test_pred_logits,_ = model(X)\n","\n","          # 2. Calculate and accumulate loss\n","          loss = loss_fn(test_pred_logits, y)\n","          test_loss += loss.item()\n","\n","          # Calculate and accumulate accuracy\n","          test_pred_labels = test_pred_logits.argmax(dim=1)\n","          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","  # Adjust metrics to get average loss and accuracy per batch \n","  test_loss = test_loss / len(dataloader)\n","  test_acc = test_acc / len(dataloader)\n","  return test_loss, test_acc\n","\n","def train(model: torch.nn.Module, \n","          train_dataloader: torch.utils.data.DataLoader, \n","          test_dataloader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List]:\n","  print(\"Working on \",device)\n","  \"\"\"Trains and tests a PyTorch model.\n","  \n","\n","  Passes a target PyTorch models through train_step() and test_step()\n","  functions for a number of epochs, training and testing the model\n","  in the same epoch loop.\n","\n","  Calculates, prints and stores evaluation metrics throughout.\n","\n","  Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for \n","    each epoch.\n","    In the form: {train_loss: [...],\n","                  train_acc: [...],\n","                  test_loss: [...],\n","                  test_acc: [...]} \n","    For example if training for epochs=2: \n","                 {train_loss: [2.0616, 1.0537],\n","                  train_acc: [0.3945, 0.3945],\n","                  test_loss: [1.2641, 1.5706],\n","                  test_acc: [0.3400, 0.2973]} \n","  \"\"\"\n","  # Create empty results dictionary\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": []\n","  }\n","\n","  # Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs),'training epochs'):\n","    train_loss, train_acc = 0,0\n","    \n","    train_loss, train_acc = train_step(model=model,\n","                                      dataloader=train_dataloader,\n","                                      loss_fn=loss_fn,\n","                                      optimizer=optimizer,\n","                                      device=device)\n","\n","    \n","    print(\"trained \")\n","    test_loss, test_acc = test_step(model=model,\n","      dataloader=test_dataloader,\n","      loss_fn=loss_fn,\n","      device=device)\n","\n","    # Print out what's happening\n","    print(\n","      f\"Epoch: {epoch+1} | \"\n","      f\"train_loss: {train_loss:.4f} | \"\n","      f\"train_acc: {train_acc:.4f} | \"\n","      f\"test_loss: {test_loss:.4f} | \"\n","      f\"test_acc: {test_acc:.4f}\"\n","    )\n","\n","    # Update results dictionary\n","    results[\"train_loss\"].append(train_loss)\n","    results[\"train_acc\"].append(train_acc)\n","    results[\"test_loss\"].append(test_loss)\n","    results[\"test_acc\"].append(test_acc)\n","    if epoch % 1 == 0 :\n","        torch.save(model.state_dict(),f\"model/model_state_{epoch:03}.pth\")\n","        torch.save(optimizer.state_dict(),f\"model/optimizer_state{epoch:03}.pth\")\n","\n","  # Return the filled results at the end of the epochs\n","  return results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.472634Z","iopub.status.busy":"2024-09-22T16:15:31.472256Z","iopub.status.idle":"2024-09-22T16:15:31.485345Z","shell.execute_reply":"2024-09-22T16:15:31.484489Z","shell.execute_reply.started":"2024-09-22T16:15:31.472586Z"},"trusted":true},"outputs":[],"source":["# x = 4\n","# print(f'{epoch:03}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.486788Z","iopub.status.busy":"2024-09-22T16:15:31.486494Z","iopub.status.idle":"2024-09-22T16:15:31.494738Z","shell.execute_reply":"2024-09-22T16:15:31.493843Z","shell.execute_reply.started":"2024-09-22T16:15:31.486758Z"},"trusted":true},"outputs":[],"source":["loss_fn = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.496347Z","iopub.status.busy":"2024-09-22T16:15:31.495950Z","iopub.status.idle":"2024-09-22T16:15:31.505746Z","shell.execute_reply":"2024-09-22T16:15:31.504801Z","shell.execute_reply.started":"2024-09-22T16:15:31.496296Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.507647Z","iopub.status.busy":"2024-09-22T16:15:31.507018Z","iopub.status.idle":"2024-09-22T16:15:31.515469Z","shell.execute_reply":"2024-09-22T16:15:31.514670Z","shell.execute_reply.started":"2024-09-22T16:15:31.507604Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# pretrained_vit = pretrained_vit.to(device)\n","(next(pretrained_vit.parameters()).device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:15:31.516930Z","iopub.status.busy":"2024-09-22T16:15:31.516606Z","iopub.status.idle":"2024-09-22T16:15:31.526594Z","shell.execute_reply":"2024-09-22T16:15:31.525717Z","shell.execute_reply.started":"2024-09-22T16:15:31.516899Z"},"trusted":true},"outputs":[{"data":{"text/plain":["NoneType"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["type(k1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["d5d476e6c7a142cc816f45928b1429d5","f6241074a580437db318bd57ad9c31aa","91042fb52d2d49eba35ccb997dd81190","9323a29f5fe944f689ee72198398520a","b4f398b7f66c4c6490419a2c35232d67","73a7dbd60deb4700bafd0696ce2c7ccc","5d67f490ed464d13a4d894950632f15a","57e73146134e4120a7f30869f1734d05","50c7f4d40bb04eafa57ffcc8b0c38261","061fc5d314804961915d9dec8a37e859","6961abee91ab44c3bbdd9e1d72d80dbf","ae0fdfd782d844fc9699481dd8334e0f","e2faa2e3716144feb79747a1bcfaaeed","54edf2049515415a86af5c76594a4b7e","c8b33e773bea4164a5aaa563f3057beb","bfe0a1770579416f8ce81e599565d055","66df007960cd46e8b635175768d1a634","7d352baf44ab44a6bf22e2ecdaade75c","43438a05ce314feb8141b9416e5ea103","db4394bd3fbe4369aa5809ba136a85f5","147fd898dba54f5c85766ea6271bd439","379536dbe5a74422920fa97c113f396b"]},"execution":{"iopub.execute_input":"2024-09-22T16:15:31.528051Z","iopub.status.busy":"2024-09-22T16:15:31.527694Z","iopub.status.idle":"2024-09-22T16:28:59.096567Z","shell.execute_reply":"2024-09-22T16:28:59.095598Z","shell.execute_reply.started":"2024-09-22T16:15:31.528002Z"},"id":"a49408b4-24d9-4bb1-90a2-dd61c08f78a4","outputId":"2380b549-b545-43b8-b5c2-f80ca2c768d5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model loaded for trianing is successful!\n","Working on  cuda\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b8e1dedd94c4870b6c1e91ae00bff41","version_major":2,"version_minor":0},"text/plain":["training epochs:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04814edd208a41c68b2292642f70c08e","version_major":2,"version_minor":0},"text/plain":["training batches:   0%|          | 0/1563 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["trained \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60195aada66b49ca90a9aa173657a046","version_major":2,"version_minor":0},"text/plain":["testing batches:   0%|          | 0/313 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 0.2693 | train_acc: 0.9132 | test_loss: 0.2182 | test_acc: 0.9286\n"]}],"source":["def reset():\n","    \"\"\"Reset the model and optimizer.\"\"\"\n","    torch.manual_seed(42)\n","    \n","    model = get_vit()\n","    \n","    # Create optimizer and loss function\n","    optimizer = torch.optim.Adam(params=model.parameters(),\n","                                 lr=1e-3)\n","    \n","    return model, optimizer\n","\n","NUM_EPOCHS = 1\n","\n","pretrained_vit, optimizer = reset()\n","# Train the classifier head of the pretrained ViT feature extractor model\n","\n","pretrained_vit_results = train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=NUM_EPOCHS,\n","                                      device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:28:59.098226Z","iopub.status.busy":"2024-09-22T16:28:59.097915Z","iopub.status.idle":"2024-09-22T16:28:59.103296Z","shell.execute_reply":"2024-09-22T16:28:59.102268Z","shell.execute_reply.started":"2024-09-22T16:28:59.098194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model trained\n"]}],"source":["print(\"model trained\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:29:06.636312Z","iopub.status.busy":"2024-09-22T16:29:06.635900Z","iopub.status.idle":"2024-09-22T16:29:06.640643Z","shell.execute_reply":"2024-09-22T16:29:06.639599Z","shell.execute_reply.started":"2024-09-22T16:29:06.636271Z"},"trusted":true},"outputs":[],"source":["# raise er"]},{"cell_type":"markdown","metadata":{"id":"233717e4-9983-47ed-9ef2-5a079df9a971"},"source":["### 10.5 Plot feature extractor ViT model loss curves\n","\n","Our pretrained ViT feature model numbers look good on the training and test sets.\n","\n","How do the loss curves look?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:29:16.094080Z","iopub.status.busy":"2024-09-22T16:29:16.092980Z","iopub.status.idle":"2024-09-22T16:29:17.613539Z","shell.execute_reply":"2024-09-22T16:29:17.612379Z","shell.execute_reply.started":"2024-09-22T16:29:16.094033Z"},"trusted":true},"outputs":[],"source":["!mkdir model_state_dict\n","torch.save(pretrained_vit.state_dict(), \"model_state_dict/model_state_dict.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:29:17.616465Z","iopub.status.busy":"2024-09-22T16:29:17.616114Z","iopub.status.idle":"2024-09-22T16:29:17.627266Z","shell.execute_reply":"2024-09-22T16:29:17.626499Z","shell.execute_reply.started":"2024-09-22T16:29:17.616409Z"},"trusted":true},"outputs":[],"source":["import zipfile\n","import os\n","from IPython.display import FileLink, display\n","\n","# Path to the file or directory to zip\n","# file_to_zip = 'model_new'\n","\n","def get_zip(file_to_zip):\n","    zip_file_name = f'{file_to_zip}_zip.zip'\n","\n","    # Function to zip a directory\n","    def zip_dir(directory, zip_file):\n","        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            for root, dirs, files in os.walk(directory):\n","                for file in files:\n","                    file_path = os.path.join(root, file)\n","                    zipf.write(file_path, os.path.relpath(file_path, directory))\n","\n","    # Zip the file or directory\n","    if os.path.isdir(file_to_zip):\n","        zip_dir(file_to_zip, zip_file_name)\n","    else:\n","        with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n","            zipf.write(file_to_zip, os.path.basename(file_to_zip))\n","\n","\n","    print(\"Zipped\")\n","    # Specify the path to your file\n","    file_path = zip_file_name\n","\n","    # Get the size of the file in bytes\n","    file_size = os.path.getsize(file_path)\n","\n","    # print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n","\n","\n","    # Display the download link\n","    download_link = FileLink(zip_file_name)\n","    display(download_link)\n","    print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:29:17.628710Z","iopub.status.busy":"2024-09-22T16:29:17.628403Z","iopub.status.idle":"2024-09-22T16:29:37.342937Z","shell.execute_reply":"2024-09-22T16:29:37.340500Z","shell.execute_reply.started":"2024-09-22T16:29:17.628680Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Zipped\n"]},{"data":{"text/html":["<a href='model_state_dict_zip.zip' target='_blank'>model_state_dict_zip.zip</a><br>"],"text/plain":["/kaggle/working/model_state_dict_zip.zip"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The size of the file is : 304 MB\n"]}],"source":["get_zip('model_state_dict')\n","# get_zip('opt_state_dict')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:29:37.345698Z","iopub.status.busy":"2024-09-22T16:29:37.345216Z","iopub.status.idle":"2024-09-22T16:29:37.399534Z","shell.execute_reply":"2024-09-22T16:29:37.393652Z","shell.execute_reply.started":"2024-09-22T16:29:37.345648Z"},"trusted":true},"outputs":[{"ename":"EOFError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n","\u001b[0;31mEOFError\u001b[0m: "]}],"source":["raise EOFError"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:36:33.049154Z","iopub.status.busy":"2024-09-22T16:36:33.048411Z","iopub.status.idle":"2024-09-22T16:36:45.903820Z","shell.execute_reply":"2024-09-22T16:36:45.902592Z","shell.execute_reply.started":"2024-09-22T16:36:33.049112Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.17)\n","Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.7.4)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.4)\n","Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\n","Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.7)\n"]}],"source":["!pip install kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:39:41.661122Z","iopub.status.busy":"2024-09-22T16:39:41.660672Z","iopub.status.idle":"2024-09-22T16:39:42.698261Z","shell.execute_reply":"2024-09-22T16:39:42.697155Z","shell.execute_reply.started":"2024-09-22T16:39:41.661080Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BigQuery_Helper\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:39:57.172647Z","iopub.status.busy":"2024-09-22T16:39:57.172111Z","iopub.status.idle":"2024-09-22T16:39:57.181251Z","shell.execute_reply":"2024-09-22T16:39:57.180361Z","shell.execute_reply.started":"2024-09-22T16:39:57.172589Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["kaggle.json file created successfully.\n"]}],"source":["import json\n","\n","# Paste your kaggle.json credentials directly if needed (if not uploaded through UI)\n","kaggle_credentials = {\n","  \"username\": \"asterisk007\",\n","  \"key\": \"e8c8d3b18c96b8705d9726e06b43aa77\"\n","}\n","\n","import os\n","import json\n","\n","\n","\n","# Explicitly create the directory /root/.kaggle\n","kaggle_dir = '/root/.kaggle'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(kaggle_dir, exist_ok=True)\n","\n","# Write the kaggle.json file in /root/.kaggle\n","with open(os.path.join(kaggle_dir, \"kaggle.json\"), \"w\") as f:\n","    json.dump(kaggle_credentials, f)\n","\n","# Optionally set the correct file permissions\n","os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n","\n","print(\"kaggle.json file created successfully.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:42:55.769633Z","iopub.status.busy":"2024-09-22T16:42:55.769219Z","iopub.status.idle":"2024-09-22T16:42:56.803503Z","shell.execute_reply":"2024-09-22T16:42:56.802518Z","shell.execute_reply.started":"2024-09-22T16:42:55.769595Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model_state_dict.pth\n"]}],"source":["!ls model_state_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:44:20.178916Z","iopub.status.busy":"2024-09-22T16:44:20.178502Z","iopub.status.idle":"2024-09-22T16:44:21.215966Z","shell.execute_reply":"2024-09-22T16:44:21.214997Z","shell.execute_reply.started":"2024-09-22T16:44:20.178872Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset-metadata.json  model_state_dict.pth\n"]}],"source":["!ls kaggle_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:44:49.993886Z","iopub.status.busy":"2024-09-22T16:44:49.993433Z","iopub.status.idle":"2024-09-22T16:44:52.065338Z","shell.execute_reply":"2024-09-22T16:44:52.064130Z","shell.execute_reply.started":"2024-09-22T16:44:49.993846Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mv: cannot stat 'model_state_dict/model_state_dict.pth': No such file or directory\n","Done\n"]}],"source":["!mkdir -p kaggle_dataset\n","\n","# Move the model weights into the dataset folder\n","!mv model_state_dict/model_state_dict.pth kaggle_dataset/\n","\n","# Create a metadata file required by Kaggle\n","dataset_metadata = {\n","    \"title\": \"Vit pretrained finetuned on cifar10 with attention\",\n","    \"id\": \"asterisk007/vit-2-cifar10-attn\",  # Customize it to your username and desired dataset name\n","    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n","}\n","\n","# Save the metadata file\n","with open('kaggle_dataset/dataset-metadata.json', 'w') as f:\n","    json.dump(dataset_metadata, f)\n","\n","print(\"Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T16:44:52.990894Z","iopub.status.busy":"2024-09-22T16:44:52.990485Z","iopub.status.idle":"2024-09-22T16:45:00.557765Z","shell.execute_reply":"2024-09-22T16:45:00.556587Z","shell.execute_reply.started":"2024-09-22T16:44:52.990855Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting upload for file model_state_dict.pth\n","100%|████████████████████████████████████████| 327M/327M [00:04<00:00, 84.5MB/s]\n","Upload successful: model_state_dict.pth (327MB)\n","Your public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/asterisk007/vit-2-cifar10-attn\n"]}],"source":["\n","# Use Kaggle API to create the dataset\n","!kaggle datasets create -p kaggle_dataset --dir-mode zip --public"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.401004Z","iopub.status.idle":"2024-09-22T16:29:37.401470Z","shell.execute_reply":"2024-09-22T16:29:37.401274Z","shell.execute_reply.started":"2024-09-22T16:29:37.401254Z"},"trusted":true},"outputs":[],"source":["# get_zip('model_state_dict')\n","# get_zip('opt_state_dict')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.403360Z","iopub.status.idle":"2024-09-22T16:29:37.404303Z","shell.execute_reply":"2024-09-22T16:29:37.404039Z","shell.execute_reply.started":"2024-09-22T16:29:37.404009Z"},"trusted":true},"outputs":[],"source":["# get_zip('model_new/optimizer_state001.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.405636Z","iopub.status.idle":"2024-09-22T16:29:37.406408Z","shell.execute_reply":"2024-09-22T16:29:37.406106Z","shell.execute_reply.started":"2024-09-22T16:29:37.406072Z"},"id":"3c0af18e-6419-4dd6-b8ea-f5830bbd63d5","trusted":true},"outputs":[],"source":["# Plot the loss curves\n","from helper_functions import plot_loss_curves\n","\n","plot_loss_curves(pretrained_vit_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.407923Z","iopub.status.idle":"2024-09-22T16:29:37.408438Z","shell.execute_reply":"2024-09-22T16:29:37.408207Z","shell.execute_reply.started":"2024-09-22T16:29:37.408182Z"},"trusted":true},"outputs":[],"source":["m0 = pretrained_vit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.409982Z","iopub.status.idle":"2024-09-22T16:29:37.410550Z","shell.execute_reply":"2024-09-22T16:29:37.410272Z","shell.execute_reply.started":"2024-09-22T16:29:37.410243Z"},"trusted":true},"outputs":[],"source":["dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","\n","# Pass the dummy input through the model to check\n","output = m0(dummy_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.413810Z","iopub.status.idle":"2024-09-22T16:29:37.414308Z","shell.execute_reply":"2024-09-22T16:29:37.414032Z","shell.execute_reply.started":"2024-09-22T16:29:37.414002Z"},"trusted":true},"outputs":[],"source":["m0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.416401Z","iopub.status.idle":"2024-09-22T16:29:37.416927Z","shell.execute_reply":"2024-09-22T16:29:37.416691Z","shell.execute_reply.started":"2024-09-22T16:29:37.416666Z"},"trusted":true},"outputs":[],"source":["m0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.418257Z","iopub.status.idle":"2024-09-22T16:29:37.419330Z","shell.execute_reply":"2024-09-22T16:29:37.419038Z","shell.execute_reply.started":"2024-09-22T16:29:37.419006Z"},"trusted":true},"outputs":[],"source":["pretrained_vit = m0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.421141Z","iopub.status.idle":"2024-09-22T16:29:37.422667Z","shell.execute_reply":"2024-09-22T16:29:37.422382Z","shell.execute_reply.started":"2024-09-22T16:29:37.422352Z"},"trusted":true},"outputs":[],"source":["pretrained_vit = m0"]},{"cell_type":"markdown","metadata":{},"source":["### Getting attention outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.424199Z","iopub.status.idle":"2024-09-22T16:29:37.424997Z","shell.execute_reply":"2024-09-22T16:29:37.424733Z","shell.execute_reply.started":"2024-09-22T16:29:37.424704Z"},"trusted":true},"outputs":[],"source":["pretrained_vit = model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.426234Z","iopub.status.idle":"2024-09-22T16:29:37.427444Z","shell.execute_reply":"2024-09-22T16:29:37.427177Z","shell.execute_reply.started":"2024-09-22T16:29:37.427145Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.models as models\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","attention_outputs = []\n","\n","def _get_attention_output(module, input, output):\n","    attention_outputs.append(output)\n","\n","\n","\n","def remove_all_hooks(model):\n","    N  = len(model.encoder.layers)\n","    for i in range(N):\n","        model.encoder.layers[i].self_attention._forward_hooks.clear()\n","    \n","def add_hooks(model):\n","    N  = len(model.encoder.layers)\n","    for i in range(N):\n","        model.encoder.layers[i].self_attention.register_forward_hook(_get_attention_output)\n","        \n","\n","def get_attentions(new_input_tensor):\n","    # Clear the attention outputs list before the next forward pass\n","    \n","    attention_outputs.clear()\n","\n","\n","\n","    \n","    # Forward pass for the new input\n","    with torch.inference_mode():\n","        outputs = pretrained_vit(new_input_tensor)\n","    return [i[0] for i in attention_outputs].copy()\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.428731Z","iopub.status.idle":"2024-09-22T16:29:37.429770Z","shell.execute_reply":"2024-09-22T16:29:37.429514Z","shell.execute_reply.started":"2024-09-22T16:29:37.429484Z"},"trusted":true},"outputs":[],"source":["'''setup'''\n","# Load the pretrained ViT model\n","pretrained_vit = pretrained_vit.to(device)\n","\n","remove_all_hooks(pretrained_vit)\n","\n","add_hooks(pretrained_vit)\n","pretrained_vit.eval()\n","\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.431642Z","iopub.status.idle":"2024-09-22T16:29:37.432057Z","shell.execute_reply":"2024-09-22T16:29:37.431859Z","shell.execute_reply.started":"2024-09-22T16:29:37.431837Z"},"trusted":true},"outputs":[],"source":["'''Run for each input'''\n","\n","        \n","# Prepare your new input tensor\n","new_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n","\n","'''12 layers so 12 entries, each is a tensor'''\n","k1 = get_attentions(new_input_tensor)\n","\n","print([i.mean().item() for bi in k1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.434136Z","iopub.status.idle":"2024-09-22T16:29:37.434544Z","shell.execute_reply":"2024-09-22T16:29:37.434352Z","shell.execute_reply.started":"2024-09-22T16:29:37.434333Z"},"trusted":true},"outputs":[],"source":["len(k1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.435705Z","iopub.status.idle":"2024-09-22T16:29:37.436072Z","shell.execute_reply":"2024-09-22T16:29:37.435906Z","shell.execute_reply.started":"2024-09-22T16:29:37.435887Z"},"trusted":true},"outputs":[],"source":["len(k1[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.437364Z","iopub.status.idle":"2024-09-22T16:29:37.437803Z","shell.execute_reply":"2024-09-22T16:29:37.437628Z","shell.execute_reply.started":"2024-09-22T16:29:37.437608Z"},"trusted":true},"outputs":[],"source":["for i in k1[0]:\n","    print(i, type(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.439823Z","iopub.status.idle":"2024-09-22T16:29:37.440177Z","shell.execute_reply":"2024-09-22T16:29:37.440022Z","shell.execute_reply.started":"2024-09-22T16:29:37.440003Z"},"trusted":true},"outputs":[],"source":["# Clear the attention outputs list before the next forward pass\n","attention_outputs.clear()\n","\n","# Prepare your new input tensor\n","new_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n","\n","# Forward pass for the new input\n","outputs = pretrained_vit(new_input_tensor)\n","\n","# Now attention_outputs contains the attention outputs from each block\n","for idx, attn in enumerate(attention_outputs):\n","    print(f\"Attention output from block {idx}: {attn.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.441155Z","iopub.status.idle":"2024-09-22T16:29:37.441563Z","shell.execute_reply":"2024-09-22T16:29:37.441358Z","shell.execute_reply.started":"2024-09-22T16:29:37.441338Z"},"trusted":true},"outputs":[],"source":["len(pretrained_vit.encoder.layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.442763Z","iopub.status.idle":"2024-09-22T16:29:37.443086Z","shell.execute_reply":"2024-09-22T16:29:37.442937Z","shell.execute_reply.started":"2024-09-22T16:29:37.442920Z"},"trusted":true},"outputs":[],"source":["\n","# def remove_all_hooks(module):\n","#     for hook in module._forward_hooks.values():\n","#         hook[0].remove()\n","#     module._forward_hooks.clear()\n","\n","def remove_all_hooks(model):\n","    N  = len(model.encoder.layers)\n","    for i in range(N):\n","        model.encoder.layers[i].self_attention._forward_hooks.clear()\n","    \n","remove_all_hooks(pretrained_vit)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.444057Z","iopub.status.idle":"2024-09-22T16:29:37.444406Z","shell.execute_reply":"2024-09-22T16:29:37.444247Z","shell.execute_reply.started":"2024-09-22T16:29:37.444229Z"},"trusted":true},"outputs":[],"source":["def remove_hook(mdl: nn.Module, hook):\n","    \"\"\"\n","    ref: https://github.com/pytorch/pytorch/issues/5037\n","    \"\"\"\n","#     handle = mdl.register_forward_hook(hook)\n","    hook.remove()\n","\n","\n","def remove_hooks(mdl: nn.Module, hooks = None):\n","    \"\"\"\n","    ref: https://github.com/pytorch/pytorch/issues/5037\n","    \"\"\"\n","    if hooks is None:\n","        hooks = mdl._forward_hooks\n","    for hook in hooks:\n","        remove_hook(mdl, hook)\n","        \n","for block in pretrained_vit.encoder.layers:\n","    remove_hooks(block.self_attention)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.445655Z","iopub.status.idle":"2024-09-22T16:29:37.446048Z","shell.execute_reply":"2024-09-22T16:29:37.445873Z","shell.execute_reply.started":"2024-09-22T16:29:37.445853Z"},"trusted":true},"outputs":[],"source":["for hk in (pretrained_vit.encoder.layers[0].self_attention._forward_hooks.values()):\n","    hk.remove()\n","    print(\"removed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.447604Z","iopub.status.idle":"2024-09-22T16:29:37.447960Z","shell.execute_reply":"2024-09-22T16:29:37.447803Z","shell.execute_reply.started":"2024-09-22T16:29:37.447785Z"},"trusted":true},"outputs":[],"source":["pretrained_vit.encoder.layers[0].self_attention._forward_hooks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.449287Z","iopub.status.idle":"2024-09-22T16:29:37.449690Z","shell.execute_reply":"2024-09-22T16:29:37.449510Z","shell.execute_reply.started":"2024-09-22T16:29:37.449490Z"},"trusted":true},"outputs":[],"source":["l2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.450844Z","iopub.status.idle":"2024-09-22T16:29:37.451201Z","shell.execute_reply":"2024-09-22T16:29:37.451044Z","shell.execute_reply.started":"2024-09-22T16:29:37.451026Z"},"trusted":true},"outputs":[],"source":["l1.keys"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.452420Z","iopub.status.idle":"2024-09-22T16:29:37.452810Z","shell.execute_reply":"2024-09-22T16:29:37.452657Z","shell.execute_reply.started":"2024-09-22T16:29:37.452627Z"},"trusted":true},"outputs":[],"source":["# Clear the attention outputs list before the next forward pass\n","attention_outputs.clear()\n","\n","# Prepare your new input tensor\n","new_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n","\n","# Forward pass for the new input\n","outputs = pretrained_vit(new_input_tensor)\n","\n","# Now attention_outputs contains the attention outputs from each block\n","# for idx, attn in enumerate(attention_outputs):\n","#     print(f\"Attention output from block {idx}: {attn.shape}\")\n","\n","len(attention_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.454076Z","iopub.status.idle":"2024-09-22T16:29:37.454480Z","shell.execute_reply":"2024-09-22T16:29:37.454280Z","shell.execute_reply.started":"2024-09-22T16:29:37.454261Z"},"trusted":true},"outputs":[],"source":["\n","# Clear the attention outputs list before the next forward pass\n","attention_outputs.clear()\n","\n","# Prepare your new input tensor\n","hooks_registered = False\n","if not hooks_registered:\n","    for block in pretrained_vit.encoder.layers:\n","        block.self_attention.register_forward_hook(get_attention_output)\n","    hooks_registered = True\n","    \n","new_input_tensor = torch.randn(1, 3, 224, 224).to(device) # Example new input\n","\n","# Forward pass for the new input\n","outputs = pretrained_vit(new_input_tensor)\n","\n","# Now attention_outputs contains the attention outputs from each block\n","for idx, attn in enumerate(attention_outputs):\n","    print(f\"Attention output from block {idx}: {round((attn[0]).median().item()*1000)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.455392Z","iopub.status.idle":"2024-09-22T16:29:37.455817Z","shell.execute_reply":"2024-09-22T16:29:37.455642Z","shell.execute_reply.started":"2024-09-22T16:29:37.455621Z"},"trusted":true},"outputs":[],"source":["len(attention_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.457450Z","iopub.status.idle":"2024-09-22T16:29:37.457837Z","shell.execute_reply":"2024-09-22T16:29:37.457678Z","shell.execute_reply.started":"2024-09-22T16:29:37.457658Z"},"trusted":true},"outputs":[],"source":["(attn[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.459932Z","iopub.status.idle":"2024-09-22T16:29:37.460290Z","shell.execute_reply":"2024-09-22T16:29:37.460130Z","shell.execute_reply.started":"2024-09-22T16:29:37.460112Z"},"trusted":true},"outputs":[],"source":["len(pretrained_vit.encoder.layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.461528Z","iopub.status.idle":"2024-09-22T16:29:37.461919Z","shell.execute_reply":"2024-09-22T16:29:37.461758Z","shell.execute_reply.started":"2024-09-22T16:29:37.461739Z"},"trusted":true},"outputs":[],"source":["len(attention_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.463484Z","iopub.status.idle":"2024-09-22T16:29:37.463879Z","shell.execute_reply":"2024-09-22T16:29:37.463710Z","shell.execute_reply.started":"2024-09-22T16:29:37.463690Z"},"trusted":true},"outputs":[],"source":["dummy_input.shape"]},{"cell_type":"markdown","metadata":{"id":"3ac9256f-90fb-4c75-8100-38977886aa80"},"source":["Woah!\n","\n","Those are some close to textbook looking (really good) loss curves (check out [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) for what an ideal loss curve should look like).\n","\n","That's the power of transfer learning!\n","\n","We managed to get outstanding results with the *same* model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.\n","\n","What do you think?\n","\n","Would our feature extractor model improve more if you kept training it?"]},{"cell_type":"markdown","metadata":{"id":"eab07548-3b1c-43a3-9f8d-02672ef1f47c"},"source":["### 10.6 Save feature extractor ViT model and check file size\n","\n","It looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.\n","\n","Perhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks it's pizza, steak or sushi).\n","\n","To do so we can first save our model with the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.464888Z","iopub.status.idle":"2024-09-22T16:29:37.465253Z","shell.execute_reply":"2024-09-22T16:29:37.465072Z","shell.execute_reply.started":"2024-09-22T16:29:37.465054Z"},"id":"0fd00943-01aa-4ef4-b366-3cb859a25b6f","trusted":true},"outputs":[],"source":["# Save the model\n","from going_modular.going_modular import utils\n","\n","utils.save_model(model=pretrained_vit,\n","                 target_dir=\"models\",\n","                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")"]},{"cell_type":"markdown","metadata":{"id":"0d115e5c-46a0-4063-a3d5-24609f2c9f51"},"source":["And since we're thinking about deploying this model, it'd be good to know the size of it (in megabytes or MB).\n","\n","Since we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.\n","\n","We can check the size of our model in bytes using the `st_size` attribute of Python's [`pathlib.Path().stat()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) method whilst passing it our model's filepath name.\n","\n","We can then scale the size in bytes to megabytes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.466507Z","iopub.status.idle":"2024-09-22T16:29:37.466885Z","shell.execute_reply":"2024-09-22T16:29:37.466720Z","shell.execute_reply.started":"2024-09-22T16:29:37.466701Z"},"id":"f52ef12c-b88e-4796-84eb-981491a84334","trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","# Get the model size in bytes then convert to megabytes\n","pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n","print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"]},{"cell_type":"markdown","metadata":{"id":"6b63b857-04e1-460c-a510-fc61231b5bc4"},"source":["Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.\n","\n","How does this compare to the EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 9](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it)?\n","\n","| **Model** | **Model size (MB)** | **Test loss** | **Test accuracy** |\n","| ----- | ----- | ----- | ------ |\n","| EffNetB2 feature extractor^ | 29 | ~0.3906 | ~0.9384 |\n","| ViT feature extractor | 327 | ~0.1084 | ~0.9384 |\n","\n","> **Note:** ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.\n","\n","The EffNetB2 model is ~11x smaller than the ViT model with similar results for test loss and accuracy.\n","\n","However, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data).\n","\n","But in terms of deployment, if we were comparing these two models, something we'd need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?\n","\n","Perhaps such a large model would take longer to load/run and wouldn't provide as good an experience as EffNetB2 which performs similarly but at a much reduced size."]},{"cell_type":"markdown","metadata":{"id":"2adf6c78-95c9-4c0c-b143-6d66d3b7aa25"},"source":["## 11. Make predictions on a custom image\n","\n","And finally, we'll finish with the ultimate test, predicting on our own custom data.\n","\n","Let's download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.\n","\n","To do so, let's use the `pred_and_plot()` function we created in [06. PyTorch Transfer Learning section 6](https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set), for convenience, I saved this function to [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) on the course GitHub."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.468316Z","iopub.status.idle":"2024-09-22T16:29:37.468684Z","shell.execute_reply":"2024-09-22T16:29:37.468523Z","shell.execute_reply.started":"2024-09-22T16:29:37.468504Z"},"id":"16aa8e02-e209-450d-920e-806fde1997f5","trusted":true},"outputs":[],"source":["import requests\n","\n","# Import function to make predictions on images and plot them\n","from going_modular.going_modular.predictions import pred_and_plot_image\n","\n","# Setup custom image path\n","custom_image_path = image_path / \"04-pizza-dad.jpeg\"\n","\n","# Download the image if it doesn't already exist\n","if not custom_image_path.is_file():\n","    with open(custom_image_path, \"wb\") as f:\n","        # When downloading from GitHub, need to use the \"raw\" file link\n","        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n","        print(f\"Downloading {custom_image_path}...\")\n","        f.write(request.content)\n","else:\n","    print(f\"{custom_image_path} already exists, skipping download.\")\n","\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"]},{"cell_type":"markdown","metadata":{"id":"d19162cf-0129-44cb-a083-e4d94db6d10a"},"source":["Two thumbs up!\n","\n","Congratulations!\n","\n","We've gone all the way from research paper to usable model code on our own custom images!"]},{"cell_type":"markdown","metadata":{"id":"b2d4e7fc-4b0c-4466-8530-2a81f41eab76"},"source":["## Main takeaways\n","\n","* With the explosion of machine learning, new research papers detailing advancements come out every day. And it's impossible to keep up with it *all* but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.\n","* Machine learning research papers often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).\n","* The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.\n","    * With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at [Paperswithcode.com](https://paperswithcode.com/)\n","* Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we've done in this notebook) can be very helpful for understanding.\n","* Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform *very* well with little data.\n","* Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).\n","    * A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?"]},{"cell_type":"markdown","metadata":{"id":"04b1569b-117e-43fd-9e0b-324157cb82a4"},"source":["## Exercises\n","\n","> **Note:** These exercises expect the use of `torchvision` v0.13+ (released July 2022), previous versions may work but will likely have errors.\n","\n","All of the exercises are focused on practicing the code above.\n","\n","You should be able to complete them by referencing each section or by following the resource(s) linked.\n","\n","All exercises should be completed using [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n","\n","**Resources:**\n","\n","* [Exercise template notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/08_pytorch_paper_replicating_exercises.ipynb).\n","* [Example solutions notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/08_pytorch_paper_replicating_exercise_solutions.ipynb) (try the exercises *before* looking at this).\n","    * See a live [video walkthrough of the solutions on YouTube](https://youtu.be/tjpW_BY8y3g) (errors and all).\n","\n","1. Replicate the ViT architecture we created with in-built [PyTorch transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n","    * You'll want to look into replacing our `TransformerEncoderBlock()` class with [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) (these contain the same layers as our custom blocks).\n","    * You can stack `torch.nn.TransformerEncoderLayer()`'s on top of each other with [`torch.nn.TransformerEncoder()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder).\n","2. Turn the custom ViT architecture we created into a Python script, for example, `vit.py`.\n","    * You should be able to import an entire ViT model using something like`from vit import ViT`.\n","3. Train a pretrained ViT feature extractor model (like the one we made in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-bring-in-pretrained-vit-from-torchvisionmodels-on-same-dataset)) on 20% of the pizza, steak and sushi data like the dataset we used in [07. PyTorch Experiment Tracking section 7.3](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#73-download-different-datasets).\n","    * See how it performs compared to the EffNetB2 model we compared it to in [08. PyTorch Paper Replicating section 10.6](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size).\n","4. Try repeating the steps from excercise 3 but this time use the \"`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`\" pretrained weights from [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16).\n","    * **Note:** ViT pretrained with SWAG weights has a minimum input image size of `(384, 384)` (the pretrained ViT in exercise 3 has a minimum input size of `(224, 224)`), though this is accessible in the weights `.transforms()` method.\n","5. Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:\n","    * ImageNet-21k pretraining (more data).\n","    * Learning rate warmup.\n","    * Learning rate decay.\n","    * Gradient clipping."]},{"cell_type":"markdown","metadata":{"id":"dd69be46-cb68-4391-9834-8f87d8814722"},"source":["## Extra-curriculum\n","\n","* There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580). Despite of the upgrades, we stuck with replicating a \"vanilla Vision Transformer\" in this notebook because if you understand the structure of the original, you can bridge to different iterations.\n","* The [`vit-pytorch` repository on GitHub by lucidrains](https://github.com/lucidrains/vit-pytorch) is one of the most extensive resources of different ViT architectures implemented in PyTorch. It's a phenomenal reference and one I used often to create the materials we've been through in this chapter.\n","* PyTorch have their [own implementation of the ViT architecture on GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py), it's used as the basis of the pretrained ViT models in `torchvision.models`.\n","* Jay Alammar has fantastic illustrations and explanations on his blog of the [attention mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) (the foundation of Transformer models) and [Transformer models](https://jalammar.github.io/illustrated-transformer/).\n","* Adrish Dey has a fantastic [write up of Layer Normalization](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) (a main component of the ViT architecture) can help neural network training.\n","* The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper.\n","* Yannic Kilcher's YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:\n","    * [Attention is all you need](https://www.youtube.com/watch?v=iDulhoQ2pro) (the paper that introduced the Transformer architecture).\n","    * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://youtu.be/TrdevFK_am4) (the paper that introduced the ViT architecture)."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5748862,"sourceId":9456842,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"vscode":{"interpreter":{"hash":"110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"061fc5d314804961915d9dec8a37e859":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"147fd898dba54f5c85766ea6271bd439":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"379536dbe5a74422920fa97c113f396b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43438a05ce314feb8141b9416e5ea103":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c7f4d40bb04eafa57ffcc8b0c38261":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"54edf2049515415a86af5c76594a4b7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_43438a05ce314feb8141b9416e5ea103","max":1563,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db4394bd3fbe4369aa5809ba136a85f5","value":694}},"57e73146134e4120a7f30869f1734d05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d67f490ed464d13a4d894950632f15a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66df007960cd46e8b635175768d1a634":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6961abee91ab44c3bbdd9e1d72d80dbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a7dbd60deb4700bafd0696ce2c7ccc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d352baf44ab44a6bf22e2ecdaade75c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91042fb52d2d49eba35ccb997dd81190":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e73146134e4120a7f30869f1734d05","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50c7f4d40bb04eafa57ffcc8b0c38261","value":0}},"9323a29f5fe944f689ee72198398520a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_061fc5d314804961915d9dec8a37e859","placeholder":"​","style":"IPY_MODEL_6961abee91ab44c3bbdd9e1d72d80dbf","value":" 0/10 [00:00&lt;?, ?it/s]"}},"ae0fdfd782d844fc9699481dd8334e0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2faa2e3716144feb79747a1bcfaaeed","IPY_MODEL_54edf2049515415a86af5c76594a4b7e","IPY_MODEL_c8b33e773bea4164a5aaa563f3057beb"],"layout":"IPY_MODEL_bfe0a1770579416f8ce81e599565d055"}},"b4f398b7f66c4c6490419a2c35232d67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfe0a1770579416f8ce81e599565d055":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b33e773bea4164a5aaa563f3057beb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_147fd898dba54f5c85766ea6271bd439","placeholder":"​","style":"IPY_MODEL_379536dbe5a74422920fa97c113f396b","value":" 694/1563 [05:27&lt;07:08,  2.03it/s]"}},"d5d476e6c7a142cc816f45928b1429d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6241074a580437db318bd57ad9c31aa","IPY_MODEL_91042fb52d2d49eba35ccb997dd81190","IPY_MODEL_9323a29f5fe944f689ee72198398520a"],"layout":"IPY_MODEL_b4f398b7f66c4c6490419a2c35232d67"}},"db4394bd3fbe4369aa5809ba136a85f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2faa2e3716144feb79747a1bcfaaeed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66df007960cd46e8b635175768d1a634","placeholder":"​","style":"IPY_MODEL_7d352baf44ab44a6bf22e2ecdaade75c","value":"training batches:  44%"}},"f6241074a580437db318bd57ad9c31aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a7dbd60deb4700bafd0696ce2c7ccc","placeholder":"​","style":"IPY_MODEL_5d67f490ed464d13a4d894950632f15a","value":"training epochs:   0%"}}}}},"nbformat":4,"nbformat_minor":5}
