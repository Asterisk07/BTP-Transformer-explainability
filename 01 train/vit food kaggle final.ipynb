{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"d5d476e6c7a142cc816f45928b1429d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6241074a580437db318bd57ad9c31aa","IPY_MODEL_91042fb52d2d49eba35ccb997dd81190","IPY_MODEL_9323a29f5fe944f689ee72198398520a"],"layout":"IPY_MODEL_b4f398b7f66c4c6490419a2c35232d67"}},"f6241074a580437db318bd57ad9c31aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a7dbd60deb4700bafd0696ce2c7ccc","placeholder":"​","style":"IPY_MODEL_5d67f490ed464d13a4d894950632f15a","value":"training epochs:   0%"}},"91042fb52d2d49eba35ccb997dd81190":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e73146134e4120a7f30869f1734d05","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50c7f4d40bb04eafa57ffcc8b0c38261","value":0}},"9323a29f5fe944f689ee72198398520a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_061fc5d314804961915d9dec8a37e859","placeholder":"​","style":"IPY_MODEL_6961abee91ab44c3bbdd9e1d72d80dbf","value":" 0/10 [00:00&lt;?, ?it/s]"}},"b4f398b7f66c4c6490419a2c35232d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a7dbd60deb4700bafd0696ce2c7ccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d67f490ed464d13a4d894950632f15a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57e73146134e4120a7f30869f1734d05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c7f4d40bb04eafa57ffcc8b0c38261":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"061fc5d314804961915d9dec8a37e859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6961abee91ab44c3bbdd9e1d72d80dbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae0fdfd782d844fc9699481dd8334e0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2faa2e3716144feb79747a1bcfaaeed","IPY_MODEL_54edf2049515415a86af5c76594a4b7e","IPY_MODEL_c8b33e773bea4164a5aaa563f3057beb"],"layout":"IPY_MODEL_bfe0a1770579416f8ce81e599565d055"}},"e2faa2e3716144feb79747a1bcfaaeed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66df007960cd46e8b635175768d1a634","placeholder":"​","style":"IPY_MODEL_7d352baf44ab44a6bf22e2ecdaade75c","value":"training batches:  44%"}},"54edf2049515415a86af5c76594a4b7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_43438a05ce314feb8141b9416e5ea103","max":1563,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db4394bd3fbe4369aa5809ba136a85f5","value":694}},"c8b33e773bea4164a5aaa563f3057beb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_147fd898dba54f5c85766ea6271bd439","placeholder":"​","style":"IPY_MODEL_379536dbe5a74422920fa97c113f396b","value":" 694/1563 [05:27&lt;07:08,  2.03it/s]"}},"bfe0a1770579416f8ce81e599565d055":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66df007960cd46e8b635175768d1a634":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d352baf44ab44a6bf22e2ecdaade75c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43438a05ce314feb8141b9416e5ea103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db4394bd3fbe4369aa5809ba136a85f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"147fd898dba54f5c85766ea6271bd439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"379536dbe5a74422920fa97c113f396b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0. Getting setup\n\nAs we've done previously, let's make sure we've got all of the modules we'll need for this section.\n\nWe'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n\nTo do so, we'll download [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n\nWe'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n\n`torchinfo` will help later on to give us a visual representation of our model.\n\nAnd since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.","metadata":{"id":"7a8913de-e49e-40c9-89c7-6b847fac9def"}},{"cell_type":"code","source":"# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebe46d77-6c4d-4102-9994-2cb89f633f18","outputId":"027b2a5d-262a-4205-b274-1bb055b41e5c","execution":{"iopub.status.busy":"2024-09-20T12:22:28.019995Z","iopub.execute_input":"2024-09-20T12:22:28.020498Z","iopub.status.idle":"2024-09-20T12:22:28.030761Z","shell.execute_reply.started":"2024-09-20T12:22:28.020454Z","shell.execute_reply":"2024-09-20T12:22:28.029795Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"torch version: 2.4.0\ntorchvision version: 0.19.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> **Note:** If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` and `torchvision`.\n\nNow we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) script from GitHub.\n\nThe `helper_functions.py` script contains several functions we created in previous sections:\n* `set_seeds()` to set the random seeds (created in [07. PyTorch Experiment Tracking section 0](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds)).\n* `download_data()` to download a data source given a link (created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n* `plot_loss_curves()` to inspect our model's training results (created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0))\n\n> **Note:** It may be a better idea for many of the functions in the `helper_functions.py` script to be merged into `going_modular/going_modular/utils.py`, perhaps that's an extension you'd like to try.\n","metadata":{"id":"30caf875-557e-410f-8dff-bd4a9f6c7ae4"}},{"cell_type":"code","source":"# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves","metadata":{"id":"960eb156-c1b1-4e76-a812-01bf045835bd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d14f2af2-ca52-40c3-d9fc-16764f03a64b","execution":{"iopub.status.busy":"2024-09-20T12:22:28.032472Z","iopub.execute_input":"2024-09-20T12:22:28.032827Z","iopub.status.idle":"2024-09-20T12:22:28.052290Z","shell.execute_reply.started":"2024-09-20T12:22:28.032777Z","shell.execute_reply":"2024-09-20T12:22:28.051340Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`.","metadata":{"id":"4f9bdd26-26ac-4756-bd8e-b7a50799f28b"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"5e246f92-e509-474e-b6c7-c82cf11cb8ca","outputId":"c4505b6c-fbc3-41dd-a841-7b9c02dbf2b5","colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.status.busy":"2024-09-20T12:22:28.053921Z","iopub.execute_input":"2024-09-20T12:22:28.054224Z","iopub.status.idle":"2024-09-20T12:22:28.066342Z","shell.execute_reply.started":"2024-09-20T12:22:28.054171Z","shell.execute_reply":"2024-09-20T12:22:28.065502Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"\n# raise ZeroDivisionError","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"RSlwLcWz13BX","outputId":"64c4a228-ccd3-4035-c17d-ad485ed9cd63","execution":{"iopub.status.busy":"2024-09-20T12:22:28.067416Z","iopub.execute_input":"2024-09-20T12:22:28.067711Z","iopub.status.idle":"2024-09-20T12:22:28.074002Z","shell.execute_reply.started":"2024-09-20T12:22:28.067678Z","shell.execute_reply":"2024-09-20T12:22:28.073039Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 10. Using a pretrained ViT from `torchvision.models` on the same dataset\n\nWe've discussed the benefits of using pretrained models in [06. PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/).\n\nBut since we've now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.\n\n### 10.1 Why use a pretrained model?\n\nAn important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.\n\nAnd in modern day machine learning, the original fully trained ViT would likely not be considered a \"super large\" training setup (models are continually getting bigger and bigger).\n\nReading the ViT paper section 4.2:\n\n> Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately **30 days**.\n\nAs of July 2022, the [price for renting a TPUv3](https://cloud.google.com/tpu/pricing) (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.\n\nTo rent one for 30 straight days would cost **$5,760 USD**.\n\nThis cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it's not.\n\nSo having a pretrained model available through resources like [`torchvision.models`](https://pytorch.org/vision/stable/models.html), the [`timm` (Torch Image Models) library](https://github.com/rwightman/pytorch-image-models), the [HuggingFace Hub](https://huggingface.co/models) or even from the authors of the papers themselves (there's a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I'm a big fan of this trend, many of these resources can be found on [Paperswithcode.com](https://paperswithcode.com/)).\n\nIf you're focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I'd highly recommend using a pretrained model.","metadata":{"id":"de0f9531-64f3-4e13-8482-ce545d608900"}},{"cell_type":"markdown","source":"### 10.2 Getting a pretrained ViT model and creating a feature extractor\n\nWe can get a pretrained ViT model from `torchvision.models`.\n\nWe'll go from the top by first making sure we've got the right versions of `torch` and `torchvision`.\n\n> **Note:** The following code requires `torch` v0.12+ and `torchvision` v0.13+ to use the latest `torchvision` model weights API.","metadata":{"id":"93027389-1309-47c0-85d3-50e241b617b0"}},{"cell_type":"code","source":"# The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)","metadata":{"id":"30de8333-74b0-49ae-a81e-0266e6325f26","execution":{"iopub.status.busy":"2024-09-20T12:22:28.075916Z","iopub.execute_input":"2024-09-20T12:22:28.076210Z","iopub.status.idle":"2024-09-20T12:22:28.082508Z","shell.execute_reply.started":"2024-09-20T12:22:28.076164Z","shell.execute_reply":"2024-09-20T12:22:28.081592Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"2.4.0\n0.19.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then we'll setup device-agnostic code.","metadata":{"id":"45a65cda-db08-441c-9f60-cf79138e029d"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"b0b87f68-98cc-49f8-89bd-ff220a757f76","execution":{"iopub.status.busy":"2024-09-20T12:22:28.083642Z","iopub.execute_input":"2024-09-20T12:22:28.084206Z","iopub.status.idle":"2024-09-20T12:22:28.092807Z","shell.execute_reply.started":"2024-09-20T12:22:28.084149Z","shell.execute_reply":"2024-09-20T12:22:28.091785Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"class_names=['airplane',\n  'automobile',\n  'bird',\n  'cat',\n  'deer',\n  'dog',\n  'frog',\n  'horse',\n  'ship',\n  'truck']","metadata":{"id":"KVorL4SeZ3vd","execution":{"iopub.status.busy":"2024-09-20T12:22:28.093937Z","iopub.execute_input":"2024-09-20T12:22:28.094328Z","iopub.status.idle":"2024-09-20T12:22:28.100797Z","shell.execute_reply.started":"2024-09-20T12:22:28.094284Z","shell.execute_reply":"2024-09-20T12:22:28.099787Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll get the pretrained ViT-Base with patch size 16 from `torchvision.models` and prepare it for our FoodVision Mini use case by turning it into a feature extractor transfer learning model.\n\nSpecifically, we'll:\n1. Get the pretrained weights for ViT-Base trained on ImageNet-1k from [`torchvision.models.ViT_B_16_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights) (`DEFAULT` stands for best available).\n2. Setup a ViT model instance via `torchvision.models.vit_b_16`, pass it the pretrained weights step 1 and send it to the target device.\n3. Freeze all of the parameters in the base ViT model created in step 2 by setting their `requires_grad` attribute to `False`.\n4. Update the classifier head of the ViT model created in step 2 to suit our own problem by changing the number of `out_features` to our number of classes (pizza, steak, sushi).\n\nWe covered steps like this in 06. PyTorch Transfer Learning [section 3.2: Setting up a pretrained model](https://www.learnpytorch.io/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model) and [section 3.4: Freezing the base model and changing the output layer to suit our needs](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs).","metadata":{"id":"f3d05630-aa4c-41cc-b7c0-ac9de0a4390c"}},{"cell_type":"code","source":"# 1. Get pretrained weights for ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n\ndef get_vit():\n  # 2. Setup a ViT model instance with pretrained weights\n  pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n  # 3. Freeze the base parameters\n  for parameter in pretrained_vit.parameters():\n      parameter.requires_grad = False\n\n  # 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n  # set_seeds()\n  torch.manual_seed(42)\n\n  import torch.nn as nn\n  pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n  # pretrained_vit # uncomment for model output\n  return pretrained_vit\n\npretrained_vit = get_vit()","metadata":{"id":"b8e2dda6-8af0-4255-815f-4d885fa4b477","execution":{"iopub.status.busy":"2024-09-20T12:22:28.101909Z","iopub.execute_input":"2024-09-20T12:22:28.102278Z","iopub.status.idle":"2024-09-20T12:22:29.470471Z","shell.execute_reply.started":"2024-09-20T12:22:28.102236Z","shell.execute_reply":"2024-09-20T12:22:29.469629Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Pretrained ViT feature extractor model created!\n\nLet's now check it out by printing a `torchinfo.summary()`.","metadata":{"id":"182fc970-1650-48b3-914d-0cb3e287beec"}},{"cell_type":"code","source":"# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=pretrained_vit,\n        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)","metadata":{"id":"8fbd83a1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0f66c3a-dc38-4124-8942-5e6dde5021e0","execution":{"iopub.status.busy":"2024-09-20T12:22:29.471661Z","iopub.execute_input":"2024-09-20T12:22:29.471995Z","iopub.status.idle":"2024-09-20T12:22:29.566595Z","shell.execute_reply.started":"2024-09-20T12:22:29.471957Z","shell.execute_reply":"2024-09-20T12:22:29.565695Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nVisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 10]             768                  Partial\n├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n├─Linear (heads)                                             [32, 768]            [32, 10]             7,690                True\n============================================================================================================================================\nTotal params: 85,806,346\nTrainable params: 7,690\nNon-trainable params: 85,798,656\nTotal mult-adds (G): 5.52\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3330.74\nParams size (MB): 229.22\nEstimated Total Size (MB): 3579.23\n============================================================================================================================================"},"metadata":{}}]},{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-summary-output-pytorch-vit.png\" alt=\"output of pytorch pretrained ViT model summary\" width=900 />\n\nWoohoo!\n\nNotice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).\n\nAnd the total number of parameters, 85,800,963, is the same as our custom made ViT model above.\n\nBut the number of trainable parameters for `pretrained_vit` is much, much lower than our custom `vit` at only 2,307 compared to 85,800,963 (in our custom `vit`, since we're training from scratch, all parameters are trainable).\n\nThis means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory.","metadata":{"id":"90c176e5-6453-4911-b8ec-97bab43b437d"}},{"cell_type":"markdown","source":"### 10.3 Preparing data for the pretrained ViT model\n\nWe downloaded and created DataLoaders for our own ViT model back in section 2.\n\nSo we don't necessarily need to do it again.\n\nBut in the name of practice, let's download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.\n\nWe can download pizza, steak and sushi images from the course GitHub and the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data).\n    ","metadata":{"id":"a50dfe1f-a475-473d-bc23-ef3c58ba4854"}},{"cell_type":"code","source":"# from helper_functions import download_data\n\n# # Download pizza, steak, sushi images from GitHub\n# image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n#                            destination=\"pizza_steak_sushi\")\n\n# image_path","metadata":{"id":"94cb3900","execution":{"iopub.status.busy":"2024-09-20T12:22:29.567831Z","iopub.execute_input":"2024-09-20T12:22:29.568318Z","iopub.status.idle":"2024-09-20T12:22:29.572340Z","shell.execute_reply.started":"2024-09-20T12:22:29.568272Z","shell.execute_reply":"2024-09-20T12:22:29.571612Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"And now we'll setup the training and test directory paths.","metadata":{"id":"4696fecb-cd74-41ca-b1f7-02bbaf7f8ed3"}},{"cell_type":"code","source":"# # Setup train and test directory paths\n# train_dir = image_path / \"train\"\n# test_dir = image_path / \"test\"\n# train_dir, test_dir","metadata":{"id":"2e6ae0fe-73c0-4930-988a-e4df903084b6","execution":{"iopub.status.busy":"2024-09-20T12:22:29.575737Z","iopub.execute_input":"2024-09-20T12:22:29.576034Z","iopub.status.idle":"2024-09-20T12:22:29.581022Z","shell.execute_reply.started":"2024-09-20T12:22:29.576000Z","shell.execute_reply":"2024-09-20T12:22:29.579987Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll transform our images into tensors and turn the tensors into DataLoaders.\n\nSince we're using a pretrained model from `torchvision.models` we can call the `transforms()` method on it to get its required transforms.\n\nRemember, if you're going to use a pretrained model, it's generally important to **ensure your own custom data is transformed/formatted in the same way the data the original model was trained on**.\n\nWe covered this method of \"automatic\" transform creation in [06. PyTorch Transfer Learning section 2.2](https://www.learnpytorch.io/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation).","metadata":{"id":"c8736ad3-f510-4418-8c8e-f6cc3f2e1788"}},{"cell_type":"code","source":"# Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)","metadata":{"id":"6f48d40b-11f6-4e74-8503-cc29e073140e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6465f95a-df14-4874-88cc-9c6b9f8731be","execution":{"iopub.status.busy":"2024-09-20T12:22:29.581977Z","iopub.execute_input":"2024-09-20T12:22:29.582311Z","iopub.status.idle":"2024-09-20T12:22:29.591172Z","shell.execute_reply.started":"2024-09-20T12:22:29.582278Z","shell.execute_reply":"2024-09-20T12:22:29.590417Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntrain_data = datasets.CIFAR10(root='cifar-10', train=True, download=True, transform=pretrained_vit_transforms)\ntest_data = datasets.CIFAR10(root='cifar-10', train=False, download=True, transform=pretrained_vit_transforms)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCMo22fMYy-7","outputId":"3169fd06-9d46-4dbc-e92e-77e8dd4622c7","execution":{"iopub.status.busy":"2024-09-20T12:22:29.592289Z","iopub.execute_input":"2024-09-20T12:22:29.592665Z","iopub.status.idle":"2024-09-20T12:22:31.272871Z","shell.execute_reply.started":"2024-09-20T12:22:29.592620Z","shell.execute_reply":"2024-09-20T12:22:31.271671Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# len(test_dataloader_pretrained)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34WmsqJZ4rcC","outputId":"1ae79f46-5fea-452b-cd3b-39cb310518d6","execution":{"iopub.status.busy":"2024-09-20T12:22:31.274239Z","iopub.execute_input":"2024-09-20T12:22:31.274633Z","iopub.status.idle":"2024-09-20T12:22:31.279063Z","shell.execute_reply.started":"2024-09-20T12:22:31.274587Z","shell.execute_reply":"2024-09-20T12:22:31.277968Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xVVnPNht5Ak-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"_TrhxAgR5A50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader_pretrained = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_dataloader_pretrained = DataLoader(test_data, batch_size=32, shuffle=False)\n\n# Get class names\nclass_names = train_data.classes\n\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5z3SyrRYzcl","outputId":"451d1644-b033-460f-84c9-f7371371263a","execution":{"iopub.status.busy":"2024-09-20T12:22:31.280395Z","iopub.execute_input":"2024-09-20T12:22:31.280850Z","iopub.status.idle":"2024-09-20T12:22:31.291011Z","shell.execute_reply.started":"2024-09-20T12:22:31.280788Z","shell.execute_reply":"2024-09-20T12:22:31.290078Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7cd0d77689d0>,\n <torch.utils.data.dataloader.DataLoader at 0x7cd0d776a200>,\n ['airplane',\n  'automobile',\n  'bird',\n  'cat',\n  'deer',\n  'dog',\n  'frog',\n  'horse',\n  'ship',\n  'truck'])"},"metadata":{}}]},{"cell_type":"code","source":"\n# train_dataloader_pretrained.device","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"tW4kYFrj5Bbe","outputId":"e6659d66-8bcb-4fb9-a4bf-e7b319882267","execution":{"iopub.status.busy":"2024-09-20T12:22:31.292060Z","iopub.execute_input":"2024-09-20T12:22:31.292342Z","iopub.status.idle":"2024-09-20T12:22:31.297684Z","shell.execute_reply.started":"2024-09-20T12:22:31.292311Z","shell.execute_reply":"2024-09-20T12:22:31.296835Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"And now we've got transforms ready, we can turn our images into DataLoaders using the `data_setup.create_dataloaders()` method we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n\nSince we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicking an improvement found in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580), a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32.","metadata":{"id":"76244403-6d3b-472f-a4f0-ccbaa3dfd764"}},{"cell_type":"code","source":"# # Setup dataloaders\n# train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n#                                                                                                      test_dir=test_dir,\n#                                                                                                      transform=pretrained_vit_transforms,\n#                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n","metadata":{"id":"dd2f58ff-6182-453a-a802-70ff98c09557","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"0076dd5f-4a28-494d-fe5a-e3e64eabb66d","execution":{"iopub.status.busy":"2024-09-20T12:22:31.298998Z","iopub.execute_input":"2024-09-20T12:22:31.299545Z","iopub.status.idle":"2024-09-20T12:22:31.305728Z","shell.execute_reply.started":"2024-09-20T12:22:31.299502Z","shell.execute_reply":"2024-09-20T12:22:31.304789Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### 10.4 Train feature extractor ViT model\n\nFeature extractor model ready, DataLoaders ready, time to train!\n\nAs before we'll use the Adam optimizer (`torch.optim.Adam()`) with a learning rate of `1e-3` and `torch.nn.CrossEntropyLoss()` as the loss function.\n\nOur `engine.train()` function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) will take care of the rest.","metadata":{"id":"4e9da731-3c11-4d79-9e68-f006fcedd288"}},{"cell_type":"code","source":"2","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:31.306815Z","iopub.execute_input":"2024-09-20T12:22:31.309308Z","iopub.status.idle":"2024-09-20T12:22:31.314823Z","shell.execute_reply.started":"2024-09-20T12:22:31.309261Z","shell.execute_reply":"2024-09-20T12:22:31.314026Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"# !mkdir model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:31.315928Z","iopub.execute_input":"2024-09-20T12:22:31.316261Z","iopub.status.idle":"2024-09-20T12:22:31.321922Z","shell.execute_reply.started":"2024-09-20T12:22:31.316214Z","shell.execute_reply":"2024-09-20T12:22:31.320987Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!mkdir model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:31.323162Z","iopub.execute_input":"2024-09-20T12:22:31.323536Z","iopub.status.idle":"2024-09-20T12:22:32.359395Z","shell.execute_reply.started":"2024-09-20T12:22:31.323495Z","shell.execute_reply":"2024-09-20T12:22:32.358134Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory 'model': File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in tqdm(enumerate(dataloader),\"training batches\",total=len(dataloader),):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in tqdm(enumerate(dataloader),\"testing batches\",total=len(dataloader),):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n  print(\"Working on \",device)\n  \"\"\"Trains and tests a PyTorch model.\n  \n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs),'training epochs'):\n    train_loss, train_acc = 0,0\n    \n    train_loss, train_acc = train_step(model=model,\n                                      dataloader=train_dataloader,\n                                      loss_fn=loss_fn,\n                                      optimizer=optimizer,\n                                      device=device)\n\n    \n    print(\"trained \")\n    test_loss, test_acc = test_step(model=model,\n      dataloader=test_dataloader,\n      loss_fn=loss_fn,\n      device=device)\n\n    # Print out what's happening\n    print(\n      f\"Epoch: {epoch+1} | \"\n      f\"train_loss: {train_loss:.4f} | \"\n      f\"train_acc: {train_acc:.4f} | \"\n      f\"test_loss: {test_loss:.4f} | \"\n      f\"test_acc: {test_acc:.4f}\"\n    )\n\n    # Update results dictionary\n    results[\"train_loss\"].append(train_loss)\n    results[\"train_acc\"].append(train_acc)\n    results[\"test_loss\"].append(test_loss)\n    results[\"test_acc\"].append(test_acc)\n    if epoch % 1 == 0 :\n        torch.save(model.state_dict(),f\"model/model_state_{epoch:03}.pth\")\n        torch.save(optimizer.state_dict(),f\"model/optimizer_state{epoch:03}.pth\")\n\n  # Return the filled results at the end of the epochs\n  return results","metadata":{"id":"PIs-NogO3oAR","execution":{"iopub.status.busy":"2024-09-20T12:22:32.361765Z","iopub.execute_input":"2024-09-20T12:22:32.362188Z","iopub.status.idle":"2024-09-20T12:22:32.387499Z","shell.execute_reply.started":"2024-09-20T12:22:32.362140Z","shell.execute_reply":"2024-09-20T12:22:32.386551Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# x = 4\n# print(f'{epoch:03}')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:32.388962Z","iopub.execute_input":"2024-09-20T12:22:32.389413Z","iopub.status.idle":"2024-09-20T12:22:32.399429Z","shell.execute_reply.started":"2024-09-20T12:22:32.389371Z","shell.execute_reply":"2024-09-20T12:22:32.398415Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:32.400680Z","iopub.execute_input":"2024-09-20T12:22:32.401090Z","iopub.status.idle":"2024-09-20T12:22:32.408273Z","shell.execute_reply.started":"2024-09-20T12:22:32.401042Z","shell.execute_reply":"2024-09-20T12:22:32.407251Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:22:32.409695Z","iopub.execute_input":"2024-09-20T12:22:32.410071Z","iopub.status.idle":"2024-09-20T12:22:32.418767Z","shell.execute_reply.started":"2024-09-20T12:22:32.410027Z","shell.execute_reply":"2024-09-20T12:22:32.417613Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"def reset():\n    \"\"\"Reset the model and optimizer.\"\"\"\n    set_seeds()\n    \n    model = get_vit()\n    \n    # Create optimizer and loss function\n    optimizer = torch.optim.Adam(params=model.parameters(),\n                                 lr=1e-3)\n    \n    return model, optimizer\n\n\npretrained_vit, optimizer = reset()\n# Train the classifier head of the pretrained ViT feature extractor model\n\npretrained_vit_results = train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=20,\n                                      device=device)","metadata":{"id":"a49408b4-24d9-4bb1-90a2-dd61c08f78a4","colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["d5d476e6c7a142cc816f45928b1429d5","f6241074a580437db318bd57ad9c31aa","91042fb52d2d49eba35ccb997dd81190","9323a29f5fe944f689ee72198398520a","b4f398b7f66c4c6490419a2c35232d67","73a7dbd60deb4700bafd0696ce2c7ccc","5d67f490ed464d13a4d894950632f15a","57e73146134e4120a7f30869f1734d05","50c7f4d40bb04eafa57ffcc8b0c38261","061fc5d314804961915d9dec8a37e859","6961abee91ab44c3bbdd9e1d72d80dbf","ae0fdfd782d844fc9699481dd8334e0f","e2faa2e3716144feb79747a1bcfaaeed","54edf2049515415a86af5c76594a4b7e","c8b33e773bea4164a5aaa563f3057beb","bfe0a1770579416f8ce81e599565d055","66df007960cd46e8b635175768d1a634","7d352baf44ab44a6bf22e2ecdaade75c","43438a05ce314feb8141b9416e5ea103","db4394bd3fbe4369aa5809ba136a85f5","147fd898dba54f5c85766ea6271bd439","379536dbe5a74422920fa97c113f396b"]},"outputId":"2380b549-b545-43b8-b5c2-f80ca2c768d5","execution":{"iopub.status.busy":"2024-09-20T12:22:32.420325Z","iopub.execute_input":"2024-09-20T12:22:32.420669Z","iopub.status.idle":"2024-09-20T12:54:09.319810Z","shell.execute_reply.started":"2024-09-20T12:22:32.420635Z","shell.execute_reply":"2024-09-20T12:54:09.316981Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Working on  cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"training epochs:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde27b0085df4adebdd3760aaf3282c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f3e36d3bbb4631a8d652a8f67aca65"}},"metadata":{}},{"name":"stdout","text":"trained \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"testing batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3652baf202a746f49a1ddfcbc1315596"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 0.2149 | train_acc: 0.9339 | test_loss: 0.1650 | test_acc: 0.9473\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"training batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e038aa1f41344dd49be8a5a188e6ebc0"}},"metadata":{}},{"name":"stdout","text":"trained \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"testing batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485cb7d6228e412ca5479d40e6fac24f"}},"metadata":{}},{"name":"stdout","text":"Epoch: 2 | train_loss: 0.1410 | train_acc: 0.9534 | test_loss: 0.1577 | test_acc: 0.9488\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"training batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05f4a0d5030a4567925b685c840c21d5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m pretrained_vit, optimizer \u001b[38;5;241m=\u001b[39m reset()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the classifier head of the pretrained ViT feature extractor model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pretrained_vit_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_vit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[46], line 167\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining epochs\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    165\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 167\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m   test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    176\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    177\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    178\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice)\n","Cell \u001b[0;32mIn[46], line 46\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m---> 46\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"!ls model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:54:17.114167Z","iopub.execute_input":"2024-09-20T12:54:17.115304Z","iopub.status.idle":"2024-09-20T12:54:18.149199Z","shell.execute_reply.started":"2024-09-20T12:54:17.115261Z","shell.execute_reply":"2024-09-20T12:54:18.148047Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"model_state_000.pth  optimizer_state000.pth\nmodel_state_001.pth  optimizer_state001.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# !rm -rf model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:54:09.320818Z","iopub.status.idle":"2024-09-20T12:54:09.321257Z","shell.execute_reply.started":"2024-09-20T12:54:09.321039Z","shell.execute_reply":"2024-09-20T12:54:09.321060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:54:09.322679Z","iopub.status.idle":"2024-09-20T12:54:09.323025Z","shell.execute_reply.started":"2024-09-20T12:54:09.322857Z","shell.execute_reply":"2024-09-20T12:54:09.322874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Holy cow!\n\nLooks like our pretrained ViT feature extractor performed far better than our custom ViT model trained from scratch (in the same amount of time).\n\nLet's get visual.","metadata":{"id":"f8309f97-a93c-4975-b837-8387a517b6f4"}},{"cell_type":"markdown","source":"### 10.5 Plot feature extractor ViT model loss curves\n\nOur pretrained ViT feature model numbers look good on the training and test sets.\n\nHow do the loss curves look?","metadata":{"id":"233717e4-9983-47ed-9ef2-5a079df9a971"}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:56:41.738642Z","iopub.execute_input":"2024-09-20T12:56:41.739540Z","iopub.status.idle":"2024-09-20T12:57:21.411113Z","shell.execute_reply.started":"2024-09-20T12:56:41.739492Z","shell.execute_reply":"2024-09-20T12:57:21.410144Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model_zip.zip","text/html":"<a href='model_zip.zip' target='_blank'>model_zip.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# !mkdir model_new","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:59:30.891891Z","iopub.execute_input":"2024-09-20T12:59:30.892653Z","iopub.status.idle":"2024-09-20T12:59:31.931628Z","shell.execute_reply.started":"2024-09-20T12:59:30.892610Z","shell.execute_reply":"2024-09-20T12:59:31.930380Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# !ls model","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:00:33.190738Z","iopub.execute_input":"2024-09-20T13:00:33.191189Z","iopub.status.idle":"2024-09-20T13:00:34.232923Z","shell.execute_reply.started":"2024-09-20T13:00:33.191149Z","shell.execute_reply":"2024-09-20T13:00:34.231590Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"model_state_000.pth  optimizer_state000.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# !ls model_new","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:00:39.687634Z","iopub.execute_input":"2024-09-20T13:00:39.688423Z","iopub.status.idle":"2024-09-20T13:00:40.728173Z","shell.execute_reply.started":"2024-09-20T13:00:39.688383Z","shell.execute_reply":"2024-09-20T13:00:40.726967Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"model_state_001.pth  optimizer_state001.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# !mv 'model/optimizer_state001.pth' 'model/model_state_001.pth' model_new/","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:00:24.186066Z","iopub.execute_input":"2024-09-20T13:00:24.186460Z","iopub.status.idle":"2024-09-20T13:00:25.236761Z","shell.execute_reply.started":"2024-09-20T13:00:24.186426Z","shell.execute_reply":"2024-09-20T13:00:25.235642Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"mv: cannot stat 'model/optimizer_state001.pth': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink, display\n\n# Path to the file or directory to zip\n# file_to_zip = 'model_new'\n\ndef get_zip(file_to_zip):\n    zip_file_name = f'{file_to_zip}_zip.zip'\n\n    # Function to zip a directory\n    def zip_dir(directory, zip_file):\n        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(directory):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, directory))\n\n    # Zip the file or directory\n    if os.path.isdir(file_to_zip):\n        zip_dir(file_to_zip, zip_file_name)\n    else:\n        with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n            zipf.write(file_to_zip, os.path.basename(file_to_zip))\n\n\n    print(\"Zipped\")\n    # Specify the path to your file\n    file_path = zip_file_name\n\n    # Get the size of the file in bytes\n    file_size = os.path.getsize(file_path)\n\n    # print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n\n\n    # Display the download link\n    download_link = FileLink(zip_file_name)\n    display(download_link)\n    print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:16:26.765659Z","iopub.execute_input":"2024-09-20T13:16:26.766050Z","iopub.status.idle":"2024-09-20T13:16:26.775550Z","shell.execute_reply.started":"2024-09-20T13:16:26.766013Z","shell.execute_reply":"2024-09-20T13:16:26.774507Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"!ls model_new","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:05:38.009612Z","iopub.execute_input":"2024-09-20T13:05:38.010306Z","iopub.status.idle":"2024-09-20T13:05:39.051089Z","shell.execute_reply.started":"2024-09-20T13:05:38.010266Z","shell.execute_reply":"2024-09-20T13:05:39.049965Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"model_state_001.pth  optimizer_state001.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir opt_state_dict","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:17:08.339380Z","iopub.execute_input":"2024-09-20T13:17:08.339764Z","iopub.status.idle":"2024-09-20T13:17:09.375917Z","shell.execute_reply.started":"2024-09-20T13:17:08.339730Z","shell.execute_reply":"2024-09-20T13:17:09.374672Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"!mv 'model_new/optimizer_state001.pth' opt_state_dict/","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:17:17.235247Z","iopub.execute_input":"2024-09-20T13:17:17.235653Z","iopub.status.idle":"2024-09-20T13:17:18.280299Z","shell.execute_reply.started":"2024-09-20T13:17:17.235619Z","shell.execute_reply":"2024-09-20T13:17:18.278947Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# get_zip('model_state_dict')\nget_zip('opt_state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:16:31.054637Z","iopub.execute_input":"2024-09-20T13:16:31.055299Z","iopub.status.idle":"2024-09-20T13:16:51.269115Z","shell.execute_reply.started":"2024-09-20T13:16:31.055250Z","shell.execute_reply":"2024-09-20T13:16:51.267904Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Zipped\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model_state_dict_zip.zip","text/html":"<a href='model_state_dict_zip.zip' target='_blank'>model_state_dict_zip.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"The size of the file is : 305 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# get_zip('model_state_dict')\nget_zip('opt_state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:18:10.657899Z","iopub.execute_input":"2024-09-20T13:18:10.658324Z","iopub.status.idle":"2024-09-20T13:18:10.671575Z","shell.execute_reply.started":"2024-09-20T13:18:10.658283Z","shell.execute_reply":"2024-09-20T13:18:10.670461Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Zipped\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/opt_state_dict_zip.zip","text/html":"<a href='opt_state_dict_zip.zip' target='_blank'>opt_state_dict_zip.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"The size of the file is : 0 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# get_zip('model_new/optimizer_state001.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:07:11.836558Z","iopub.execute_input":"2024-09-20T13:07:11.837361Z","iopub.status.idle":"2024-09-20T13:07:31.542916Z","shell.execute_reply.started":"2024-09-20T13:07:11.837321Z","shell.execute_reply":"2024-09-20T13:07:31.541842Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model_new_zip.zip","text/html":"<a href='model_new_zip.zip' target='_blank'>model_new_zip.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"The size of the file is : 305 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:01:29.359800Z","iopub.execute_input":"2024-09-20T13:01:29.360167Z","iopub.status.idle":"2024-09-20T13:01:29.365994Z","shell.execute_reply.started":"2024-09-20T13:01:29.360099Z","shell.execute_reply":"2024-09-20T13:01:29.364935Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"The size of the file is : 305 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)","metadata":{"id":"3c0af18e-6419-4dd6-b8ea-f5830bbd63d5","execution":{"iopub.status.busy":"2024-09-20T12:54:41.413493Z","iopub.execute_input":"2024-09-20T12:54:41.414018Z","iopub.status.idle":"2024-09-20T12:54:41.455155Z","shell.execute_reply.started":"2024-09-20T12:54:41.413975Z","shell.execute_reply":"2024-09-20T12:54:41.453268Z"},"trusted":true},"execution_count":52,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the loss curves\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_loss_curves\n\u001b[0;32m----> 4\u001b[0m plot_loss_curves(\u001b[43mpretrained_vit_results\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'pretrained_vit_results' is not defined"],"ename":"NameError","evalue":"name 'pretrained_vit_results' is not defined","output_type":"error"}]},{"cell_type":"code","source":"raise EOFError","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0 = pretrained_vit","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:47:12.362526Z","iopub.execute_input":"2024-09-20T13:47:12.363390Z","iopub.status.idle":"2024-09-20T13:47:12.367817Z","shell.execute_reply.started":"2024-09-20T13:47:12.363350Z","shell.execute_reply":"2024-09-20T13:47:12.366720Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"dummy_input = torch.randn(1, 3, 224, 224).to(device)\n\n# Pass the dummy input through the model to check\noutput = m0(dummy_input)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:48:53.521771Z","iopub.execute_input":"2024-09-20T13:48:53.522449Z","iopub.status.idle":"2024-09-20T13:48:53.555416Z","shell.execute_reply.started":"2024-09-20T13:48:53.522408Z","shell.execute_reply":"2024-09-20T13:48:53.554496Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"m0","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:52:17.964527Z","iopub.execute_input":"2024-09-20T13:52:17.965387Z","iopub.status.idle":"2024-09-20T13:52:17.973737Z","shell.execute_reply.started":"2024-09-20T13:52:17.965346Z","shell.execute_reply":"2024-09-20T13:52:17.972874Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"m0","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:57:19.226621Z","iopub.execute_input":"2024-09-20T13:57:19.227403Z","iopub.status.idle":"2024-09-20T13:57:19.235829Z","shell.execute_reply.started":"2024-09-20T13:57:19.227365Z","shell.execute_reply":"2024-09-20T13:57:19.234825Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"pretrained_vit = m0","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:57:48.457290Z","iopub.execute_input":"2024-09-20T13:57:48.458300Z","iopub.status.idle":"2024-09-20T13:57:48.462395Z","shell.execute_reply.started":"2024-09-20T13:57:48.458254Z","shell.execute_reply":"2024-09-20T13:57:48.461463Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:57:44.388521Z","iopub.execute_input":"2024-09-20T13:57:44.389465Z","iopub.status.idle":"2024-09-20T13:57:44.397763Z","shell.execute_reply.started":"2024-09-20T13:57:44.389426Z","shell.execute_reply":"2024-09-20T13:57:44.396626Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"pretrained_vit = m0","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:11:04.222081Z","iopub.execute_input":"2024-09-20T14:11:04.223019Z","iopub.status.idle":"2024-09-20T14:11:04.227610Z","shell.execute_reply.started":"2024-09-20T14:11:04.222979Z","shell.execute_reply":"2024-09-20T14:11:04.226562Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"### Getting attention outputs","metadata":{}},{"cell_type":"code","source":"pretrained_vit = model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nattention_outputs = []\n\ndef _get_attention_output(module, input, output):\n    attention_outputs.append(output)\n\n\n\ndef remove_all_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention._forward_hooks.clear()\n    \ndef add_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention.register_forward_hook(_get_attention_output)\n        \n\ndef get_attentions(new_input_tensor):\n    # Clear the attention outputs list before the next forward pass\n    \n    attention_outputs.clear()\n\n\n\n    \n    # Forward pass for the new input\n    with torch.inference_mode():\n        outputs = pretrained_vit(new_input_tensor)\n    return [i[0] for i in attention_outputs].copy()\n   ","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:36:15.731090Z","iopub.execute_input":"2024-09-20T14:36:15.731955Z","iopub.status.idle":"2024-09-20T14:36:15.739946Z","shell.execute_reply.started":"2024-09-20T14:36:15.731912Z","shell.execute_reply":"2024-09-20T14:36:15.739036Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:31:23.685654Z","iopub.execute_input":"2024-09-20T14:31:23.686037Z","iopub.status.idle":"2024-09-20T14:31:23.690307Z","shell.execute_reply.started":"2024-09-20T14:31:23.686000Z","shell.execute_reply":"2024-09-20T14:31:23.689274Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"'''setup'''\n# Load the pretrained ViT model\npretrained_vit = pretrained_vit.to(device)\n\nremove_all_hooks(pretrained_vit)\n\nadd_hooks(pretrained_vit)\npretrained_vit.eval()\n\npass","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:36:33.743736Z","iopub.execute_input":"2024-09-20T14:36:33.744152Z","iopub.status.idle":"2024-09-20T14:36:33.755675Z","shell.execute_reply.started":"2024-09-20T14:36:33.744115Z","shell.execute_reply":"2024-09-20T14:36:33.754594Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"'''Run for each input'''\n\n        \n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n'''12 layers so 12 entries, each is a tensor'''\nk1 = get_attentions(new_input_tensor)\n\nprint([i.mean().item() for bi in k1])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:36:52.111822Z","iopub.execute_input":"2024-09-20T14:36:52.112559Z","iopub.status.idle":"2024-09-20T14:36:52.142947Z","shell.execute_reply.started":"2024-09-20T14:36:52.112519Z","shell.execute_reply":"2024-09-20T14:36:52.141996Z"},"trusted":true},"execution_count":243,"outputs":[{"name":"stdout","text":"[0.0010027071693912148, 0.0018613445572555065, -0.001076946733519435, 0.00020621073781512678, 0.001712940982542932, 0.002814047271385789, 0.0025550711434334517, 0.0017358362674713135, -0.0017727121012285352, 0.0012763823615387082, 0.00015895516844466329, 0.005125285126268864]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(k1)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:36:56.760826Z","iopub.execute_input":"2024-09-20T14:36:56.761650Z","iopub.status.idle":"2024-09-20T14:36:56.767571Z","shell.execute_reply.started":"2024-09-20T14:36:56.761606Z","shell.execute_reply":"2024-09-20T14:36:56.766551Z"},"trusted":true},"execution_count":244,"outputs":[{"execution_count":244,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"len(k1[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:32:35.664158Z","iopub.execute_input":"2024-09-20T14:32:35.665077Z","iopub.status.idle":"2024-09-20T14:32:35.671235Z","shell.execute_reply.started":"2024-09-20T14:32:35.665034Z","shell.execute_reply":"2024-09-20T14:32:35.670249Z"},"trusted":true},"execution_count":231,"outputs":[{"execution_count":231,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"for i in k1[0]:\n    print(i, type(i))","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:32:49.160963Z","iopub.execute_input":"2024-09-20T14:32:49.161878Z","iopub.status.idle":"2024-09-20T14:32:49.170698Z","shell.execute_reply.started":"2024-09-20T14:32:49.161839Z","shell.execute_reply":"2024-09-20T14:32:49.169501Z"},"trusted":true},"execution_count":232,"outputs":[{"name":"stdout","text":"tensor([[[ 0.0074, -0.0098,  0.0107,  ...,  0.0073, -0.0220,  0.0204],\n         [ 0.0169,  0.0275,  0.0461,  ...,  0.0207, -0.0019,  0.0250],\n         [ 0.0159,  0.0318,  0.0466,  ...,  0.0252, -0.0009,  0.0204],\n         ...,\n         [ 0.0162,  0.0318,  0.0496,  ...,  0.0226,  0.0006,  0.0245],\n         [ 0.0152,  0.0258,  0.0447,  ...,  0.0214, -0.0021,  0.0202],\n         [ 0.0137,  0.0384,  0.0546,  ...,  0.0219,  0.0014,  0.0285]]],\n       device='cuda:0') <class 'torch.Tensor'>\nNone <class 'NoneType'>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:31:46.862068Z","iopub.execute_input":"2024-09-20T14:31:46.862807Z","iopub.status.idle":"2024-09-20T14:31:46.872612Z","shell.execute_reply.started":"2024-09-20T14:31:46.862762Z","shell.execute_reply":"2024-09-20T14:31:46.871603Z"},"trusted":true},"execution_count":224,"outputs":[{"execution_count":224,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 0.0080, -0.0061,  0.0153,  ..., -0.0005, -0.0258,  0.0285],\n          [ 0.0159,  0.0376,  0.0617,  ...,  0.0091, -0.0120,  0.0552],\n          [ 0.0161,  0.0368,  0.0570,  ...,  0.0128, -0.0078,  0.0485],\n          ...,\n          [ 0.0163,  0.0378,  0.0576,  ...,  0.0147, -0.0116,  0.0426],\n          [ 0.0148,  0.0246,  0.0436,  ...,  0.0126, -0.0085,  0.0425],\n          [ 0.0159,  0.0335,  0.0535,  ...,  0.0121, -0.0085,  0.0518]]],\n        device='cuda:0'),\n None)"},"metadata":{}}]},{"cell_type":"code","source":"# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\nfor idx, attn in enumerate(attention_outputs):\n    print(f\"Attention output from block {idx}: {attn.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:23:35.037599Z","iopub.execute_input":"2024-09-20T14:23:35.038437Z","iopub.status.idle":"2024-09-20T14:23:35.044807Z","shell.execute_reply.started":"2024-09-20T14:23:35.038391Z","shell.execute_reply":"2024-09-20T14:23:35.043793Z"},"trusted":true},"execution_count":208,"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"torch.utils.hooks.RemovableHandle"},"metadata":{}}]},{"cell_type":"code","source":"len(pretrained_vit.encoder.layers)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:25:36.804961Z","iopub.execute_input":"2024-09-20T14:25:36.805403Z","iopub.status.idle":"2024-09-20T14:25:36.811864Z","shell.execute_reply.started":"2024-09-20T14:25:36.805366Z","shell.execute_reply":"2024-09-20T14:25:36.810889Z"},"trusted":true},"execution_count":216,"outputs":[{"execution_count":216,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"\n# def remove_all_hooks(module):\n#     for hook in module._forward_hooks.values():\n#         hook[0].remove()\n#     module._forward_hooks.clear()\n\ndef remove_all_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention._forward_hooks.clear()\n    \nremove_all_hooks(pretrained_vit)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:26:15.379505Z","iopub.execute_input":"2024-09-20T14:26:15.379896Z","iopub.status.idle":"2024-09-20T14:26:15.385904Z","shell.execute_reply.started":"2024-09-20T14:26:15.379861Z","shell.execute_reply":"2024-09-20T14:26:15.384626Z"},"trusted":true},"execution_count":218,"outputs":[]},{"cell_type":"code","source":"def remove_hook(mdl: nn.Module, hook):\n    \"\"\"\n    ref: https://github.com/pytorch/pytorch/issues/5037\n    \"\"\"\n#     handle = mdl.register_forward_hook(hook)\n    hook.remove()\n\n\ndef remove_hooks(mdl: nn.Module, hooks = None):\n    \"\"\"\n    ref: https://github.com/pytorch/pytorch/issues/5037\n    \"\"\"\n    if hooks is None:\n        hooks = mdl._forward_hooks\n    for hook in hooks:\n        remove_hook(mdl, hook)\n        \nfor block in pretrained_vit.encoder.layers:\n    remove_hooks(block.self_attention)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:21:12.750133Z","iopub.execute_input":"2024-09-20T14:21:12.750532Z","iopub.status.idle":"2024-09-20T14:21:12.812596Z","shell.execute_reply.started":"2024-09-20T14:21:12.750492Z","shell.execute_reply":"2024-09-20T14:21:12.811263Z"},"trusted":true},"execution_count":190,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[190], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         remove_hook(mdl, hook)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m pretrained_vit\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mremove_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[190], line 16\u001b[0m, in \u001b[0;36mremove_hooks\u001b[0;34m(mdl, hooks)\u001b[0m\n\u001b[1;32m     14\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m mdl\u001b[38;5;241m.\u001b[39m_forward_hooks\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mremove_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[190], line 6\u001b[0m, in \u001b[0;36mremove_hook\u001b[0;34m(mdl, hook)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    ref: https://github.com/pytorch/pytorch/issues/5037\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     handle = mdl.register_forward_hook(hook)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mhook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m()\n","\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'remove'"],"ename":"AttributeError","evalue":"'int' object has no attribute 'remove'","output_type":"error"}]},{"cell_type":"code","source":"for hk in (pretrained_vit.encoder.layers[0].self_attention._forward_hooks.values()):\n    hk.remove()\n    print(\"removed\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:22:56.345439Z","iopub.execute_input":"2024-09-20T14:22:56.345919Z","iopub.status.idle":"2024-09-20T14:22:56.377268Z","shell.execute_reply.started":"2024-09-20T14:22:56.345874Z","shell.execute_reply":"2024-09-20T14:22:56.375996Z"},"trusted":true},"execution_count":204,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[204], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hk \u001b[38;5;129;01min\u001b[39;00m (pretrained_vit\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'remove'"],"ename":"AttributeError","evalue":"'function' object has no attribute 'remove'","output_type":"error"}]},{"cell_type":"code","source":"pretrained_vit.encoder.layers[0].self_attention._forward_hooks","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:24:07.474121Z","iopub.execute_input":"2024-09-20T14:24:07.475026Z","iopub.status.idle":"2024-09-20T14:24:07.481179Z","shell.execute_reply.started":"2024-09-20T14:24:07.474984Z","shell.execute_reply":"2024-09-20T14:24:07.480224Z"},"trusted":true},"execution_count":211,"outputs":[{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"OrderedDict()"},"metadata":{}}]},{"cell_type":"code","source":"l2","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:22:51.363394Z","iopub.execute_input":"2024-09-20T14:22:51.364176Z","iopub.status.idle":"2024-09-20T14:22:51.369984Z","shell.execute_reply.started":"2024-09-20T14:22:51.364121Z","shell.execute_reply":"2024-09-20T14:22:51.369019Z"},"trusted":true},"execution_count":203,"outputs":[{"execution_count":203,"output_type":"execute_result","data":{"text/plain":"odict_values([<function get_attention_output at 0x7cd0b41fe950>, <function get_attention_output at 0x7cd0b40bc160>, <function get_attention_output at 0x7cd0b40bd510>, <function get_attention_output at 0x7cd0b423c310>, <function get_attention_output at 0x7cd0b41feb90>, <function get_attention_output at 0x7cd0b40bd6c0>, <function get_attention_output at 0x7cd0b43544c0>, <function get_attention_output at 0x7cd0b40be8c0>, <function get_attention_output at 0x7cd0b40be8c0>, <function get_attention_output at 0x7cd0b40be8c0>, <function get_attention_output at 0x7cd0b40bdbd0>, <function get_attention_output at 0x7cd0b40bfd90>, <function get_attention_output at 0x7cd0b40be7a0>])"},"metadata":{}}]},{"cell_type":"code","source":"l1.keys","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:18:12.198888Z","iopub.execute_input":"2024-09-20T14:18:12.199277Z","iopub.status.idle":"2024-09-20T14:18:12.205717Z","shell.execute_reply.started":"2024-09-20T14:18:12.199237Z","shell.execute_reply":"2024-09-20T14:18:12.204746Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"<function OrderedDict.keys>"},"metadata":{}}]},{"cell_type":"code","source":"# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\n# for idx, attn in enumerate(attention_outputs):\n#     print(f\"Attention output from block {idx}: {attn.shape}\")\n\nlen(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:24:18.687307Z","iopub.execute_input":"2024-09-20T14:24:18.687648Z","iopub.status.idle":"2024-09-20T14:24:18.706554Z","shell.execute_reply.started":"2024-09-20T14:24:18.687616Z","shell.execute_reply":"2024-09-20T14:24:18.705654Z"},"trusted":true},"execution_count":215,"outputs":[{"execution_count":215,"output_type":"execute_result","data":{"text/plain":"143"},"metadata":{}}]},{"cell_type":"code","source":"\n# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nhooks_registered = False\nif not hooks_registered:\n    for block in pretrained_vit.encoder.layers:\n        block.self_attention.register_forward_hook(get_attention_output)\n    hooks_registered = True\n    \nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device) # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\nfor idx, attn in enumerate(attention_outputs):\n    print(f\"Attention output from block {idx}: {round((attn[0]).median().item()*1000)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:12:22.520405Z","iopub.execute_input":"2024-09-20T14:12:22.520798Z","iopub.status.idle":"2024-09-20T14:12:22.594985Z","shell.execute_reply.started":"2024-09-20T14:12:22.520761Z","shell.execute_reply":"2024-09-20T14:12:22.594025Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stdout","text":"Attention output from block 0: 1\nAttention output from block 1: 1\nAttention output from block 2: 1\nAttention output from block 3: 1\nAttention output from block 4: 1\nAttention output from block 5: 1\nAttention output from block 6: 1\nAttention output from block 7: 1\nAttention output from block 8: 1\nAttention output from block 9: 1\nAttention output from block 10: 3\nAttention output from block 11: 3\nAttention output from block 12: 3\nAttention output from block 13: 3\nAttention output from block 14: 3\nAttention output from block 15: 3\nAttention output from block 16: 3\nAttention output from block 17: 3\nAttention output from block 18: 3\nAttention output from block 19: 3\nAttention output from block 20: -2\nAttention output from block 21: -2\nAttention output from block 22: -2\nAttention output from block 23: -2\nAttention output from block 24: -2\nAttention output from block 25: -2\nAttention output from block 26: -2\nAttention output from block 27: -2\nAttention output from block 28: -2\nAttention output from block 29: -2\nAttention output from block 30: -1\nAttention output from block 31: -1\nAttention output from block 32: -1\nAttention output from block 33: -1\nAttention output from block 34: -1\nAttention output from block 35: -1\nAttention output from block 36: -1\nAttention output from block 37: -1\nAttention output from block 38: -1\nAttention output from block 39: -1\nAttention output from block 40: 4\nAttention output from block 41: 4\nAttention output from block 42: 4\nAttention output from block 43: 4\nAttention output from block 44: 4\nAttention output from block 45: 4\nAttention output from block 46: 4\nAttention output from block 47: 4\nAttention output from block 48: 4\nAttention output from block 49: 4\nAttention output from block 50: 3\nAttention output from block 51: 3\nAttention output from block 52: 3\nAttention output from block 53: 3\nAttention output from block 54: 3\nAttention output from block 55: 3\nAttention output from block 56: 3\nAttention output from block 57: 3\nAttention output from block 58: 3\nAttention output from block 59: 3\nAttention output from block 60: 5\nAttention output from block 61: 5\nAttention output from block 62: 5\nAttention output from block 63: 5\nAttention output from block 64: 5\nAttention output from block 65: 5\nAttention output from block 66: 5\nAttention output from block 67: 5\nAttention output from block 68: 5\nAttention output from block 69: 5\nAttention output from block 70: 4\nAttention output from block 71: 4\nAttention output from block 72: 4\nAttention output from block 73: 4\nAttention output from block 74: 4\nAttention output from block 75: 4\nAttention output from block 76: 4\nAttention output from block 77: 4\nAttention output from block 78: 4\nAttention output from block 79: 4\nAttention output from block 80: -5\nAttention output from block 81: -5\nAttention output from block 82: -5\nAttention output from block 83: -5\nAttention output from block 84: -5\nAttention output from block 85: -5\nAttention output from block 86: -5\nAttention output from block 87: -5\nAttention output from block 88: -5\nAttention output from block 89: -5\nAttention output from block 90: -4\nAttention output from block 91: -4\nAttention output from block 92: -4\nAttention output from block 93: -4\nAttention output from block 94: -4\nAttention output from block 95: -4\nAttention output from block 96: -4\nAttention output from block 97: -4\nAttention output from block 98: -4\nAttention output from block 99: -4\nAttention output from block 100: -1\nAttention output from block 101: -1\nAttention output from block 102: -1\nAttention output from block 103: -1\nAttention output from block 104: -1\nAttention output from block 105: -1\nAttention output from block 106: -1\nAttention output from block 107: -1\nAttention output from block 108: -1\nAttention output from block 109: -1\nAttention output from block 110: -3\nAttention output from block 111: -3\nAttention output from block 112: -3\nAttention output from block 113: -3\nAttention output from block 114: -3\nAttention output from block 115: -3\nAttention output from block 116: -3\nAttention output from block 117: -3\nAttention output from block 118: -3\nAttention output from block 119: -3\n","output_type":"stream"}]},{"cell_type":"code","source":"len(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:06:31.147312Z","iopub.execute_input":"2024-09-20T14:06:31.148232Z","iopub.status.idle":"2024-09-20T14:06:31.154143Z","shell.execute_reply.started":"2024-09-20T14:06:31.148173Z","shell.execute_reply":"2024-09-20T14:06:31.153152Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"(attn[0]).shape","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:08:33.187529Z","iopub.execute_input":"2024-09-20T14:08:33.187952Z","iopub.status.idle":"2024-09-20T14:08:33.194546Z","shell.execute_reply.started":"2024-09-20T14:08:33.187912Z","shell.execute_reply":"2024-09-20T14:08:33.193510Z"},"trusted":true},"execution_count":132,"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 197, 768])"},"metadata":{}}]},{"cell_type":"code","source":"len(pretrained_vit.encoder.layers)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:03:02.773310Z","iopub.execute_input":"2024-09-20T14:03:02.773738Z","iopub.status.idle":"2024-09-20T14:03:02.780345Z","shell.execute_reply.started":"2024-09-20T14:03:02.773701Z","shell.execute_reply":"2024-09-20T14:03:02.779248Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"len(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T14:04:46.018737Z","iopub.execute_input":"2024-09-20T14:04:46.019519Z","iopub.status.idle":"2024-09-20T14:04:46.025805Z","shell.execute_reply.started":"2024-09-20T14:04:46.019480Z","shell.execute_reply":"2024-09-20T14:04:46.024816Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"72"},"metadata":{}}]},{"cell_type":"code","source":"dummy_input.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-20T13:58:59.818308Z","iopub.execute_input":"2024-09-20T13:58:59.819441Z","iopub.status.idle":"2024-09-20T13:58:59.825621Z","shell.execute_reply.started":"2024-09-20T13:58:59.819398Z","shell.execute_reply":"2024-09-20T13:58:59.824446Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 224, 224])"},"metadata":{}}]},{"cell_type":"markdown","source":"Woah!\n\nThose are some close to textbook looking (really good) loss curves (check out [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) for what an ideal loss curve should look like).\n\nThat's the power of transfer learning!\n\nWe managed to get outstanding results with the *same* model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.\n\nWhat do you think?\n\nWould our feature extractor model improve more if you kept training it?","metadata":{"id":"3ac9256f-90fb-4c75-8100-38977886aa80"}},{"cell_type":"markdown","source":"### 10.6 Save feature extractor ViT model and check file size\n\nIt looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.\n\nPerhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks it's pizza, steak or sushi).\n\nTo do so we can first save our model with the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).","metadata":{"id":"eab07548-3b1c-43a3-9f8d-02672ef1f47c"}},{"cell_type":"code","source":"# Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")","metadata":{"id":"0fd00943-01aa-4ef4-b366-3cb859a25b6f","execution":{"iopub.status.busy":"2024-09-20T12:54:41.455831Z","iopub.status.idle":"2024-09-20T12:54:41.456220Z","shell.execute_reply.started":"2024-09-20T12:54:41.456009Z","shell.execute_reply":"2024-09-20T12:54:41.456028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And since we're thinking about deploying this model, it'd be good to know the size of it (in megabytes or MB).\n\nSince we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.\n\nWe can check the size of our model in bytes using the `st_size` attribute of Python's [`pathlib.Path().stat()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) method whilst passing it our model's filepath name.\n\nWe can then scale the size in bytes to megabytes.","metadata":{"id":"0d115e5c-46a0-4063-a3d5-24609f2c9f51"}},{"cell_type":"code","source":"from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")","metadata":{"id":"f52ef12c-b88e-4796-84eb-981491a84334","execution":{"iopub.status.busy":"2024-09-20T12:54:41.457721Z","iopub.status.idle":"2024-09-20T12:54:41.458054Z","shell.execute_reply.started":"2024-09-20T12:54:41.457888Z","shell.execute_reply":"2024-09-20T12:54:41.457905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.\n\nHow does this compare to the EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 9](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it)?\n\n| **Model** | **Model size (MB)** | **Test loss** | **Test accuracy** |\n| ----- | ----- | ----- | ------ |\n| EffNetB2 feature extractor^ | 29 | ~0.3906 | ~0.9384 |\n| ViT feature extractor | 327 | ~0.1084 | ~0.9384 |\n\n> **Note:** ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.\n\nThe EffNetB2 model is ~11x smaller than the ViT model with similar results for test loss and accuracy.\n\nHowever, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data).\n\nBut in terms of deployment, if we were comparing these two models, something we'd need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?\n\nPerhaps such a large model would take longer to load/run and wouldn't provide as good an experience as EffNetB2 which performs similarly but at a much reduced size.","metadata":{"id":"6b63b857-04e1-460c-a510-fc61231b5bc4"}},{"cell_type":"markdown","source":"## 11. Make predictions on a custom image\n\nAnd finally, we'll finish with the ultimate test, predicting on our own custom data.\n\nLet's download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.\n\nTo do so, let's use the `pred_and_plot()` function we created in [06. PyTorch Transfer Learning section 6](https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set), for convenience, I saved this function to [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) on the course GitHub.","metadata":{"id":"2adf6c78-95c9-4c0c-b143-6d66d3b7aa25"}},{"cell_type":"code","source":"import requests\n\n# Import function to make predictions on images and plot them\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Setup custom image path\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)","metadata":{"id":"16aa8e02-e209-450d-920e-806fde1997f5","execution":{"iopub.status.busy":"2024-09-20T12:54:41.459311Z","iopub.status.idle":"2024-09-20T12:54:41.459650Z","shell.execute_reply.started":"2024-09-20T12:54:41.459478Z","shell.execute_reply":"2024-09-20T12:54:41.459495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two thumbs up!\n\nCongratulations!\n\nWe've gone all the way from research paper to usable model code on our own custom images!","metadata":{"id":"d19162cf-0129-44cb-a083-e4d94db6d10a"}},{"cell_type":"markdown","source":"## Main takeaways\n\n* With the explosion of machine learning, new research papers detailing advancements come out every day. And it's impossible to keep up with it *all* but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.\n* Machine learning research papers often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).\n* The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.\n    * With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at [Paperswithcode.com](https://paperswithcode.com/)\n* Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we've done in this notebook) can be very helpful for understanding.\n* Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform *very* well with little data.\n* Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).\n    * A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?","metadata":{"id":"b2d4e7fc-4b0c-4466-8530-2a81f41eab76"}},{"cell_type":"markdown","source":"## Exercises\n\n> **Note:** These exercises expect the use of `torchvision` v0.13+ (released July 2022), previous versions may work but will likely have errors.\n\nAll of the exercises are focused on practicing the code above.\n\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\n\nAll exercises should be completed using [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n\n**Resources:**\n\n* [Exercise template notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/08_pytorch_paper_replicating_exercises.ipynb).\n* [Example solutions notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/08_pytorch_paper_replicating_exercise_solutions.ipynb) (try the exercises *before* looking at this).\n    * See a live [video walkthrough of the solutions on YouTube](https://youtu.be/tjpW_BY8y3g) (errors and all).\n\n1. Replicate the ViT architecture we created with in-built [PyTorch transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n    * You'll want to look into replacing our `TransformerEncoderBlock()` class with [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) (these contain the same layers as our custom blocks).\n    * You can stack `torch.nn.TransformerEncoderLayer()`'s on top of each other with [`torch.nn.TransformerEncoder()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder).\n2. Turn the custom ViT architecture we created into a Python script, for example, `vit.py`.\n    * You should be able to import an entire ViT model using something like`from vit import ViT`.\n3. Train a pretrained ViT feature extractor model (like the one we made in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-bring-in-pretrained-vit-from-torchvisionmodels-on-same-dataset)) on 20% of the pizza, steak and sushi data like the dataset we used in [07. PyTorch Experiment Tracking section 7.3](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#73-download-different-datasets).\n    * See how it performs compared to the EffNetB2 model we compared it to in [08. PyTorch Paper Replicating section 10.6](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size).\n4. Try repeating the steps from excercise 3 but this time use the \"`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`\" pretrained weights from [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16).\n    * **Note:** ViT pretrained with SWAG weights has a minimum input image size of `(384, 384)` (the pretrained ViT in exercise 3 has a minimum input size of `(224, 224)`), though this is accessible in the weights `.transforms()` method.\n5. Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:\n    * ImageNet-21k pretraining (more data).\n    * Learning rate warmup.\n    * Learning rate decay.\n    * Gradient clipping.","metadata":{"id":"04b1569b-117e-43fd-9e0b-324157cb82a4"}},{"cell_type":"markdown","source":"## Extra-curriculum\n\n* There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580). Despite of the upgrades, we stuck with replicating a \"vanilla Vision Transformer\" in this notebook because if you understand the structure of the original, you can bridge to different iterations.\n* The [`vit-pytorch` repository on GitHub by lucidrains](https://github.com/lucidrains/vit-pytorch) is one of the most extensive resources of different ViT architectures implemented in PyTorch. It's a phenomenal reference and one I used often to create the materials we've been through in this chapter.\n* PyTorch have their [own implementation of the ViT architecture on GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py), it's used as the basis of the pretrained ViT models in `torchvision.models`.\n* Jay Alammar has fantastic illustrations and explanations on his blog of the [attention mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) (the foundation of Transformer models) and [Transformer models](https://jalammar.github.io/illustrated-transformer/).\n* Adrish Dey has a fantastic [write up of Layer Normalization](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) (a main component of the ViT architecture) can help neural network training.\n* The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper.\n* Yannic Kilcher's YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:\n    * [Attention is all you need](https://www.youtube.com/watch?v=iDulhoQ2pro) (the paper that introduced the Transformer architecture).\n    * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://youtu.be/TrdevFK_am4) (the paper that introduced the ViT architecture).","metadata":{"id":"dd69be46-cb68-4391-9834-8f87d8814722"}}]}