{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"d5d476e6c7a142cc816f45928b1429d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6241074a580437db318bd57ad9c31aa","IPY_MODEL_91042fb52d2d49eba35ccb997dd81190","IPY_MODEL_9323a29f5fe944f689ee72198398520a"],"layout":"IPY_MODEL_b4f398b7f66c4c6490419a2c35232d67"}},"f6241074a580437db318bd57ad9c31aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a7dbd60deb4700bafd0696ce2c7ccc","placeholder":"​","style":"IPY_MODEL_5d67f490ed464d13a4d894950632f15a","value":"training epochs:   0%"}},"91042fb52d2d49eba35ccb997dd81190":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e73146134e4120a7f30869f1734d05","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50c7f4d40bb04eafa57ffcc8b0c38261","value":0}},"9323a29f5fe944f689ee72198398520a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_061fc5d314804961915d9dec8a37e859","placeholder":"​","style":"IPY_MODEL_6961abee91ab44c3bbdd9e1d72d80dbf","value":" 0/10 [00:00&lt;?, ?it/s]"}},"b4f398b7f66c4c6490419a2c35232d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a7dbd60deb4700bafd0696ce2c7ccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d67f490ed464d13a4d894950632f15a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57e73146134e4120a7f30869f1734d05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c7f4d40bb04eafa57ffcc8b0c38261":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"061fc5d314804961915d9dec8a37e859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6961abee91ab44c3bbdd9e1d72d80dbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae0fdfd782d844fc9699481dd8334e0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2faa2e3716144feb79747a1bcfaaeed","IPY_MODEL_54edf2049515415a86af5c76594a4b7e","IPY_MODEL_c8b33e773bea4164a5aaa563f3057beb"],"layout":"IPY_MODEL_bfe0a1770579416f8ce81e599565d055"}},"e2faa2e3716144feb79747a1bcfaaeed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66df007960cd46e8b635175768d1a634","placeholder":"​","style":"IPY_MODEL_7d352baf44ab44a6bf22e2ecdaade75c","value":"training batches:  44%"}},"54edf2049515415a86af5c76594a4b7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_43438a05ce314feb8141b9416e5ea103","max":1563,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db4394bd3fbe4369aa5809ba136a85f5","value":694}},"c8b33e773bea4164a5aaa563f3057beb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_147fd898dba54f5c85766ea6271bd439","placeholder":"​","style":"IPY_MODEL_379536dbe5a74422920fa97c113f396b","value":" 694/1563 [05:27&lt;07:08,  2.03it/s]"}},"bfe0a1770579416f8ce81e599565d055":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66df007960cd46e8b635175768d1a634":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d352baf44ab44a6bf22e2ecdaade75c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43438a05ce314feb8141b9416e5ea103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db4394bd3fbe4369aa5809ba136a85f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"147fd898dba54f5c85766ea6271bd439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"379536dbe5a74422920fa97c113f396b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9456842,"sourceType":"datasetVersion","datasetId":5748862}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0. Getting setup\n\nAs we've done previously, let's make sure we've got all of the modules we'll need for this section.\n\nWe'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n\nTo do so, we'll download [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n\nWe'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n\n`torchinfo` will help later on to give us a visual representation of our model.\n\nAnd since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.","metadata":{"id":"7a8913de-e49e-40c9-89c7-6b847fac9def"}},{"cell_type":"code","source":"# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebe46d77-6c4d-4102-9994-2cb89f633f18","outputId":"027b2a5d-262a-4205-b274-1bb055b41e5c","execution":{"iopub.status.busy":"2024-09-22T16:12:28.191575Z","iopub.execute_input":"2024-09-22T16:12:28.192016Z","iopub.status.idle":"2024-09-22T16:12:28.202174Z","shell.execute_reply.started":"2024-09-22T16:12:28.191967Z","shell.execute_reply":"2024-09-22T16:12:28.201215Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"torch version: 2.4.0\ntorchvision version: 0.19.0\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\nsummary","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:28.203795Z","iopub.execute_input":"2024-09-22T16:12:28.204122Z","iopub.status.idle":"2024-09-22T16:12:28.221637Z","shell.execute_reply.started":"2024-09-22T16:12:28.204090Z","shell.execute_reply":"2024-09-22T16:12:28.220761Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<function torchinfo.torchinfo.summary(model: 'nn.Module', input_size: 'INPUT_SIZE_TYPE | None' = None, input_data: 'INPUT_DATA_TYPE | None' = None, batch_dim: 'int | None' = None, cache_forward_pass: 'bool | None' = None, col_names: 'Iterable[str] | None' = None, col_width: 'int' = 25, depth: 'int' = 3, device: 'torch.device | str | None' = None, dtypes: 'list[torch.dtype] | None' = None, mode: 'str | None' = None, row_settings: 'Iterable[str] | None' = None, verbose: 'int | None' = None, **kwargs: 'Any') -> 'ModelStatistics'>"},"metadata":{}}]},{"cell_type":"markdown","source":"> **Note:** If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` and `torchvision`.\n\nNow we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) script from GitHub.\n\nThe `helper_functions.py` script contains several functions we created in previous sections:\n* `set_seeds()` to set the random seeds (created in [07. PyTorch Experiment Tracking section 0](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds)).\n* `download_data()` to download a data source given a link (created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n* `plot_loss_curves()` to inspect our model's training results (created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0))\n\n> **Note:** It may be a better idea for many of the functions in the `helper_functions.py` script to be merged into `going_modular/going_modular/utils.py`, perhaps that's an extension you'd like to try.\n","metadata":{"id":"30caf875-557e-410f-8dff-bd4a9f6c7ae4"}},{"cell_type":"code","source":"# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves","metadata":{"id":"960eb156-c1b1-4e76-a812-01bf045835bd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d14f2af2-ca52-40c3-d9fc-16764f03a64b","execution":{"iopub.status.busy":"2024-09-22T16:12:28.222786Z","iopub.execute_input":"2024-09-22T16:12:28.223146Z","iopub.status.idle":"2024-09-22T16:12:28.242131Z","shell.execute_reply.started":"2024-09-22T16:12:28.223105Z","shell.execute_reply":"2024-09-22T16:12:28.241366Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`.","metadata":{"id":"4f9bdd26-26ac-4756-bd8e-b7a50799f28b"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"5e246f92-e509-474e-b6c7-c82cf11cb8ca","outputId":"c4505b6c-fbc3-41dd-a841-7b9c02dbf2b5","colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.status.busy":"2024-09-22T16:12:28.244215Z","iopub.execute_input":"2024-09-22T16:12:28.244534Z","iopub.status.idle":"2024-09-22T16:12:28.256806Z","shell.execute_reply.started":"2024-09-22T16:12:28.244503Z","shell.execute_reply":"2024-09-22T16:12:28.255948Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"\n# raise ZeroDivisionError","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"RSlwLcWz13BX","outputId":"64c4a228-ccd3-4035-c17d-ad485ed9cd63","execution":{"iopub.status.busy":"2024-09-22T16:12:28.257901Z","iopub.execute_input":"2024-09-22T16:12:28.258285Z","iopub.status.idle":"2024-09-22T16:12:28.265417Z","shell.execute_reply.started":"2024-09-22T16:12:28.258239Z","shell.execute_reply":"2024-09-22T16:12:28.264753Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 10. Using a pretrained ViT from `torchvision.models` on the same dataset\n\nWe've discussed the benefits of using pretrained models in [06. PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/).\n\nBut since we've now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.\n\n### 10.1 Why use a pretrained model?\n\nAn important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.\n\nAnd in modern day machine learning, the original fully trained ViT would likely not be considered a \"super large\" training setup (models are continually getting bigger and bigger).\n\nReading the ViT paper section 4.2:\n\n> Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately **30 days**.\n\nAs of July 2022, the [price for renting a TPUv3](https://cloud.google.com/tpu/pricing) (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.\n\nTo rent one for 30 straight days would cost **$5,760 USD**.\n\nThis cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it's not.\n\nSo having a pretrained model available through resources like [`torchvision.models`](https://pytorch.org/vision/stable/models.html), the [`timm` (Torch Image Models) library](https://github.com/rwightman/pytorch-image-models), the [HuggingFace Hub](https://huggingface.co/models) or even from the authors of the papers themselves (there's a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I'm a big fan of this trend, many of these resources can be found on [Paperswithcode.com](https://paperswithcode.com/)).\n\nIf you're focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I'd highly recommend using a pretrained model.","metadata":{"id":"de0f9531-64f3-4e13-8482-ce545d608900"}},{"cell_type":"markdown","source":"### 10.2 Getting a pretrained ViT model and creating a feature extractor\n\nWe can get a pretrained ViT model from `torchvision.models`.\n\nWe'll go from the top by first making sure we've got the right versions of `torch` and `torchvision`.\n\n> **Note:** The following code requires `torch` v0.12+ and `torchvision` v0.13+ to use the latest `torchvision` model weights API.","metadata":{"id":"93027389-1309-47c0-85d3-50e241b617b0"}},{"cell_type":"code","source":"# The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)","metadata":{"id":"30de8333-74b0-49ae-a81e-0266e6325f26","execution":{"iopub.status.busy":"2024-09-22T16:12:28.266739Z","iopub.execute_input":"2024-09-22T16:12:28.267097Z","iopub.status.idle":"2024-09-22T16:12:28.276056Z","shell.execute_reply.started":"2024-09-22T16:12:28.267058Z","shell.execute_reply":"2024-09-22T16:12:28.275202Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"2.4.0\n0.19.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls models","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:28.277091Z","iopub.execute_input":"2024-09-22T16:12:28.277369Z","iopub.status.idle":"2024-09-22T16:12:29.305967Z","shell.execute_reply.started":"2024-09-22T16:12:28.277339Z","shell.execute_reply":"2024-09-22T16:12:29.304982Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"ls: cannot access 'models': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Setup for new file","metadata":{}},{"cell_type":"code","source":"!pip install ml_collections","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:29.307534Z","iopub.execute_input":"2024-09-22T16:12:29.307869Z","iopub.status.idle":"2024-09-22T16:12:42.431542Z","shell.execute_reply.started":"2024-09-22T16:12:29.307836Z","shell.execute_reply":"2024-09-22T16:12:42.430374Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ml_collections in /opt/conda/lib/python3.10/site-packages (0.1.1)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from ml_collections) (1.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml_collections) (6.0.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from ml_collections) (1.16.0)\nRequirement already satisfied: contextlib2 in /opt/conda/lib/python3.10/site-packages (from ml_collections) (21.6.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/byM1902/ViT_visualization/\n!mv ViT_visualization/models .\n!rm -rf ViT_visualization\n!ls models","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:42.434998Z","iopub.execute_input":"2024-09-22T16:12:42.435339Z","iopub.status.idle":"2024-09-22T16:12:47.657624Z","shell.execute_reply.started":"2024-09-22T16:12:42.435299Z","shell.execute_reply":"2024-09-22T16:12:47.656416Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Cloning into 'ViT_visualization'...\nremote: Enumerating objects: 98, done.\u001b[K\nremote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98 (from 1)\u001b[K\nReceiving objects: 100% (98/98), 16.27 MiB | 38.39 MiB/s, done.\nResolving deltas: 100% (36/36), done.\nconfigs.py  modeling.py  modeling_VA.py  modeling_resnet.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import typing\nimport io\nimport os\n\nimport torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom urllib.request import urlretrieve\n\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom models.modeling import VisionTransformer, CONFIGS\nprint(\"imported succesfully\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:47.659096Z","iopub.execute_input":"2024-09-22T16:12:47.659449Z","iopub.status.idle":"2024-09-22T16:12:47.866860Z","shell.execute_reply.started":"2024-09-22T16:12:47.659400Z","shell.execute_reply":"2024-09-22T16:12:47.865960Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"imported succesfully\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### loading new file","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''loading weights'''\n!mkdir -p weights\n!wget -O weights/ViT-B_16-224.npz /tmp/Ubuntu.iso https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:12:47.868084Z","iopub.execute_input":"2024-09-22T16:12:47.868695Z","iopub.status.idle":"2024-09-22T16:13:04.187420Z","shell.execute_reply.started":"2024-09-22T16:12:47.868660Z","shell.execute_reply":"2024-09-22T16:13:04.186205Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/tmp/Ubuntu.iso: Scheme missing.\n--2024-09-22 16:12:49--  https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.207, 74.125.195.207, 74.125.142.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 346335542 (330M) [application/octet-stream]\nSaving to: 'weights/ViT-B_16-224.npz'\n\nweights/ViT-B_16-22 100%[===================>] 330.29M  27.4MB/s    in 13s     \n\n2024-09-22 16:13:04 (24.9 MB/s) - 'weights/ViT-B_16-224.npz' saved [346335542/346335542]\n\nFINISHED --2024-09-22 16:13:04--\nTotal wall clock time: 14s\nDownloaded: 1 files, 330M in 13s (24.9 MB/s)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !rm -rf weights","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:13:04.189113Z","iopub.execute_input":"2024-09-22T16:13:04.189602Z","iopub.status.idle":"2024-09-22T16:13:04.194671Z","shell.execute_reply.started":"2024-09-22T16:13:04.189555Z","shell.execute_reply":"2024-09-22T16:13:04.193576Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# # Prepare Model\n# config = CONFIGS[\"ViT-B_16\"]\n# model = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True)\n# model.load_from(np.load(\"weights/ViT-B_16-224.npz\"))\n\n# # write herae model weights loading code tk\n# model.eval()\n# print('model change model evaluation is successful!')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:13:04.196021Z","iopub.execute_input":"2024-09-22T16:13:04.196357Z","iopub.status.idle":"2024-09-22T16:13:04.207482Z","shell.execute_reply.started":"2024-09-22T16:13:04.196314Z","shell.execute_reply":"2024-09-22T16:13:04.206659Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Then we'll setup device-agnostic code.","metadata":{"id":"45a65cda-db08-441c-9f60-cf79138e029d"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"b0b87f68-98cc-49f8-89bd-ff220a757f76","execution":{"iopub.status.busy":"2024-09-22T16:13:04.208621Z","iopub.execute_input":"2024-09-22T16:13:04.208976Z","iopub.status.idle":"2024-09-22T16:13:04.220965Z","shell.execute_reply.started":"2024-09-22T16:13:04.208944Z","shell.execute_reply":"2024-09-22T16:13:04.220131Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"class_names=['airplane',\n  'automobile',\n  'bird',\n  'cat',\n  'deer',\n  'dog',\n  'frog',\n  'horse',\n  'ship',\n  'truck']","metadata":{"id":"KVorL4SeZ3vd","execution":{"iopub.status.busy":"2024-09-22T16:13:04.222033Z","iopub.execute_input":"2024-09-22T16:13:04.222298Z","iopub.status.idle":"2024-09-22T16:13:04.230318Z","shell.execute_reply.started":"2024-09-22T16:13:04.222268Z","shell.execute_reply":"2024-09-22T16:13:04.229492Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"num_classes = len(class_names)\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:13:04.231642Z","iopub.execute_input":"2024-09-22T16:13:04.231984Z","iopub.status.idle":"2024-09-22T16:13:04.241926Z","shell.execute_reply.started":"2024-09-22T16:13:04.231953Z","shell.execute_reply":"2024-09-22T16:13:04.240751Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"config = CONFIGS[\"ViT-B_16\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:13:04.243016Z","iopub.execute_input":"2024-09-22T16:13:04.243300Z","iopub.status.idle":"2024-09-22T16:13:04.250553Z","shell.execute_reply.started":"2024-09-22T16:13:04.243270Z","shell.execute_reply":"2024-09-22T16:13:04.249714Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# 1. Get pretrained weights for ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n\ndef get_vit():\n  \n    pretrained_vit = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=False)\n    pretrained_vit.load_from(np.load(\"weights/ViT-B_16-224.npz\"))\n\n\n    pretrained_vit.train()\n    print('model loaded for trianing is successful!')\n\n    # 3. Freeze the base parameters\n    for parameter in pretrained_vit.parameters():\n      parameter.requires_grad = False\n\n    # 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n    # set_seeds()\n    torch.manual_seed(42)\n\n    import torch.nn as nn\n    pretrained_vit.head = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n    # pretrained_vit # uncomment for model output\n    return pretrained_vit.to(device)\n\npretrained_vit = get_vit()","metadata":{"id":"b8e2dda6-8af0-4255-815f-4d885fa4b477","execution":{"iopub.status.busy":"2024-09-22T16:14:26.545997Z","iopub.execute_input":"2024-09-22T16:14:26.546844Z","iopub.status.idle":"2024-09-22T16:14:28.612338Z","shell.execute_reply.started":"2024-09-22T16:14:26.546798Z","shell.execute_reply":"2024-09-22T16:14:28.611561Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"model loaded for trianing is successful!\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_vit","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:14:28.614008Z","iopub.execute_input":"2024-09-22T16:14:28.614354Z","iopub.status.idle":"2024-09-22T16:14:28.621539Z","shell.execute_reply.started":"2024-09-22T16:14:28.614312Z","shell.execute_reply":"2024-09-22T16:14:28.620754Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (transformer): Transformer(\n    (embeddings): Embeddings(\n      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x Block(\n          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (ffn): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (attn): Attention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (proj_dropout): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n        )\n      )\n      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    )\n  )\n  (head): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"pretrained_vit.head","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:20.174261Z","iopub.execute_input":"2024-09-22T16:15:20.175228Z","iopub.status.idle":"2024-09-22T16:15:20.181234Z","shell.execute_reply.started":"2024-09-22T16:15:20.175183Z","shell.execute_reply":"2024-09-22T16:15:20.180256Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Linear(in_features=768, out_features=10, bias=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Pretrained ViT feature extractor model created!\n\nLet's now check it out by printing a `torchinfo.summary()`.","metadata":{"id":"182fc970-1650-48b3-914d-0cb3e287beec"}},{"cell_type":"code","source":"pretrained_vit","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:20.182935Z","iopub.execute_input":"2024-09-22T16:15:20.183268Z","iopub.status.idle":"2024-09-22T16:15:20.195901Z","shell.execute_reply.started":"2024-09-22T16:15:20.183237Z","shell.execute_reply":"2024-09-22T16:15:20.194948Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (transformer): Transformer(\n    (embeddings): Embeddings(\n      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x Block(\n          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (ffn): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (attn): Attention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (attn_dropout): Dropout(p=0.0, inplace=False)\n            (proj_dropout): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n        )\n      )\n      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    )\n  )\n  (head): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=pretrained_vit,\n        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)","metadata":{"id":"8fbd83a1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0f66c3a-dc38-4124-8942-5e6dde5021e0","execution":{"iopub.status.busy":"2024-09-22T16:15:20.816172Z","iopub.execute_input":"2024-09-22T16:15:20.816843Z","iopub.status.idle":"2024-09-22T16:15:21.832716Z","shell.execute_reply.started":"2024-09-22T16:15:20.816801Z","shell.execute_reply":"2024-09-22T16:15:21.831650Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"=======================================================================================================================================\nLayer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n=======================================================================================================================================\nVisionTransformer (VisionTransformer)                   [32, 3, 224, 224]    [32, 10]             --                   Partial\n├─Transformer (transformer)                             [32, 3, 224, 224]    [32, 197, 768]       --                   False\n│    └─Embeddings (embeddings)                          [32, 3, 224, 224]    [32, 197, 768]       152,064              False\n│    │    └─Conv2d (patch_embeddings)                   [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n│    │    └─Dropout (dropout)                           [32, 197, 768]       [32, 197, 768]       --                   --\n│    └─Encoder (encoder)                                [32, 197, 768]       [32, 197, 768]       --                   False\n│    │    └─ModuleList (layer)                          --                   --                   (85,054,464)         False\n│    │    └─LayerNorm (encoder_norm)                    [32, 197, 768]       [32, 197, 768]       (1,536)              False\n├─Linear (head)                                         [32, 768]            [32, 10]             7,690                True\n=======================================================================================================================================\nTotal params: 85,806,346\nTrainable params: 7,690\nNon-trainable params: 85,798,656\nTotal mult-adds (G): 6.43\n=======================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 5189.86\nParams size (MB): 342.62\nEstimated Total Size (MB): 5551.75\n======================================================================================================================================="},"metadata":{}}]},{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-summary-output-pytorch-vit.png\" alt=\"output of pytorch pretrained ViT model summary\" width=900 />\n\nWoohoo!\n\nNotice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).\n\nAnd the total number of parameters, 85,800,963, is the same as our custom made ViT model above.\n\nBut the number of trainable parameters for `pretrained_vit` is much, much lower than our custom `vit` at only 2,307 compared to 85,800,963 (in our custom `vit`, since we're training from scratch, all parameters are trainable).\n\nThis means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory.","metadata":{"id":"90c176e5-6453-4911-b8ec-97bab43b437d"}},{"cell_type":"markdown","source":"### 10.3 Preparing data for the pretrained ViT model\n\nWe downloaded and created DataLoaders for our own ViT model back in section 2.\n\nSo we don't necessarily need to do it again.\n\nBut in the name of practice, let's download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.\n\nWe can download pizza, steak and sushi images from the course GitHub and the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data).\n    ","metadata":{"id":"a50dfe1f-a475-473d-bc23-ef3c58ba4854"}},{"cell_type":"code","source":"# from helper_functions import download_data\n\n# # Download pizza, steak, sushi images from GitHub\n# image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n#                            destination=\"pizza_steak_sushi\")\n\n# image_path","metadata":{"id":"94cb3900","execution":{"iopub.status.busy":"2024-09-22T16:15:21.835042Z","iopub.execute_input":"2024-09-22T16:15:21.835909Z","iopub.status.idle":"2024-09-22T16:15:21.840140Z","shell.execute_reply.started":"2024-09-22T16:15:21.835857Z","shell.execute_reply":"2024-09-22T16:15:21.839011Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"And now we'll setup the training and test directory paths.","metadata":{"id":"4696fecb-cd74-41ca-b1f7-02bbaf7f8ed3"}},{"cell_type":"code","source":"# # Setup train and test directory paths\n# train_dir = image_path / \"train\"\n# test_dir = image_path / \"test\"\n# train_dir, test_dir","metadata":{"id":"2e6ae0fe-73c0-4930-988a-e4df903084b6","execution":{"iopub.status.busy":"2024-09-22T16:15:21.841595Z","iopub.execute_input":"2024-09-22T16:15:21.842000Z","iopub.status.idle":"2024-09-22T16:15:21.853002Z","shell.execute_reply.started":"2024-09-22T16:15:21.841955Z","shell.execute_reply":"2024-09-22T16:15:21.852005Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll transform our images into tensors and turn the tensors into DataLoaders.\n\nSince we're using a pretrained model from `torchvision.models` we can call the `transforms()` method on it to get its required transforms.\n\nRemember, if you're going to use a pretrained model, it's generally important to **ensure your own custom data is transformed/formatted in the same way the data the original model was trained on**.\n\nWe covered this method of \"automatic\" transform creation in [06. PyTorch Transfer Learning section 2.2](https://www.learnpytorch.io/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation).","metadata":{"id":"c8736ad3-f510-4418-8c8e-f6cc3f2e1788"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)","metadata":{"id":"6f48d40b-11f6-4e74-8503-cc29e073140e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6465f95a-df14-4874-88cc-9c6b9f8731be","execution":{"iopub.status.busy":"2024-09-22T16:15:21.856004Z","iopub.execute_input":"2024-09-22T16:15:21.856346Z","iopub.status.idle":"2024-09-22T16:15:21.864545Z","shell.execute_reply.started":"2024-09-22T16:15:21.856313Z","shell.execute_reply":"2024-09-22T16:15:21.863692Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntrain_data = datasets.CIFAR10(root='cifar-10', train=True, download=True, transform=pretrained_vit_transforms)\ntest_data = datasets.CIFAR10(root='cifar-10', train=False, download=True, transform=pretrained_vit_transforms)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCMo22fMYy-7","outputId":"3169fd06-9d46-4dbc-e92e-77e8dd4622c7","execution":{"iopub.status.busy":"2024-09-22T16:15:21.865689Z","iopub.execute_input":"2024-09-22T16:15:21.866036Z","iopub.status.idle":"2024-09-22T16:15:30.345489Z","shell.execute_reply.started":"2024-09-22T16:15:21.866005Z","shell.execute_reply":"2024-09-22T16:15:30.344439Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar-10/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:04<00:00, 35270139.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting cifar-10/cifar-10-python.tar.gz to cifar-10\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# len(test_dataloader_pretrained)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34WmsqJZ4rcC","outputId":"1ae79f46-5fea-452b-cd3b-39cb310518d6","execution":{"iopub.status.busy":"2024-09-22T16:15:30.346774Z","iopub.execute_input":"2024-09-22T16:15:30.347093Z","iopub.status.idle":"2024-09-22T16:15:30.351065Z","shell.execute_reply.started":"2024-09-22T16:15:30.347061Z","shell.execute_reply":"2024-09-22T16:15:30.350174Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xVVnPNht5Ak-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"_TrhxAgR5A50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader_pretrained = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_dataloader_pretrained = DataLoader(test_data, batch_size=32, shuffle=False)\n\n# Get class names\nclass_names = train_data.classes\n\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5z3SyrRYzcl","outputId":"451d1644-b033-460f-84c9-f7371371263a","execution":{"iopub.status.busy":"2024-09-22T16:15:30.352324Z","iopub.execute_input":"2024-09-22T16:15:30.352691Z","iopub.status.idle":"2024-09-22T16:15:30.365244Z","shell.execute_reply.started":"2024-09-22T16:15:30.352656Z","shell.execute_reply":"2024-09-22T16:15:30.364410Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7ebe6f891ab0>,\n <torch.utils.data.dataloader.DataLoader at 0x7ebe6f891540>,\n ['airplane',\n  'automobile',\n  'bird',\n  'cat',\n  'deer',\n  'dog',\n  'frog',\n  'horse',\n  'ship',\n  'truck'])"},"metadata":{}}]},{"cell_type":"code","source":"\n# train_dataloader_pretrained.device","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"tW4kYFrj5Bbe","outputId":"e6659d66-8bcb-4fb9-a4bf-e7b319882267","execution":{"iopub.status.busy":"2024-09-22T16:15:30.366288Z","iopub.execute_input":"2024-09-22T16:15:30.366560Z","iopub.status.idle":"2024-09-22T16:15:30.374266Z","shell.execute_reply.started":"2024-09-22T16:15:30.366532Z","shell.execute_reply":"2024-09-22T16:15:30.373568Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"And now we've got transforms ready, we can turn our images into DataLoaders using the `data_setup.create_dataloaders()` method we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n\nSince we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicking an improvement found in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580), a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32.","metadata":{"id":"76244403-6d3b-472f-a4f0-ccbaa3dfd764"}},{"cell_type":"code","source":"# # Setup dataloaders\n# train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n#                                                                                                      test_dir=test_dir,\n#                                                                                                      transform=pretrained_vit_transforms,\n#                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n","metadata":{"id":"dd2f58ff-6182-453a-a802-70ff98c09557","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"0076dd5f-4a28-494d-fe5a-e3e64eabb66d","execution":{"iopub.status.busy":"2024-09-22T16:15:30.375245Z","iopub.execute_input":"2024-09-22T16:15:30.375582Z","iopub.status.idle":"2024-09-22T16:15:30.383686Z","shell.execute_reply.started":"2024-09-22T16:15:30.375538Z","shell.execute_reply":"2024-09-22T16:15:30.382903Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### 10.4 Train feature extractor ViT model\n\nFeature extractor model ready, DataLoaders ready, time to train!\n\nAs before we'll use the Adam optimizer (`torch.optim.Adam()`) with a learning rate of `1e-3` and `torch.nn.CrossEntropyLoss()` as the loss function.\n\nOur `engine.train()` function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) will take care of the rest.","metadata":{"id":"4e9da731-3c11-4d79-9e68-f006fcedd288"}},{"cell_type":"code","source":"2","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:30.387172Z","iopub.execute_input":"2024-09-22T16:15:30.387594Z","iopub.status.idle":"2024-09-22T16:15:30.394842Z","shell.execute_reply.started":"2024-09-22T16:15:30.387540Z","shell.execute_reply":"2024-09-22T16:15:30.394001Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"# !mkdir model","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:30.395770Z","iopub.execute_input":"2024-09-22T16:15:30.396036Z","iopub.status.idle":"2024-09-22T16:15:30.403026Z","shell.execute_reply.started":"2024-09-22T16:15:30.396006Z","shell.execute_reply":"2024-09-22T16:15:30.402178Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!mkdir model","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:30.404039Z","iopub.execute_input":"2024-09-22T16:15:30.404392Z","iopub.status.idle":"2024-09-22T16:15:31.434951Z","shell.execute_reply.started":"2024-09-22T16:15:30.404351Z","shell.execute_reply":"2024-09-22T16:15:31.433672Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"k1 = None\nk2 = None","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.436866Z","iopub.execute_input":"2024-09-22T16:15:31.437715Z","iopub.status.idle":"2024-09-22T16:15:31.442107Z","shell.execute_reply.started":"2024-09-22T16:15:31.437663Z","shell.execute_reply":"2024-09-22T16:15:31.441095Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in tqdm(enumerate(dataloader),\"training batches\",total=len(dataloader),):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      global k1, k2\n      y_pred,_ = model(X)\n      k1 = y_pred\n      k2 = y\n#       y_pred = torch.tensor(y_pred).to(device)\n      \n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in tqdm(enumerate(dataloader),\"testing batches\",total=len(dataloader),):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits,_ = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n  print(\"Working on \",device)\n  \"\"\"Trains and tests a PyTorch model.\n  \n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs),'training epochs'):\n    train_loss, train_acc = 0,0\n    \n    train_loss, train_acc = train_step(model=model,\n                                      dataloader=train_dataloader,\n                                      loss_fn=loss_fn,\n                                      optimizer=optimizer,\n                                      device=device)\n\n    \n    print(\"trained \")\n    test_loss, test_acc = test_step(model=model,\n      dataloader=test_dataloader,\n      loss_fn=loss_fn,\n      device=device)\n\n    # Print out what's happening\n    print(\n      f\"Epoch: {epoch+1} | \"\n      f\"train_loss: {train_loss:.4f} | \"\n      f\"train_acc: {train_acc:.4f} | \"\n      f\"test_loss: {test_loss:.4f} | \"\n      f\"test_acc: {test_acc:.4f}\"\n    )\n\n    # Update results dictionary\n    results[\"train_loss\"].append(train_loss)\n    results[\"train_acc\"].append(train_acc)\n    results[\"test_loss\"].append(test_loss)\n    results[\"test_acc\"].append(test_acc)\n    if epoch % 1 == 0 :\n        torch.save(model.state_dict(),f\"model/model_state_{epoch:03}.pth\")\n        torch.save(optimizer.state_dict(),f\"model/optimizer_state{epoch:03}.pth\")\n\n  # Return the filled results at the end of the epochs\n  return results","metadata":{"id":"PIs-NogO3oAR","execution":{"iopub.status.busy":"2024-09-22T16:15:31.443961Z","iopub.execute_input":"2024-09-22T16:15:31.444552Z","iopub.status.idle":"2024-09-22T16:15:31.471129Z","shell.execute_reply.started":"2024-09-22T16:15:31.444500Z","shell.execute_reply":"2024-09-22T16:15:31.470196Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# x = 4\n# print(f'{epoch:03}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.472256Z","iopub.execute_input":"2024-09-22T16:15:31.472634Z","iopub.status.idle":"2024-09-22T16:15:31.485345Z","shell.execute_reply.started":"2024-09-22T16:15:31.472586Z","shell.execute_reply":"2024-09-22T16:15:31.484489Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.486494Z","iopub.execute_input":"2024-09-22T16:15:31.486788Z","iopub.status.idle":"2024-09-22T16:15:31.494738Z","shell.execute_reply.started":"2024-09-22T16:15:31.486758Z","shell.execute_reply":"2024-09-22T16:15:31.493843Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.495950Z","iopub.execute_input":"2024-09-22T16:15:31.496347Z","iopub.status.idle":"2024-09-22T16:15:31.505746Z","shell.execute_reply.started":"2024-09-22T16:15:31.496296Z","shell.execute_reply":"2024-09-22T16:15:31.504801Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"# pretrained_vit = pretrained_vit.to(device)\n(next(pretrained_vit.parameters()).device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.507018Z","iopub.execute_input":"2024-09-22T16:15:31.507647Z","iopub.status.idle":"2024-09-22T16:15:31.515469Z","shell.execute_reply.started":"2024-09-22T16:15:31.507604Z","shell.execute_reply":"2024-09-22T16:15:31.514670Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"type(k1)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:15:31.516606Z","iopub.execute_input":"2024-09-22T16:15:31.516930Z","iopub.status.idle":"2024-09-22T16:15:31.526594Z","shell.execute_reply.started":"2024-09-22T16:15:31.516899Z","shell.execute_reply":"2024-09-22T16:15:31.525717Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"NoneType"},"metadata":{}}]},{"cell_type":"code","source":"def reset():\n    \"\"\"Reset the model and optimizer.\"\"\"\n    torch.manual_seed(42)\n    \n    model = get_vit()\n    \n    # Create optimizer and loss function\n    optimizer = torch.optim.Adam(params=model.parameters(),\n                                 lr=1e-3)\n    \n    return model, optimizer\n\nNUM_EPOCHS = 1\n\npretrained_vit, optimizer = reset()\n# Train the classifier head of the pretrained ViT feature extractor model\n\npretrained_vit_results = train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=NUM_EPOCHS,\n                                      device=device)","metadata":{"id":"a49408b4-24d9-4bb1-90a2-dd61c08f78a4","colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["d5d476e6c7a142cc816f45928b1429d5","f6241074a580437db318bd57ad9c31aa","91042fb52d2d49eba35ccb997dd81190","9323a29f5fe944f689ee72198398520a","b4f398b7f66c4c6490419a2c35232d67","73a7dbd60deb4700bafd0696ce2c7ccc","5d67f490ed464d13a4d894950632f15a","57e73146134e4120a7f30869f1734d05","50c7f4d40bb04eafa57ffcc8b0c38261","061fc5d314804961915d9dec8a37e859","6961abee91ab44c3bbdd9e1d72d80dbf","ae0fdfd782d844fc9699481dd8334e0f","e2faa2e3716144feb79747a1bcfaaeed","54edf2049515415a86af5c76594a4b7e","c8b33e773bea4164a5aaa563f3057beb","bfe0a1770579416f8ce81e599565d055","66df007960cd46e8b635175768d1a634","7d352baf44ab44a6bf22e2ecdaade75c","43438a05ce314feb8141b9416e5ea103","db4394bd3fbe4369aa5809ba136a85f5","147fd898dba54f5c85766ea6271bd439","379536dbe5a74422920fa97c113f396b"]},"outputId":"2380b549-b545-43b8-b5c2-f80ca2c768d5","execution":{"iopub.status.busy":"2024-09-22T16:15:31.527694Z","iopub.execute_input":"2024-09-22T16:15:31.528051Z","iopub.status.idle":"2024-09-22T16:28:59.096567Z","shell.execute_reply.started":"2024-09-22T16:15:31.528002Z","shell.execute_reply":"2024-09-22T16:28:59.095598Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"model loaded for trianing is successful!\nWorking on  cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"training epochs:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8e1dedd94c4870b6c1e91ae00bff41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04814edd208a41c68b2292642f70c08e"}},"metadata":{}},{"name":"stdout","text":"trained \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"testing batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60195aada66b49ca90a9aa173657a046"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 0.2693 | train_acc: 0.9132 | test_loss: 0.2182 | test_acc: 0.9286\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"model trained\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:28:59.097915Z","iopub.execute_input":"2024-09-22T16:28:59.098226Z","iopub.status.idle":"2024-09-22T16:28:59.103296Z","shell.execute_reply.started":"2024-09-22T16:28:59.098194Z","shell.execute_reply":"2024-09-22T16:28:59.102268Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"model trained\n","output_type":"stream"}]},{"cell_type":"code","source":"# raise er","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:06.635900Z","iopub.execute_input":"2024-09-22T16:29:06.636312Z","iopub.status.idle":"2024-09-22T16:29:06.640643Z","shell.execute_reply.started":"2024-09-22T16:29:06.636271Z","shell.execute_reply":"2024-09-22T16:29:06.639599Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### 10.5 Plot feature extractor ViT model loss curves\n\nOur pretrained ViT feature model numbers look good on the training and test sets.\n\nHow do the loss curves look?","metadata":{"id":"233717e4-9983-47ed-9ef2-5a079df9a971"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir model_state_dict\ntorch.save(pretrained_vit.state_dict(), \"model_state_dict/model_state_dict.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:16.092980Z","iopub.execute_input":"2024-09-22T16:29:16.094080Z","iopub.status.idle":"2024-09-22T16:29:17.613539Z","shell.execute_reply.started":"2024-09-22T16:29:16.094033Z","shell.execute_reply":"2024-09-22T16:29:17.612379Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink, display\n\n# Path to the file or directory to zip\n# file_to_zip = 'model_new'\n\ndef get_zip(file_to_zip):\n    zip_file_name = f'{file_to_zip}_zip.zip'\n\n    # Function to zip a directory\n    def zip_dir(directory, zip_file):\n        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(directory):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, directory))\n\n    # Zip the file or directory\n    if os.path.isdir(file_to_zip):\n        zip_dir(file_to_zip, zip_file_name)\n    else:\n        with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n            zipf.write(file_to_zip, os.path.basename(file_to_zip))\n\n\n    print(\"Zipped\")\n    # Specify the path to your file\n    file_path = zip_file_name\n\n    # Get the size of the file in bytes\n    file_size = os.path.getsize(file_path)\n\n    # print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n\n\n    # Display the download link\n    download_link = FileLink(zip_file_name)\n    display(download_link)\n    print(f\"The size of the file is : {(file_size/(2**20)):.0f} MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:17.616114Z","iopub.execute_input":"2024-09-22T16:29:17.616465Z","iopub.status.idle":"2024-09-22T16:29:17.627266Z","shell.execute_reply.started":"2024-09-22T16:29:17.616409Z","shell.execute_reply":"2024-09-22T16:29:17.626499Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"get_zip('model_state_dict')\n# get_zip('opt_state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:17.628403Z","iopub.execute_input":"2024-09-22T16:29:17.628710Z","iopub.status.idle":"2024-09-22T16:29:37.342937Z","shell.execute_reply.started":"2024-09-22T16:29:17.628680Z","shell.execute_reply":"2024-09-22T16:29:37.340500Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Zipped\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model_state_dict_zip.zip","text/html":"<a href='model_state_dict_zip.zip' target='_blank'>model_state_dict_zip.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"The size of the file is : 304 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"raise EOFError","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.345216Z","iopub.execute_input":"2024-09-22T16:29:37.345698Z","iopub.status.idle":"2024-09-22T16:29:37.399534Z","shell.execute_reply.started":"2024-09-22T16:29:37.345648Z","shell.execute_reply":"2024-09-22T16:29:37.393652Z"},"trusted":true},"execution_count":64,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n","\u001b[0;31mEOFError\u001b[0m: "],"ename":"EOFError","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"!pip install kaggle\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:36:33.048411Z","iopub.execute_input":"2024-09-22T16:36:33.049154Z","iopub.status.idle":"2024-09-22T16:36:45.903820Z","shell.execute_reply.started":"2024-09-22T16:36:33.049112Z","shell.execute_reply":"2024-09-22T16:36:45.902592Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.17)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.7.4)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.4)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.7)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:39:41.660672Z","iopub.execute_input":"2024-09-22T16:39:41.661122Z","iopub.status.idle":"2024-09-22T16:39:42.698261Z","shell.execute_reply.started":"2024-09-22T16:39:41.661080Z","shell.execute_reply":"2024-09-22T16:39:42.697155Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"BigQuery_Helper\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Paste your kaggle.json credentials directly if needed (if not uploaded through UI)\nkaggle_credentials = {\n  \"username\": \"asterisk007\",\n  \"key\": \"e8c8d3b18c96b8705d9726e06b43aa77\"\n}\n\nimport os\nimport json\n\n\n\n# Explicitly create the directory /root/.kaggle\nkaggle_dir = '/root/.kaggle'\n\n# Create the directory if it doesn't exist\nos.makedirs(kaggle_dir, exist_ok=True)\n\n# Write the kaggle.json file in /root/.kaggle\nwith open(os.path.join(kaggle_dir, \"kaggle.json\"), \"w\") as f:\n    json.dump(kaggle_credentials, f)\n\n# Optionally set the correct file permissions\nos.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n\nprint(\"kaggle.json file created successfully.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:39:57.172111Z","iopub.execute_input":"2024-09-22T16:39:57.172647Z","iopub.status.idle":"2024-09-22T16:39:57.181251Z","shell.execute_reply.started":"2024-09-22T16:39:57.172589Z","shell.execute_reply":"2024-09-22T16:39:57.180361Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"kaggle.json file created successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls model_state_dict","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:42:55.769219Z","iopub.execute_input":"2024-09-22T16:42:55.769633Z","iopub.status.idle":"2024-09-22T16:42:56.803503Z","shell.execute_reply.started":"2024-09-22T16:42:55.769595Z","shell.execute_reply":"2024-09-22T16:42:56.802518Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"model_state_dict.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls kaggle_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:44:20.178502Z","iopub.execute_input":"2024-09-22T16:44:20.178916Z","iopub.status.idle":"2024-09-22T16:44:21.215966Z","shell.execute_reply.started":"2024-09-22T16:44:20.178872Z","shell.execute_reply":"2024-09-22T16:44:21.214997Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"dataset-metadata.json  model_state_dict.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir -p kaggle_dataset\n\n# Move the model weights into the dataset folder\n!mv model_state_dict/model_state_dict.pth kaggle_dataset/\n\n# Create a metadata file required by Kaggle\ndataset_metadata = {\n    \"title\": \"Vit pretrained finetuned on cifar10 with attention\",\n    \"id\": \"asterisk007/vit-2-cifar10-attn\",  # Customize it to your username and desired dataset name\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\n\n# Save the metadata file\nwith open('kaggle_dataset/dataset-metadata.json', 'w') as f:\n    json.dump(dataset_metadata, f)\n\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:44:49.993433Z","iopub.execute_input":"2024-09-22T16:44:49.993886Z","iopub.status.idle":"2024-09-22T16:44:52.065338Z","shell.execute_reply.started":"2024-09-22T16:44:49.993846Z","shell.execute_reply":"2024-09-22T16:44:52.064130Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"mv: cannot stat 'model_state_dict/model_state_dict.pth': No such file or directory\nDone\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Use Kaggle API to create the dataset\n!kaggle datasets create -p kaggle_dataset --dir-mode zip --public","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:44:52.990485Z","iopub.execute_input":"2024-09-22T16:44:52.990894Z","iopub.status.idle":"2024-09-22T16:45:00.557765Z","shell.execute_reply.started":"2024-09-22T16:44:52.990855Z","shell.execute_reply":"2024-09-22T16:45:00.556587Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Starting upload for file model_state_dict.pth\n100%|████████████████████████████████████████| 327M/327M [00:04<00:00, 84.5MB/s]\nUpload successful: model_state_dict.pth (327MB)\nYour public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/asterisk007/vit-2-cifar10-attn\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_zip('model_state_dict')\n# get_zip('opt_state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.401004Z","iopub.status.idle":"2024-09-22T16:29:37.401470Z","shell.execute_reply.started":"2024-09-22T16:29:37.401254Z","shell.execute_reply":"2024-09-22T16:29:37.401274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_zip('model_new/optimizer_state001.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.403360Z","iopub.status.idle":"2024-09-22T16:29:37.404303Z","shell.execute_reply.started":"2024-09-22T16:29:37.404009Z","shell.execute_reply":"2024-09-22T16:29:37.404039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)","metadata":{"id":"3c0af18e-6419-4dd6-b8ea-f5830bbd63d5","execution":{"iopub.status.busy":"2024-09-22T16:29:37.405636Z","iopub.status.idle":"2024-09-22T16:29:37.406408Z","shell.execute_reply.started":"2024-09-22T16:29:37.406072Z","shell.execute_reply":"2024-09-22T16:29:37.406106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0 = pretrained_vit","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.407923Z","iopub.status.idle":"2024-09-22T16:29:37.408438Z","shell.execute_reply.started":"2024-09-22T16:29:37.408182Z","shell.execute_reply":"2024-09-22T16:29:37.408207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_input = torch.randn(1, 3, 224, 224).to(device)\n\n# Pass the dummy input through the model to check\noutput = m0(dummy_input)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.409982Z","iopub.status.idle":"2024-09-22T16:29:37.410550Z","shell.execute_reply.started":"2024-09-22T16:29:37.410243Z","shell.execute_reply":"2024-09-22T16:29:37.410272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.413810Z","iopub.status.idle":"2024-09-22T16:29:37.414308Z","shell.execute_reply.started":"2024-09-22T16:29:37.414002Z","shell.execute_reply":"2024-09-22T16:29:37.414032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.416401Z","iopub.status.idle":"2024-09-22T16:29:37.416927Z","shell.execute_reply.started":"2024-09-22T16:29:37.416666Z","shell.execute_reply":"2024-09-22T16:29:37.416691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_vit = m0","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.418257Z","iopub.status.idle":"2024-09-22T16:29:37.419330Z","shell.execute_reply.started":"2024-09-22T16:29:37.419006Z","shell.execute_reply":"2024-09-22T16:29:37.419038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_vit = m0","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.421141Z","iopub.status.idle":"2024-09-22T16:29:37.422667Z","shell.execute_reply.started":"2024-09-22T16:29:37.422352Z","shell.execute_reply":"2024-09-22T16:29:37.422382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting attention outputs","metadata":{}},{"cell_type":"code","source":"pretrained_vit = model","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.424199Z","iopub.status.idle":"2024-09-22T16:29:37.424997Z","shell.execute_reply.started":"2024-09-22T16:29:37.424704Z","shell.execute_reply":"2024-09-22T16:29:37.424733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nattention_outputs = []\n\ndef _get_attention_output(module, input, output):\n    attention_outputs.append(output)\n\n\n\ndef remove_all_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention._forward_hooks.clear()\n    \ndef add_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention.register_forward_hook(_get_attention_output)\n        \n\ndef get_attentions(new_input_tensor):\n    # Clear the attention outputs list before the next forward pass\n    \n    attention_outputs.clear()\n\n\n\n    \n    # Forward pass for the new input\n    with torch.inference_mode():\n        outputs = pretrained_vit(new_input_tensor)\n    return [i[0] for i in attention_outputs].copy()\n   ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.426234Z","iopub.status.idle":"2024-09-22T16:29:37.427444Z","shell.execute_reply.started":"2024-09-22T16:29:37.427145Z","shell.execute_reply":"2024-09-22T16:29:37.427177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''setup'''\n# Load the pretrained ViT model\npretrained_vit = pretrained_vit.to(device)\n\nremove_all_hooks(pretrained_vit)\n\nadd_hooks(pretrained_vit)\npretrained_vit.eval()\n\npass","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.428731Z","iopub.status.idle":"2024-09-22T16:29:37.429770Z","shell.execute_reply.started":"2024-09-22T16:29:37.429484Z","shell.execute_reply":"2024-09-22T16:29:37.429514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Run for each input'''\n\n        \n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n'''12 layers so 12 entries, each is a tensor'''\nk1 = get_attentions(new_input_tensor)\n\nprint([i.mean().item() for bi in k1])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.431642Z","iopub.status.idle":"2024-09-22T16:29:37.432057Z","shell.execute_reply.started":"2024-09-22T16:29:37.431837Z","shell.execute_reply":"2024-09-22T16:29:37.431859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(k1)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.434136Z","iopub.status.idle":"2024-09-22T16:29:37.434544Z","shell.execute_reply.started":"2024-09-22T16:29:37.434333Z","shell.execute_reply":"2024-09-22T16:29:37.434352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(k1[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.435705Z","iopub.status.idle":"2024-09-22T16:29:37.436072Z","shell.execute_reply.started":"2024-09-22T16:29:37.435887Z","shell.execute_reply":"2024-09-22T16:29:37.435906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in k1[0]:\n    print(i, type(i))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.437364Z","iopub.status.idle":"2024-09-22T16:29:37.437803Z","shell.execute_reply.started":"2024-09-22T16:29:37.437608Z","shell.execute_reply":"2024-09-22T16:29:37.437628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\nfor idx, attn in enumerate(attention_outputs):\n    print(f\"Attention output from block {idx}: {attn.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.439823Z","iopub.status.idle":"2024-09-22T16:29:37.440177Z","shell.execute_reply.started":"2024-09-22T16:29:37.440003Z","shell.execute_reply":"2024-09-22T16:29:37.440022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pretrained_vit.encoder.layers)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.441155Z","iopub.status.idle":"2024-09-22T16:29:37.441563Z","shell.execute_reply.started":"2024-09-22T16:29:37.441338Z","shell.execute_reply":"2024-09-22T16:29:37.441358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# def remove_all_hooks(module):\n#     for hook in module._forward_hooks.values():\n#         hook[0].remove()\n#     module._forward_hooks.clear()\n\ndef remove_all_hooks(model):\n    N  = len(model.encoder.layers)\n    for i in range(N):\n        model.encoder.layers[i].self_attention._forward_hooks.clear()\n    \nremove_all_hooks(pretrained_vit)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.442763Z","iopub.status.idle":"2024-09-22T16:29:37.443086Z","shell.execute_reply.started":"2024-09-22T16:29:37.442920Z","shell.execute_reply":"2024-09-22T16:29:37.442937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_hook(mdl: nn.Module, hook):\n    \"\"\"\n    ref: https://github.com/pytorch/pytorch/issues/5037\n    \"\"\"\n#     handle = mdl.register_forward_hook(hook)\n    hook.remove()\n\n\ndef remove_hooks(mdl: nn.Module, hooks = None):\n    \"\"\"\n    ref: https://github.com/pytorch/pytorch/issues/5037\n    \"\"\"\n    if hooks is None:\n        hooks = mdl._forward_hooks\n    for hook in hooks:\n        remove_hook(mdl, hook)\n        \nfor block in pretrained_vit.encoder.layers:\n    remove_hooks(block.self_attention)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.444057Z","iopub.status.idle":"2024-09-22T16:29:37.444406Z","shell.execute_reply.started":"2024-09-22T16:29:37.444229Z","shell.execute_reply":"2024-09-22T16:29:37.444247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for hk in (pretrained_vit.encoder.layers[0].self_attention._forward_hooks.values()):\n    hk.remove()\n    print(\"removed\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.445655Z","iopub.status.idle":"2024-09-22T16:29:37.446048Z","shell.execute_reply.started":"2024-09-22T16:29:37.445853Z","shell.execute_reply":"2024-09-22T16:29:37.445873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_vit.encoder.layers[0].self_attention._forward_hooks","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.447604Z","iopub.status.idle":"2024-09-22T16:29:37.447960Z","shell.execute_reply.started":"2024-09-22T16:29:37.447785Z","shell.execute_reply":"2024-09-22T16:29:37.447803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.449287Z","iopub.status.idle":"2024-09-22T16:29:37.449690Z","shell.execute_reply.started":"2024-09-22T16:29:37.449490Z","shell.execute_reply":"2024-09-22T16:29:37.449510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1.keys","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.450844Z","iopub.status.idle":"2024-09-22T16:29:37.451201Z","shell.execute_reply.started":"2024-09-22T16:29:37.451026Z","shell.execute_reply":"2024-09-22T16:29:37.451044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\n# for idx, attn in enumerate(attention_outputs):\n#     print(f\"Attention output from block {idx}: {attn.shape}\")\n\nlen(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.452420Z","iopub.status.idle":"2024-09-22T16:29:37.452810Z","shell.execute_reply.started":"2024-09-22T16:29:37.452627Z","shell.execute_reply":"2024-09-22T16:29:37.452657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Clear the attention outputs list before the next forward pass\nattention_outputs.clear()\n\n# Prepare your new input tensor\nhooks_registered = False\nif not hooks_registered:\n    for block in pretrained_vit.encoder.layers:\n        block.self_attention.register_forward_hook(get_attention_output)\n    hooks_registered = True\n    \nnew_input_tensor = torch.randn(1, 3, 224, 224).to(device) # Example new input\n\n# Forward pass for the new input\noutputs = pretrained_vit(new_input_tensor)\n\n# Now attention_outputs contains the attention outputs from each block\nfor idx, attn in enumerate(attention_outputs):\n    print(f\"Attention output from block {idx}: {round((attn[0]).median().item()*1000)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.454076Z","iopub.status.idle":"2024-09-22T16:29:37.454480Z","shell.execute_reply.started":"2024-09-22T16:29:37.454261Z","shell.execute_reply":"2024-09-22T16:29:37.454280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.455392Z","iopub.status.idle":"2024-09-22T16:29:37.455817Z","shell.execute_reply.started":"2024-09-22T16:29:37.455621Z","shell.execute_reply":"2024-09-22T16:29:37.455642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(attn[0]).shape","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.457450Z","iopub.status.idle":"2024-09-22T16:29:37.457837Z","shell.execute_reply.started":"2024-09-22T16:29:37.457658Z","shell.execute_reply":"2024-09-22T16:29:37.457678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pretrained_vit.encoder.layers)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.459932Z","iopub.status.idle":"2024-09-22T16:29:37.460290Z","shell.execute_reply.started":"2024-09-22T16:29:37.460112Z","shell.execute_reply":"2024-09-22T16:29:37.460130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(attention_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.461528Z","iopub.status.idle":"2024-09-22T16:29:37.461919Z","shell.execute_reply.started":"2024-09-22T16:29:37.461739Z","shell.execute_reply":"2024-09-22T16:29:37.461758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_input.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:29:37.463484Z","iopub.status.idle":"2024-09-22T16:29:37.463879Z","shell.execute_reply.started":"2024-09-22T16:29:37.463690Z","shell.execute_reply":"2024-09-22T16:29:37.463710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Woah!\n\nThose are some close to textbook looking (really good) loss curves (check out [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) for what an ideal loss curve should look like).\n\nThat's the power of transfer learning!\n\nWe managed to get outstanding results with the *same* model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.\n\nWhat do you think?\n\nWould our feature extractor model improve more if you kept training it?","metadata":{"id":"3ac9256f-90fb-4c75-8100-38977886aa80"}},{"cell_type":"markdown","source":"### 10.6 Save feature extractor ViT model and check file size\n\nIt looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.\n\nPerhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks it's pizza, steak or sushi).\n\nTo do so we can first save our model with the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).","metadata":{"id":"eab07548-3b1c-43a3-9f8d-02672ef1f47c"}},{"cell_type":"code","source":"# Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")","metadata":{"id":"0fd00943-01aa-4ef4-b366-3cb859a25b6f","execution":{"iopub.status.busy":"2024-09-22T16:29:37.464888Z","iopub.status.idle":"2024-09-22T16:29:37.465253Z","shell.execute_reply.started":"2024-09-22T16:29:37.465054Z","shell.execute_reply":"2024-09-22T16:29:37.465072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And since we're thinking about deploying this model, it'd be good to know the size of it (in megabytes or MB).\n\nSince we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.\n\nWe can check the size of our model in bytes using the `st_size` attribute of Python's [`pathlib.Path().stat()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) method whilst passing it our model's filepath name.\n\nWe can then scale the size in bytes to megabytes.","metadata":{"id":"0d115e5c-46a0-4063-a3d5-24609f2c9f51"}},{"cell_type":"code","source":"from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")","metadata":{"id":"f52ef12c-b88e-4796-84eb-981491a84334","execution":{"iopub.status.busy":"2024-09-22T16:29:37.466507Z","iopub.status.idle":"2024-09-22T16:29:37.466885Z","shell.execute_reply.started":"2024-09-22T16:29:37.466701Z","shell.execute_reply":"2024-09-22T16:29:37.466720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.\n\nHow does this compare to the EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 9](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it)?\n\n| **Model** | **Model size (MB)** | **Test loss** | **Test accuracy** |\n| ----- | ----- | ----- | ------ |\n| EffNetB2 feature extractor^ | 29 | ~0.3906 | ~0.9384 |\n| ViT feature extractor | 327 | ~0.1084 | ~0.9384 |\n\n> **Note:** ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.\n\nThe EffNetB2 model is ~11x smaller than the ViT model with similar results for test loss and accuracy.\n\nHowever, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data).\n\nBut in terms of deployment, if we were comparing these two models, something we'd need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?\n\nPerhaps such a large model would take longer to load/run and wouldn't provide as good an experience as EffNetB2 which performs similarly but at a much reduced size.","metadata":{"id":"6b63b857-04e1-460c-a510-fc61231b5bc4"}},{"cell_type":"markdown","source":"## 11. Make predictions on a custom image\n\nAnd finally, we'll finish with the ultimate test, predicting on our own custom data.\n\nLet's download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.\n\nTo do so, let's use the `pred_and_plot()` function we created in [06. PyTorch Transfer Learning section 6](https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set), for convenience, I saved this function to [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) on the course GitHub.","metadata":{"id":"2adf6c78-95c9-4c0c-b143-6d66d3b7aa25"}},{"cell_type":"code","source":"import requests\n\n# Import function to make predictions on images and plot them\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Setup custom image path\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)","metadata":{"id":"16aa8e02-e209-450d-920e-806fde1997f5","execution":{"iopub.status.busy":"2024-09-22T16:29:37.468316Z","iopub.status.idle":"2024-09-22T16:29:37.468684Z","shell.execute_reply.started":"2024-09-22T16:29:37.468504Z","shell.execute_reply":"2024-09-22T16:29:37.468523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two thumbs up!\n\nCongratulations!\n\nWe've gone all the way from research paper to usable model code on our own custom images!","metadata":{"id":"d19162cf-0129-44cb-a083-e4d94db6d10a"}},{"cell_type":"markdown","source":"## Main takeaways\n\n* With the explosion of machine learning, new research papers detailing advancements come out every day. And it's impossible to keep up with it *all* but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.\n* Machine learning research papers often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).\n* The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.\n    * With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at [Paperswithcode.com](https://paperswithcode.com/)\n* Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we've done in this notebook) can be very helpful for understanding.\n* Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform *very* well with little data.\n* Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).\n    * A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?","metadata":{"id":"b2d4e7fc-4b0c-4466-8530-2a81f41eab76"}},{"cell_type":"markdown","source":"## Exercises\n\n> **Note:** These exercises expect the use of `torchvision` v0.13+ (released July 2022), previous versions may work but will likely have errors.\n\nAll of the exercises are focused on practicing the code above.\n\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\n\nAll exercises should be completed using [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n\n**Resources:**\n\n* [Exercise template notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/08_pytorch_paper_replicating_exercises.ipynb).\n* [Example solutions notebook for 08](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/08_pytorch_paper_replicating_exercise_solutions.ipynb) (try the exercises *before* looking at this).\n    * See a live [video walkthrough of the solutions on YouTube](https://youtu.be/tjpW_BY8y3g) (errors and all).\n\n1. Replicate the ViT architecture we created with in-built [PyTorch transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n    * You'll want to look into replacing our `TransformerEncoderBlock()` class with [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) (these contain the same layers as our custom blocks).\n    * You can stack `torch.nn.TransformerEncoderLayer()`'s on top of each other with [`torch.nn.TransformerEncoder()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder).\n2. Turn the custom ViT architecture we created into a Python script, for example, `vit.py`.\n    * You should be able to import an entire ViT model using something like`from vit import ViT`.\n3. Train a pretrained ViT feature extractor model (like the one we made in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-bring-in-pretrained-vit-from-torchvisionmodels-on-same-dataset)) on 20% of the pizza, steak and sushi data like the dataset we used in [07. PyTorch Experiment Tracking section 7.3](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#73-download-different-datasets).\n    * See how it performs compared to the EffNetB2 model we compared it to in [08. PyTorch Paper Replicating section 10.6](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size).\n4. Try repeating the steps from excercise 3 but this time use the \"`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`\" pretrained weights from [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16).\n    * **Note:** ViT pretrained with SWAG weights has a minimum input image size of `(384, 384)` (the pretrained ViT in exercise 3 has a minimum input size of `(224, 224)`), though this is accessible in the weights `.transforms()` method.\n5. Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:\n    * ImageNet-21k pretraining (more data).\n    * Learning rate warmup.\n    * Learning rate decay.\n    * Gradient clipping.","metadata":{"id":"04b1569b-117e-43fd-9e0b-324157cb82a4"}},{"cell_type":"markdown","source":"## Extra-curriculum\n\n* There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580). Despite of the upgrades, we stuck with replicating a \"vanilla Vision Transformer\" in this notebook because if you understand the structure of the original, you can bridge to different iterations.\n* The [`vit-pytorch` repository on GitHub by lucidrains](https://github.com/lucidrains/vit-pytorch) is one of the most extensive resources of different ViT architectures implemented in PyTorch. It's a phenomenal reference and one I used often to create the materials we've been through in this chapter.\n* PyTorch have their [own implementation of the ViT architecture on GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py), it's used as the basis of the pretrained ViT models in `torchvision.models`.\n* Jay Alammar has fantastic illustrations and explanations on his blog of the [attention mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) (the foundation of Transformer models) and [Transformer models](https://jalammar.github.io/illustrated-transformer/).\n* Adrish Dey has a fantastic [write up of Layer Normalization](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) (a main component of the ViT architecture) can help neural network training.\n* The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper.\n* Yannic Kilcher's YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:\n    * [Attention is all you need](https://www.youtube.com/watch?v=iDulhoQ2pro) (the paper that introduced the Transformer architecture).\n    * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://youtu.be/TrdevFK_am4) (the paper that introduced the ViT architecture).","metadata":{"id":"dd69be46-cb68-4391-9834-8f87d8814722"}}]}