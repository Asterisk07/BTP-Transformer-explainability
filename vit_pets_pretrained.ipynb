{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":268736,"sourceType":"datasetVersion","datasetId":112480}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Asterisk07/BTP-Transformer-explainability/blob/main/vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"# Vision Transformers from scratch","metadata":{"id":"xTDvAeHGoJS3"}},{"cell_type":"markdown","source":"- [ViT Blogpost by Francesco Zuppichini](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n- [D2L Tutorial ](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)\n- [Brian Pulfer Medium Blogpost](https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)\n- [Lucidrains implementation Github ](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py)","metadata":{"id":"SHZBgsNCdCRk"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"wdEfT7i40Eka"}},{"cell_type":"code","source":"!pipstall einops\n# !pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSA9DeQ-0GJk","outputId":"2d254552-7b72-4085-ddd4-32f4fd57a1ce","execution":{"iopub.status.busy":"2024-08-17T07:53:39.523606Z","iopub.execute_input":"2024-08-17T07:53:39.524237Z","iopub.status.idle":"2024-08-17T07:54:09.659973Z","shell.execute_reply.started":"2024-08-17T07:53:39.524193Z","shell.execute_reply":"2024-08-17T07:54:09.658185Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:09.662627Z","iopub.execute_input":"2024-08-17T07:54:09.663135Z","iopub.status.idle":"2024-08-17T07:54:09.669981Z","shell.execute_reply.started":"2024-08-17T07:54:09.663087Z","shell.execute_reply":"2024-08-17T07:54:09.668599Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Image Patching\n\n","metadata":{"id":"K2k5RT2l4XuN"}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom torchvision.models.vision_transformer import ViT_B_32_Weights\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import ImageFolder","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:09.671533Z","iopub.execute_input":"2024-08-17T07:54:09.671926Z","iopub.status.idle":"2024-08-17T07:54:09.682966Z","shell.execute_reply.started":"2024-08-17T07:54:09.671883Z","shell.execute_reply":"2024-08-17T07:54:09.681388Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(ViT_B_32_Weights)\nfor i in ViT_B_32_Weights:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:09.686302Z","iopub.execute_input":"2024-08-17T07:54:09.686834Z","iopub.status.idle":"2024-08-17T07:54:09.697966Z","shell.execute_reply.started":"2024-08-17T07:54:09.686784Z","shell.execute_reply":"2024-08-17T07:54:09.696230Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<enum 'ViT_B_32_Weights'>\nViT_B_32_Weights.IMAGENET1K_V1\n","output_type":"stream"}]},{"cell_type":"code","source":"type(ViT_B_32_Weights.IMAGENET1K_V1)\nx=list(ViT_B_32_Weights.IMAGENET1K_V1)\nfor i in x:\n    print(i,\" \",type(i))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:09.699917Z","iopub.execute_input":"2024-08-17T07:54:09.700454Z","iopub.status.idle":"2024-08-17T07:54:10.160505Z","shell.execute_reply.started":"2024-08-17T07:54:09.700416Z","shell.execute_reply":"2024-08-17T07:54:10.148330Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mtype\u001b[39m(ViT_B_32_Weights\u001b[38;5;241m.\u001b[39mIMAGENET1K_V1)\n\u001b[0;32m----> 2\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mViT_B_32_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGENET1K_V1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mtype\u001b[39m(i))\n","\u001b[0;31mTypeError\u001b[0m: 'ViT_B_32_Weights' object is not iterable"],"ename":"TypeError","evalue":"'ViT_B_32_Weights' object is not iterable","output_type":"error"}]},{"cell_type":"code","source":"type(x)\nimport sys\nsys.getsizeof(x)\n# type(2)\n# sys.getsizeof([1,3])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.162654Z","iopub.status.idle":"2024-08-17T07:54:10.163504Z","shell.execute_reply.started":"2024-08-17T07:54:10.163154Z","shell.execute_reply":"2024-08-17T07:54:10.163184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import OxfordIIITPet\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom torchvision.transforms import Resize, ToTensor\nfrom torchvision.transforms.functional import to_pil_image\n\nto_tensor = [Resize((144, 144)), ToTensor()]\n\nimport os\n\n# List all files and directories in the dataset directory\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.166546Z","iopub.status.idle":"2024-08-17T07:54:10.167651Z","shell.execute_reply.started":"2024-08-17T07:54:10.167309Z","shell.execute_reply":"2024-08-17T07:54:10.167338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\n# Example: Load an image file\nimg = Image.open('/kaggle/input/the-oxfordiiit-pet-dataset')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.169999Z","iopub.status.idle":"2024-08-17T07:54:10.170931Z","shell.execute_reply.started":"2024-08-17T07:54:10.170601Z","shell.execute_reply":"2024-08-17T07:54:10.170628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n#     transforms.ToTensor(),          # Convert images to PyTorch tensors\n# ])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.172850Z","iopub.status.idle":"2024-08-17T07:54:10.173331Z","shell.execute_reply.started":"2024-08-17T07:54:10.173078Z","shell.execute_reply":"2024-08-17T07:54:10.173095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_tensor = [Resize((144, 144)), ToTensor()]\ndataset_path = '/kaggle/input/the-oxfordiiit-pet-dataset/images'\n# dataset = datasets.ImageFolder(root=dataset_path, transform=Compose(to_tensor))\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ndataset = datasets.ImageFolder(root=dataset_path, transform=transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.178155Z","iopub.status.idle":"2024-08-17T07:54:10.178605Z","shell.execute_reply.started":"2024-08-17T07:54:10.178400Z","shell.execute_reply":"2024-08-17T07:54:10.178419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hey\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchvision.datasets import OxfordIIITPet\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom torchvision.transforms import Resize, ToTensor\nfrom torchvision.transforms.functional import to_pil_image\n\nto_tensor = [Resize((144, 144)), ToTensor()]\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image = t(image)\n        return image, target\n\ndef preview_dataset(images, num_samples=40, cols=8):\n    \"\"\" Plots some samples from the dataset \"\"\"\n    plt.figure(figsize=(15,15))\n    idx = int(len(dataset) / num_samples)\n    print(images)\n    for i, img in enumerate(images):\n        if i % idx == 0:\n            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)\n            plt.imshow(to_pil_image(img[0]))\n\n# 200 images for each pet\ndataset = OxfordIIITPet(root='.', download=True, transforms=Compose(to_tensor))\nprint(\"Dataset fetched\")\n# preview_dataset(dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"w0a8TAbg3KQd","outputId":"289b8ea1-35c9-465a-c007-7a8293ee9f2d","execution":{"iopub.status.busy":"2024-08-17T07:54:10.179746Z","iopub.status.idle":"2024-08-17T07:54:10.180172Z","shell.execute_reply.started":"2024-08-17T07:54:10.179965Z","shell.execute_reply":"2024-08-17T07:54:10.179983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type(img) is torch.Tensor\nSCALE = 0.5","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.181516Z","iopub.status.idle":"2024-08-17T07:54:10.181955Z","shell.execute_reply.started":"2024-08-17T07:54:10.181745Z","shell.execute_reply":"2024-08-17T07:54:10.181764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(images, figsize = (15, 5), scale_factor = 1,titles=None):\n    '''handles both tensors and images'''\n    num_images = len(images)\n    \n    # Create a figure and set of subplots\n    fig, axes = plt.subplots(1, num_images, figsize=np.array(figsize) * scale_factor)\n    \n    # Ensure axes is an array, even for a single image\n    if num_images == 1:\n        axes = [axes]\n    \n    # Display each image in a subplot\n    for ax, img, title in zip(axes, images, titles or ['']*num_images):\n        if type(img)  is torch.Tensor:\n            img = img.permute(1, 2, 0).numpy()\n        # Convert color images from BGR to RGB\n        # if img.ndim == 3 and img.shape[2] == 3:\n        #     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        ax.imshow(img, cmap='gray' if len(img.shape) == 2 or img.shape[-1] == 1 else None)\n        ax.axis('off')  # Hide axis\n\n        ax.set_title(title.title())  # Set the title for the subplot\n\n    plt.show()\n# show_images(images = [new_dataset[0][0]] ,scale_factor = SCALE,titles = ['color_img'])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.183212Z","iopub.status.idle":"2024-08-17T07:54:10.183590Z","shell.execute_reply.started":"2024-08-17T07:54:10.183406Z","shell.execute_reply":"2024-08-17T07:54:10.183422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.185558Z","iopub.status.idle":"2024-08-17T07:54:10.186023Z","shell.execute_reply.started":"2024-08-17T07:54:10.185801Z","shell.execute_reply":"2024-08-17T07:54:10.185819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.187490Z","iopub.status.idle":"2024-08-17T07:54:10.187935Z","shell.execute_reply.started":"2024-08-17T07:54:10.187730Z","shell.execute_reply":"2024-08-17T07:54:10.187748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EDA : black and white'''\nidx = 0\nSCALE = 0.5\n\n# ------------\nimg = dataset[idx][0]\n# img = new_dataset[idx][0]\nimage_np = img.permute(1, 2, 0).numpy()\n# Convert to NumPy array and transpose to (H, W, C)\n\ngray_image = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis] if image_np.shape[2] == 3 else image_np\n\n# SCALE = 75/100\n\nshow_images(images = [img, gray_image,] ,scale_factor = SCALE,titles = ['original', 'grayscale'])\n# show_images(images = [grey_image, bw_image, x,] ,scale_factor = SCALE,titles = ['Greyscale','B/W','One color per row'])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.190746Z","iopub.status.idle":"2024-08-17T07:54:10.191205Z","shell.execute_reply.started":"2024-08-17T07:54:10.190982Z","shell.execute_reply":"2024-08-17T07:54:10.191001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_dataset = dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.192229Z","iopub.status.idle":"2024-08-17T07:54:10.192617Z","shell.execute_reply.started":"2024-08-17T07:54:10.192424Z","shell.execute_reply":"2024-08-17T07:54:10.192440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataset = list()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.193816Z","iopub.status.idle":"2024-08-17T07:54:10.194247Z","shell.execute_reply.started":"2024-08-17T07:54:10.194046Z","shell.execute_reply":"2024-08-17T07:54:10.194063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.196970Z","iopub.status.idle":"2024-08-17T07:54:10.197757Z","shell.execute_reply.started":"2024-08-17T07:54:10.197371Z","shell.execute_reply":"2024-08-17T07:54:10.197399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing labels\nimport gzip\nimport pickle\nwith gzip.open('classes.gz', 'rb') as f:\n    loaded_labels = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.199556Z","iopub.status.idle":"2024-08-17T07:54:10.200157Z","shell.execute_reply.started":"2024-08-17T07:54:10.199866Z","shell.execute_reply":"2024-08-17T07:54:10.199891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loaded_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.202215Z","iopub.status.idle":"2024-08-17T07:54:10.202826Z","shell.execute_reply.started":"2024-08-17T07:54:10.202504Z","shell.execute_reply":"2024-08-17T07:54:10.202531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''preprocessing : black and white'''\nnew_dataset= list()\nclass_idx = -1\nfor idx in tqdm(range(len(dataset))):\n    if (idx%50==0):\n        class_idx = idx\n    img = dataset[idx][0]\n    image_np = img.permute(1, 2, 0).numpy()\n    # Convert to NumPy array and transpose to (H, W, C)\n\n    # image_np = (image_np * 255).astype(np.uint8)\n\n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis]\n    grey_tensor = torch.from_numpy(gray_image).permute(2,0,1)\n    # Convert to convert to (1,H,W)\n\n    \n    # label = dataset[k][1]\n    # label = dataset[idx][1]\n    \n    try:\n        label = loaded_labels[class_idx]\n    except:\n        print(class_idx, \"is error\")\n        raise zero\n    new_dataset.append((grey_tensor,label))\nprint(len(new_dataset)) \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.204074Z","iopub.status.idle":"2024-08-17T07:54:10.204628Z","shell.execute_reply.started":"2024-08-17T07:54:10.204347Z","shell.execute_reply":"2024-08-17T07:54:10.204372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA : Verifying classes ","metadata":{}},{"cell_type":"code","source":"raise InterruptedError","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.206864Z","iopub.status.idle":"2024-08-17T07:54:10.207479Z","shell.execute_reply.started":"2024-08-17T07:54:10.207163Z","shell.execute_reply":"2024-08-17T07:54:10.207189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N =len(dataset)\nl1 = ['dog','cat']\n# N = 100\nN = len(new_dataset)\nl1 \n\ncontext_length = 4\nfor i in range(0,N,50):\n    print(\"image idx \",i)\n    show_images(images = [new_dataset[j][0] for j in range(i,i+context_length)] ,scale_factor = 1,titles = [l1[new_dataset[i][1]]]*context_length)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.209641Z","iopub.status.idle":"2024-08-17T07:54:10.210225Z","shell.execute_reply.started":"2024-08-17T07:54:10.209931Z","shell.execute_reply":"2024-08-17T07:54:10.209956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise InterruptedError","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.212208Z","iopub.status.idle":"2024-08-17T07:54:10.212764Z","shell.execute_reply.started":"2024-08-17T07:54:10.212518Z","shell.execute_reply":"2024-08-17T07:54:10.212538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_dict={}\n# for i in tqdm(range(len(new_dataset))):\n#     label=new_dataset[i][1]\n#     if(label not in img_dict):\n#         img_dict[new_dataset[i][1]]=[]\n#     img_dict[label].append(i)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.214823Z","iopub.status.idle":"2024-08-17T07:54:10.215458Z","shell.execute_reply.started":"2024-08-17T07:54:10.215148Z","shell.execute_reply":"2024-08-17T07:54:10.215174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(img_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.217788Z","iopub.status.idle":"2024-08-17T07:54:10.218387Z","shell.execute_reply.started":"2024-08-17T07:54:10.218086Z","shell.execute_reply":"2024-08-17T07:54:10.218111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# title=[]\n# images=[]\n# SCALE=0.5\n# for i in tqdm(img_dict):\n#     for j in range(1):\n#         idx=img_dict[i][j]\n#         img=dataset[idx][0]\n#         # label=\n#         images.append(img)\n#         title.append(i)\n#         # title.append(idx)\n    \n# # show_images(images ,SCALE,title)\n# for i in range(len(images)//10+1):\n\n#     show_images(images = images[i:i+10] ,scale_factor = SCALE,titles = title[i:i+10])\n\n\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.220816Z","iopub.status.idle":"2024-08-17T07:54:10.221499Z","shell.execute_reply.started":"2024-08-17T07:54:10.221182Z","shell.execute_reply":"2024-08-17T07:54:10.221209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gzip\nimport pickle\n\n# Save the list to a gzipped file\n# with gzip.open('binary_label_dict.gz', 'wb') as f:\n#     pickle.dump(binary_label_dict, f)\n\n# # Load the list from the gzipped file\n# with gzip.open('new_labels.gz', 'rb') as f:\n#     loaded_labels = pickle.load(f)\n\nwith gzip.open('classes.gz', 'rb') as f:\n    loaded_labels = pickle.load(f)\n\n# # Display the loaded list\n# print(\"Loaded list:\", loaded_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.223611Z","iopub.status.idle":"2024-08-17T07:54:10.224252Z","shell.execute_reply.started":"2024-08-17T07:54:10.223936Z","shell.execute_reply":"2024-08-17T07:54:10.223961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA : Changing labels manually","metadata":{}},{"cell_type":"code","source":"import gzip\nimport pickle\n\nwith gzip.open('classes.gz', 'rb') as f:\n    loaded_labels = pickle.load(f)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.225802Z","iopub.status.idle":"2024-08-17T07:54:10.226369Z","shell.execute_reply.started":"2024-08-17T07:54:10.226077Z","shell.execute_reply":"2024-08-17T07:54:10.226109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N =len(dataset)\nl1 = ['dog','cat']\nnew_labels = {}\ncontext_length = 4\nfor i in range(0,N,50):\n    show_images(images = [dataset[j][0] for j in range(i,i+context_length)] ,scale_factor = 1,titles = None)\n    # label = int(input('Enter 1 for cat, 0 for dog, -1 otherwise'))\n    # binary_label_dict[i] = label\n    label = loaded_labels[i]\n    print(type(label))\n    print(f\"Current label :  {l1[label] if label!=-1 else None}\")\n\n    label = (input('Enter \"y\" to keep the current label, else Enter 1 for cat, 0 for dog, -1 otherwise'))\n    if(label=='y'):\n        print(f\"{i} th image remains same as {loaded_labels[i]}\")\n        label = loaded_labels[i]\n    else:\n        label = int(label)\n        print(f\"{i} th image set to {l1[int(label)] if int(label)!=-1 else None}\")\n    if label == -1:\n        print(\"Error at \",i)\n        raise ZeroDivisionError\n        \n    new_labels[i] = label\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.228812Z","iopub.status.idle":"2024-08-17T07:54:10.229504Z","shell.execute_reply.started":"2024-08-17T07:54:10.229122Z","shell.execute_reply":"2024-08-17T07:54:10.229142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with gzip.open('new_labels.gz', 'wb') as f:\n    pickle.dump(new_labels, f)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.231137Z","iopub.status.idle":"2024-08-17T07:54:10.231562Z","shell.execute_reply.started":"2024-08-17T07:54:10.231365Z","shell.execute_reply":"2024-08-17T07:54:10.231381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise IOError","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.233321Z","iopub.status.idle":"2024-08-17T07:54:10.233778Z","shell.execute_reply.started":"2024-08-17T07:54:10.233551Z","shell.execute_reply":"2024-08-17T07:54:10.233567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling","metadata":{}},{"cell_type":"code","source":"loaded_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.234998Z","iopub.status.idle":"2024-08-17T07:54:10.235427Z","shell.execute_reply.started":"2024-08-17T07:54:10.235229Z","shell.execute_reply":"2024-08-17T07:54:10.235246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport torch.nn.functional as F\n\ndef downsample_image_tensor(image_tensor, factor):\n    \"\"\"\n    Downsample a 3D image tensor (1, H, W) by a given factor.\n\n    Args:\n        image_tensor (torch.Tensor): The input image tensor of shape (1, H, W).\n        factor (int): The factor by which to downsample the image.\n\n    Returns:\n        torch.Tensor: The downsampled image tensor with shape (1, new_H, new_W).\n    \"\"\"\n    # Ensure the input tensor is a 3D tensor of shape (1, H, W)\n    if len(image_tensor.shape) != 3 or image_tensor.shape[0] != 1:\n        raise ValueError(\"Input tensor must be a 3D tensor of shape (1, H, W).\")\n    \n    # Add a batch dimension to the tensor\n    image_tensor = image_tensor.unsqueeze(0)  # Now shape is (1, 1, H, W)\n\n    # Calculate the new size\n    new_size = (image_tensor.shape[2] // factor, image_tensor.shape[3] // factor)\n\n    # Downsample the image using bilinear interpolation\n    downsampled_image_tensor = F.interpolate(image_tensor, size=new_size, mode='bilinear', align_corners=False)\n\n    # Remove the batch dimension\n    downsampled_image_tensor = downsampled_image_tensor.squeeze(0)  # Now shape is (1, new_H, new_W)\n\n    return downsampled_image_tensor\n\n\n#hyperparameter\nfactor=2\n\n# downsampledImage=downsample_image_tensor(tensor,factor)\n# print(downsampledImage)\n\n# show_images(images = [img, tensor,] ,scale_factor = SCALE,titles = ['original', 'grayscale'])\n\n\n# # Example usage\n# image_tensor = torch.randn(1, 256, 256)  # Example image tensor of shape (1, H, W)\n# factor = 2\n\n# downsampled_image_tensor = downsample_image_tensor(image_tensor, factor)\n# print(downsampled_image_tensor.shape)  # Output shape should be (1, 128, 128)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.238094Z","iopub.status.idle":"2024-08-17T07:54:10.238552Z","shell.execute_reply.started":"2024-08-17T07:54:10.238336Z","shell.execute_reply":"2024-08-17T07:54:10.238357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_dataset = new_dataset.copy()\nold_dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.239614Z","iopub.status.idle":"2024-08-17T07:54:10.240082Z","shell.execute_reply.started":"2024-08-17T07:54:10.239867Z","shell.execute_reply":"2024-08-17T07:54:10.239885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"factor = 2\nnew_dataset=old_dataset.copy()\nfor i in range(len(new_dataset)):\n    # newTensor=new_dataset[i][0]\n    new_dataset[i] = (downsample_image_tensor(new_dataset[i][0],2),new_dataset[i][1])\n\nnew_dataset[0][0].shape\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.241315Z","iopub.status.idle":"2024-08-17T07:54:10.241736Z","shell.execute_reply.started":"2024-08-17T07:54:10.241510Z","shell.execute_reply":"2024-08-17T07:54:10.241525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise SectionEndError","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:54:10.243179Z","iopub.status.idle":"2024-08-17T07:54:10.243583Z","shell.execute_reply.started":"2024-08-17T07:54:10.243390Z","shell.execute_reply":"2024-08-17T07:54:10.243407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patch Images\n\n- The following is mainly from the above implementations (not my code)","metadata":{"id":"2WPMD1CTx5lL"}},{"cell_type":"code","source":"from torch import nn\nfrom einops.layers.torch import Rearrange\nfrom torch import Tensor\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n        self.patch_size = patch_size\n        super().__init__()\n        self.projection = nn.Sequential(\n            # break-down the image in s1 x s2 patches and flat them\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n        )\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.projection(x)\n        return x\n\n# Run a quick test\n\nsample_datapoint = torch.unsqueeze(new_dataset[0][0], 0)\nprint(\"Initial shape: \", sample_datapoint.shape)\nembedding = PatchEmbedding(1,1,144)(sample_datapoint)\nprint(\"Patches shape: \", embedding.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ap3RzZa0yZXt","outputId":"0091eb8f-b008-46b2-ba6d-4860a078a8de","execution":{"iopub.status.busy":"2024-08-17T07:54:10.246039Z","iopub.status.idle":"2024-08-17T07:54:10.246453Z","shell.execute_reply.started":"2024-08-17T07:54:10.246255Z","shell.execute_reply":"2024-08-17T07:54:10.246272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"id":"LyXkSfe84dEj"}},{"cell_type":"markdown","source":"Let's first implement all of the transformer building blocks. These blocks are inspired by the implementations linked above. I've left out some dropouts and normalizations at some places.","metadata":{"id":"-ohxDCNNsim0"}},{"cell_type":"code","source":"from einops import rearrange\n\nclass Attention(nn.Module):\n    def __init__(self, dim, n_heads, dropout):\n        super().__init__()\n        self.n_heads = n_heads\n        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n                                               num_heads=n_heads,\n                                               dropout=dropout)\n        self.q = torch.nn.Linear(dim, dim)\n        self.k = torch.nn.Linear(dim, dim)\n        self.v = torch.nn.Linear(dim, dim)\n\n    def forward(self, x):\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        attn_output, attn_output_weights = self.att(x, x, x)\n        return attn_output","metadata":{"id":"lbKH_cEt_AGF","execution":{"iopub.status.busy":"2024-08-17T07:54:10.247635Z","iopub.status.idle":"2024-08-17T07:54:10.248067Z","shell.execute_reply.started":"2024-08-17T07:54:10.247871Z","shell.execute_reply":"2024-08-17T07:54:10.247888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Attention(dim=128, n_heads=4, dropout=0.)(torch.ones((1, 5, 128))).shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_7Q0Quj_Ch8","outputId":"8856d2dd-eca8-48ca-c05f-1ef20dfb4236","execution":{"iopub.status.busy":"2024-08-17T07:54:10.249336Z","iopub.status.idle":"2024-08-17T07:54:10.249800Z","shell.execute_reply.started":"2024-08-17T07:54:10.249550Z","shell.execute_reply":"2024-08-17T07:54:10.249566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)","metadata":{"id":"LZIq8qATHX7p","execution":{"iopub.status.busy":"2024-08-17T07:54:10.251611Z","iopub.status.idle":"2024-08-17T07:54:10.252060Z","shell.execute_reply.started":"2024-08-17T07:54:10.251854Z","shell.execute_reply":"2024-08-17T07:54:10.251871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm = PreNorm(128, Attention(dim=128, n_heads=4, dropout=0.))\nnorm(torch.ones((1, 5, 128))).shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpYpgqCqHcrs","outputId":"4c46e5e9-8d92-4c12-9298-79cbe5da6170","execution":{"iopub.status.busy":"2024-08-17T07:54:10.254780Z","iopub.status.idle":"2024-08-17T07:54:10.255338Z","shell.execute_reply.started":"2024-08-17T07:54:10.255051Z","shell.execute_reply":"2024-08-17T07:54:10.255074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Sequential):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\nff = FeedForward(dim=128, hidden_dim=256)\nff(torch.ones((1, 5, 128))).shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGsxbcqxIGJg","outputId":"c0567e6a-99f0-4017-ec92-e940771d32dc","execution":{"iopub.status.busy":"2024-08-17T07:54:10.257928Z","iopub.status.idle":"2024-08-17T07:54:10.258423Z","shell.execute_reply.started":"2024-08-17T07:54:10.258207Z","shell.execute_reply":"2024-08-17T07:54:10.258227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualAdd(nn.Module):\n    # skip level\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        res = x\n        x = self.fn(x, **kwargs)\n        x += res\n        return x","metadata":{"id":"WuWewoFfI7Vs","execution":{"iopub.status.busy":"2024-08-17T07:54:10.260425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual_att = ResidualAdd(Attention(dim=128, n_heads=4, dropout=0.))\nresidual_att(torch.ones((1, 5, 128))).shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hiv5BDUAI73N","outputId":"0da1c01d-f3fd-4906-851b-b8a540979e2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Not all parameters are like in the original implementation\n- Some Dropouts & Norms are missing","metadata":{"id":"qewws3-Nk7zB"}},{"cell_type":"code","source":"from einops import repeat\n\nclass ViT(nn.Module):\n    def __init__(self, ch=1, img_size=144, patch_size=4, emb_dim=32,\n                n_layers=6, out_dim=37, dropout=0.1, heads=2):\n        super(ViT, self).__init__()\n\n        # Attributes\n        self.channels = ch\n        self.height = img_size\n        self.width = img_size\n        self.patch_size = patch_size\n        self.n_layers = n_layers\n        \n        # Patching\n        \n        self.patch_embedding = PatchEmbedding(in_channels=ch,\n                                              patch_size=patch_size,\n                                              emb_size=emb_dim)\n        # Learnable params\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(\n            torch.randn(1, num_patches + 1, emb_dim))\n        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n\n        # Transformer Encoder\n        self.layers = nn.ModuleList([])\n        for _ in range(n_layers):\n            transformer_block = nn.Sequential(\n                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n                \n                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n            self.layers.append(transformer_block)\n\n        # Classification head\n        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n\n\n    def forward(self, img):\n        # Get patch embedding vectors\n\n       \n        x = self.patch_embedding(img)\n        b, n, _ = x.shape\n\n        # Add cls token to inputs\n        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n\n        # Transformer layers\n        for i in range(self.n_layers):\n            x = self.layers[i](x)\n\n        # Output based on classification token\n        return self.head(x[:, 0, :])\n\nchannels=1\npatchsize=4\nimgsize=144\nemb_dim=32\n# channels=3\n# patchsize=4\n# imgsize=144\n# model = ViT()\n# model = ViT(channels,patchsize,imgsize)\nmodel = ViT(channels,imgsize,patchsize,emb_dim)\nprint(model)\nmodel(torch.ones((1, 1, 144, 144)))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVVWmPso4eGf","outputId":"8e6aa425-7739-4c26-f02f-2315689feac7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA block patchsize vs num_params\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx =list( np.arange(1,128,2))\ny = list()\nfor i in x:\n    model = ViT(patch_size=i).to(device)\n    p = model\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 101157\n    y.append(num_params)\n\nplt.plot(x,y)\nplt.xticks(ticks=np.arange(1,128,6))\nplt.yticks(ticks=np.arange(0,8,0.5))\nplt.title(\"Patchsize efffect on num_params\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data splitting","metadata":{"id":"j6j2tczm4fGs"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n# data = new_dataset\ndata = dataset\n\n\ntrain_split = int(0.8 * len(data))\ntrain, test = random_split(data, [train_split, len(data) - train_split])\n\ntrain_dataloader = DataLoader(train, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test, batch_size=32, shuffle=True)\nprint(\"hey\")","metadata":{"id":"FsdvFNcr4iFj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for batch_index, (images, labels) in enumerate(train_dataloader):\n    print(f\"Batch {batch_index + 1}:\")\n    print(\"Shape of images:\", images.shape)  # e.g., [32, 3, 224, 224]\n    print(\"Shape of labels:\", labels.shape)  # e.g., [32]\n\n    # Example of processing the first batch\n    if batch_index == 0:\n        # Print the first image and label\n        print(\"First image tensor shape:\", images[0].shape)  # e.g., [3, 224, 224]\n        print(\"First label:\", labels[0].item())  # e.g., 0 (class index)\n\n    # Break after inspecting the first batch (remove this line to process the entire dataset)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# functions : \nfrom sklearn.metrics import roc_auc_score\nimport torch\n\ndef roc_auc(logits, labels):\n    \"\"\"\n    Compute the ROC AUC score.\n\n    Args:\n        logits (torch.Tensor or np.ndarray): The raw predictions (logits) from the model. \n                                             Should be of shape (num_samples, num_classes).\n        labels (torch.Tensor or np.ndarray): The true labels. Should be of shape (num_samples, num_classes).\n\n    Returns:\n        float: The ROC AUC score.\n    \"\"\"\n\n    if len(labels.shape)==2:\n        labels = labels.squeeze()\n        # print(\"shape is \",labels.shape)\n        \n    # Convert logits to probabilities using sigmoid for binary classification\n    # if logits.ndim == 2 and logits.shape[1] == 2:  # Binary classification\n    #     probs = torch.softmax(logits, dim=1)[:, 1]  # Get probabilities for the positive class\n    # elif logits.ndim == 1:  # If logits is already a 1D tensor (e.g., for binary classification with single output)\n    #     probs = torch.sigmoid(logits)\n    # else:\n    #     raise ValueError(\"Invalid shape for logits\")\n    \n    probs = torch.sigmoid(logits)\n\n    # Convert to numpy arrays if needed\n    probs = probs.detach().numpy() if torch.is_tensor(probs) else probs\n    labels = labels.detach().numpy() if torch.is_tensor(labels) else labels\n\n    # Calculate ROC AUC score\n    auc_score = roc_auc_score(labels, probs)\n    return auc_score\n\n# Example usage\n# test_pred = model(test_data)  # Logits from your model\n# y_test = true_labels  # True labels\n# test_auc = roc_auc(test_pred, y_test)\n# print(f\"ROC AUC Score: {test_auc}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise Zer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters : \nppos_embed\ncls_token\npatch_embed : weight, bias\neach layer has 2 sublayers:\n    attention : \n    Feedforward : norm & ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_display(epoch):\n    with open(\"gradients.txt\", \"w\") as f:\n        f.write(f'Gradients at Epoch : {epoch} are')\n        f.write('\\n')\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n            # print(f\"Layer: {name}, Gradient: {param.grad}\")\n                f.write(f\"Layer: {name}, Gradient: {param.grad}\\n\")\n        f.write(\"-------------------------------------------------------------------------------------\")\n        f.write(\"\\n\\n\\n\")\n        # f.write(\"-------------------------------------------------------------------------------------\")\n    \n    \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA : Exploring gradients and parameters","metadata":{}},{"cell_type":"code","source":"# del\n# model = ViT(heads=8, n_layers = 1)\nl1  = list(model.named_parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(l1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 6\nname, x = l1[n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = 0\nfor i,l2 in enumerate(l1):\n    print(i, ' ' if i<10 else '', l2[0], (40 - len(l2[0]))*' ', '|', list(l2[1].shape), l2[1].grad is not None)\n    # s = max(s,len(l2[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list of \n43 entries\n(name, tensor, shape, param.grad is not none, gradient tensor, gradient shape, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise End EDA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained Model ","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.vision_transformer import ViT_B_32_Weights\nmodel = models.vit_b_32(weights=ViT_B_32_Weights.DEFAULT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Model Weights","metadata":{}},{"cell_type":"code","source":"state_dict = model.state_dict()\n\n# Optionally, print the names and shapes of the weights\nfor name, param in state_dict.items():\n    print(f\"Layer: {name} | Shape: {param.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(dataset.classes)\n\n# Access and replace the final classification layer\nin_features = model.head.in_features  # or model.heads.in_features if 'heads' is used\nmodel.head = nn.Linear(in_features, num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.vit_b_32(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for images, labels in train_dataloader:\n#     # Print the shape of the images tensor\n#     print(\"Shape of images:\", images.shape)\n    \n#     # Print the shape of the labels tensor\n#     print(\"Shape of labels:\", labels.shape)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# hyperparameters tk hyper\nmax_iters = 1\neval_interval = 2\nlr = 0.001\n\nparams = {\n    'ch': 1,\n    'img_size': 144,\n    'patch_size': 8,\n    'emb_dim': 4,\n    'n_layers': 2,\n    'out_dim': 1,\n    'dropout': 0.5,\n    'heads': 2\n}\n\n# ---------------------\n\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device} | Total epochs : {max_iters}\")\n# model = ViT(**params).to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=lr)\n# criterion = nn.BCEWithLogitsLoss()\ncriterion = nn.CrossEntropyLoss()\n\n\n# for epoch in range(1000):\nfor epoch in (range(1,max_iters + 1)):\n    epoch_losses = []\n    model.train()\n    eval_flag = ( epoch % eval_interval == 0 or epoch == max_iters - 1)\n    \n    for step, (inputs, labels) in enumerate(train_dataloader):\n        if len(labels.shape)==1:\n            labels = labels.reshape(-1, 1).to(dtype=torch.float32)\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        print(len(labels))\n        print(inputs.shape)\n#         loss = criterion(outputs, labels)\n\n        if eval_flag:\n            epoch_losses.append(loss.detach().item())\n            # train_acc = roc_auc(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        gradient_display(epoch)\n        optimizer.step()\n        break\n        \n\n        \n\n\n    if eval_flag:\n        model.eval()\n        with torch.inference_mode():\n        \n            train_loss =  np.mean(epoch_losses)\n            epoch_losses = []\n            total = 0\n            correct = 0\n            model.eval()\n            for step, (inputs, labels) in enumerate(test_dataloader):\n                if len(labels.shape)==1:\n                    labels = labels.reshape(-1, 1).to(dtype=torch.float32)\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                epoch_losses.append(loss.detach().item())\n\n\n                # Get the predicted classes\n                _, predicted = torch.max(outputs, 1)\n\n                # Update the total number of samples\n                total += labels.size(0)\n\n                # Update the number of correct predictions\n                correct += (predicted == labels.squeeze()).sum().item()\n\n            test_loss =  np.mean(epoch_losses)\n            test_acc = correct / total\n            # print(f'Accuracy: {accuracy*100:.2f}')\n\n            # train_acc = roc_auc(outputs, labels)\n            print(f\"Epoch: {epoch}  | Train Loss : {train_loss:.5f}  | Test Loss:  {test_loss:.5f} | Test Accuracy : {test_acc*100:.3f} % \")\n        # print(f\"step {epoch}: train loss {train_loss:.4f}, test loss {test_loss:.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"N-Q_3rEJSOfA","outputId":"f2c01d9c-37ec-43f4-d0fc-35e1cd036c50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss is None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for step, (inputs, labels) in enumerate(train_dataloader):\n    print(inputs.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs, labels = next(iter(test_dataloader))\ninputs, labels = inputs.to(device), labels.to(device)\noutputs = model(inputs)\n\nprint(\"Predicted classes\", outputs.argmax(-1))\nprint(\"Actual classes\", labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7LkzG6fOt_k","outputId":"3ffc1e1c-46a7-43bb-e8a7-498cee132bf2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\n\n# Assuming you have already defined `test_dataloader` and `model`\n\n# Move model to the correct device\nmodel.to(device)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Initialize variables to track the total and correct predictions\ntotal = 0\ncorrect = 0\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n\n        # Get the predicted classes\n        _, predicted = torch.max(outputs, 1)\n\n        # Update the total number of samples\n        total += labels.size(0)\n\n        # Update the number of correct predictions\n        correct += (predicted == labels).sum().item()\n\n# Calculate accuracy\naccuracy = correct / total\n\nprint(f'Accuracy: {accuracy*100:.2f}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiYhpaPZtPFR","outputId":"07165394-9854-4ad5-9f3b-19eab0d5040f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This needs to train much longer :)","metadata":{"id":"sM9VzohRCrC_"}}]}